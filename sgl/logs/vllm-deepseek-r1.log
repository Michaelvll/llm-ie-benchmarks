[33mTailing logs of job 32 on cluster 'benchmark'...[0m
[2m├── [0m[2mWaiting for task resources on 1 node.[0m
[2m└── [0mJob started. Streaming logs... [2m(Ctrl-C to exit log streaming; job will not be killed)[0m
[36m(task, pid=6382)[0m Using Python 3.10.13 environment at: /home/ubuntu/miniconda3
[36m(task, pid=6382)[0m Audited 1 package in 12ms
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m Waiting for vllm server to start...
[36m(task, pid=6382)[0m vllm server started
[36m(task, pid=6382)[0m benchmark_args=Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=None, dataset_name='random', dataset_path='', model=None, tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1000, random_output_len=2000, random_range_ratio=1.0, request_rate=10.0, max_concurrency=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_seperated=False, flush_cache=False, warmup_requests=1, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
[36m(task, pid=6382)[0m Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='', model='deepseek-ai/DeepSeek-R1', tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1000, random_output_len=2000, random_range_ratio=1.0, request_rate=10.0, max_concurrency=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_seperated=False, flush_cache=False, warmup_requests=1, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
[36m(task, pid=6382)[0m 
[36m(task, pid=6382)[0m If you do not want to randomly sample from a dataset, please use --dataset-name random-ids.
[36m(task, pid=6382)[0m #Input tokens: 50000
[36m(task, pid=6382)[0m #Output tokens: 100000
[36m(task, pid=6382)[0m Starting warmup with 1 sequences...
[36m(task, pid=6382)[0m Warmup completed with 1 sequences. Starting main benchmark run...
[36m(task, pid=6382)[0m 
[36m(task, pid=6382)[0m   0%|          | 0/50 [00:00<?, ?it/s]
[36m(task, pid=6382)[0m   2%|▏         | 1/50 [01:33<1:16:32, 93.72s/it]
[36m(task, pid=6382)[0m  18%|█▊        | 9/50 [01:33<05:09,  7.56s/it]  
[36m(task, pid=6382)[0m  44%|████▍     | 22/50 [01:33<01:07,  2.42s/it]
[36m(task, pid=6382)[0m  62%|██████▏   | 31/50 [01:34<00:27,  1.46s/it]
[36m(task, pid=6382)[0m  76%|███████▌  | 38/50 [01:34<00:12,  1.02s/it]
[36m(task, pid=6382)[0m  86%|████████▌ | 43/50 [01:34<00:05,  1.26it/s]
[36m(task, pid=6382)[0m  96%|█████████▌| 48/50 [01:34<00:01,  1.68it/s]
100%|██████████| 50/50 [01:35<00:00,  1.90s/it]
[36m(task, pid=6382)[0m 
[36m(task, pid=6382)[0m ============ Serving Benchmark Result ============
[36m(task, pid=6382)[0m Backend:                                 vllm      
[36m(task, pid=6382)[0m Traffic request rate:                    10.0      
[36m(task, pid=6382)[0m Max reqeuest concurrency:                not set   
[36m(task, pid=6382)[0m Successful requests:                     50        
[36m(task, pid=6382)[0m Benchmark duration (s):                  95.04     
[36m(task, pid=6382)[0m Total input tokens:                      50000     
[36m(task, pid=6382)[0m Total generated tokens:                  100000    
[36m(task, pid=6382)[0m Total generated tokens (retokenized):    99627     
[36m(task, pid=6382)[0m Request throughput (req/s):              0.53      
[36m(task, pid=6382)[0m Input token throughput (tok/s):          526.10    
[36m(task, pid=6382)[0m Output token throughput (tok/s):         1052.20   
[36m(task, pid=6382)[0m Total token throughput (tok/s):          1578.31   
[36m(task, pid=6382)[0m Concurrency:                             48.51     
[36m(task, pid=6382)[0m ----------------End-to-End Latency----------------
[36m(task, pid=6382)[0m Mean E2E Latency (ms):                   92212.10  
[36m(task, pid=6382)[0m Median E2E Latency (ms):                 92201.92  
[36m(task, pid=6382)[0m ---------------Time to First Token----------------
[36m(task, pid=6382)[0m Mean TTFT (ms):                          328.06    
[36m(task, pid=6382)[0m Median TTFT (ms):                        314.12    
[36m(task, pid=6382)[0m P99 TTFT (ms):                           662.89    
[36m(task, pid=6382)[0m ---------------Inter-Token Latency----------------
[36m(task, pid=6382)[0m Mean ITL (ms):                           46.12     
[36m(task, pid=6382)[0m Median ITL (ms):                         45.14     
[36m(task, pid=6382)[0m P95 ITL (ms):                            45.94     
[36m(task, pid=6382)[0m P99 ITL (ms):                            46.87     
[36m(task, pid=6382)[0m Max ITL (ms):                            358.36    
[36m(task, pid=6382)[0m ==================================================
[36m(task, pid=6382)[0m | Input Tokens | Output Tokens | Output Token Throughput (tok/s) |
[36m(task, pid=6382)[0m |1000 | 2000 | 1052.20 |
[36m(task, pid=6382)[0m benchmark_args=Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=None, dataset_name='random', dataset_path='', model=None, tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=5000, random_output_len=1000, random_range_ratio=1.0, request_rate=10.0, max_concurrency=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_seperated=False, flush_cache=False, warmup_requests=1, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
[36m(task, pid=6382)[0m Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='', model='deepseek-ai/DeepSeek-R1', tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=5000, random_output_len=1000, random_range_ratio=1.0, request_rate=10.0, max_concurrency=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_seperated=False, flush_cache=False, warmup_requests=1, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
[36m(task, pid=6382)[0m 
[36m(task, pid=6382)[0m If you do not want to randomly sample from a dataset, please use --dataset-name random-ids.
[36m(task, pid=6382)[0m #Input tokens: 250000
[36m(task, pid=6382)[0m #Output tokens: 50000
[36m(task, pid=6382)[0m Starting warmup with 1 sequences...
[36m(task, pid=6382)[0m Warmup completed with 1 sequences. Starting main benchmark run...
[36m(task, pid=6382)[0m 
[36m(task, pid=6382)[0m   0%|          | 0/50 [00:00<?, ?it/s]
[36m(task, pid=6382)[0m   2%|▏         | 1/50 [01:01<50:01, 61.25s/it]
[36m(task, pid=6382)[0m  10%|█         | 5/50 [01:01<06:51,  9.15s/it]
[36m(task, pid=6382)[0m  20%|██        | 10/50 [01:01<02:25,  3.65s/it]
[36m(task, pid=6382)[0m  30%|███       | 15/50 [01:01<01:09,  1.97s/it]
[36m(task, pid=6382)[0m  38%|███▊      | 19/50 [01:01<00:40,  1.30s/it]
[36m(task, pid=6382)[0m  48%|████▊     | 24/50 [01:01<00:21,  1.23it/s]
[36m(task, pid=6382)[0m  58%|█████▊    | 29/50 [01:02<00:11,  1.87it/s]
[36m(task, pid=6382)[0m  68%|██████▊   | 34/50 [01:02<00:05,  2.75it/s]
[36m(task, pid=6382)[0m  78%|███████▊  | 39/50 [01:02<00:02,  3.93it/s]
[36m(task, pid=6382)[0m  88%|████████▊ | 44/50 [01:02<00:01,  5.51it/s]
100%|██████████| 50/50 [01:02<00:00,  7.99it/s]
100%|██████████| 50/50 [01:02<00:00,  1.25s/it]
[36m(task, pid=6382)[0m 
[36m(task, pid=6382)[0m ============ Serving Benchmark Result ============
[36m(task, pid=6382)[0m Backend:                                 vllm      
[36m(task, pid=6382)[0m Traffic request rate:                    10.0      
[36m(task, pid=6382)[0m Max reqeuest concurrency:                not set   
[36m(task, pid=6382)[0m Successful requests:                     50        
[36m(task, pid=6382)[0m Benchmark duration (s):                  62.53     
[36m(task, pid=6382)[0m Total input tokens:                      250000    
[36m(task, pid=6382)[0m Total generated tokens:                  50000     
[36m(task, pid=6382)[0m Total generated tokens (retokenized):    49837     
[36m(task, pid=6382)[0m Request throughput (req/s):              0.80      
[36m(task, pid=6382)[0m Input token throughput (tok/s):          3998.01   
[36m(task, pid=6382)[0m Output token throughput (tok/s):         799.60    
[36m(task, pid=6382)[0m Total token throughput (tok/s):          4797.62   
[36m(task, pid=6382)[0m Concurrency:                             47.89     
[36m(task, pid=6382)[0m ----------------End-to-End Latency----------------
[36m(task, pid=6382)[0m Mean E2E Latency (ms):                   59895.99  
[36m(task, pid=6382)[0m Median E2E Latency (ms):                 60022.97  
[36m(task, pid=6382)[0m ---------------Time to First Token----------------
[36m(task, pid=6382)[0m Mean TTFT (ms):                          5793.65   
[36m(task, pid=6382)[0m Median TTFT (ms):                        5924.34   
[36m(task, pid=6382)[0m P99 TTFT (ms):                           10211.59  
[36m(task, pid=6382)[0m ---------------Inter-Token Latency----------------
[36m(task, pid=6382)[0m Mean ITL (ms):                           54.32     
[36m(task, pid=6382)[0m Median ITL (ms):                         47.88     
[36m(task, pid=6382)[0m P95 ITL (ms):                            48.55     
[36m(task, pid=6382)[0m P99 ITL (ms):                            485.29    
[36m(task, pid=6382)[0m Max ITL (ms):                            1462.60   
[36m(task, pid=6382)[0m ==================================================
[36m(task, pid=6382)[0m | Input Tokens | Output Tokens | Output Token Throughput (tok/s) |
[36m(task, pid=6382)[0m |1000 | 2000 | 1052.20 |
[36m(task, pid=6382)[0m |5000 | 1000 | 799.60 |
[36m(task, pid=6382)[0m benchmark_args=Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=None, dataset_name='random', dataset_path='', model=None, tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=10000, random_output_len=500, random_range_ratio=1.0, request_rate=10.0, max_concurrency=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_seperated=False, flush_cache=False, warmup_requests=1, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
[36m(task, pid=6382)[0m Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='', model='deepseek-ai/DeepSeek-R1', tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=10000, random_output_len=500, random_range_ratio=1.0, request_rate=10.0, max_concurrency=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_seperated=False, flush_cache=False, warmup_requests=1, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
[36m(task, pid=6382)[0m 
[36m(task, pid=6382)[0m If you do not want to randomly sample from a dataset, please use --dataset-name random-ids.
[36m(task, pid=6382)[0m #Input tokens: 500000
[36m(task, pid=6382)[0m #Output tokens: 25000
[36m(task, pid=6382)[0m Starting warmup with 1 sequences...
[36m(task, pid=6382)[0m Warmup completed with 1 sequences. Starting main benchmark run...
[36m(task, pid=6382)[0m 
[36m(task, pid=6382)[0m   0%|          | 0/50 [00:00<?, ?it/s]
[36m(task, pid=6382)[0m   2%|▏         | 1/50 [00:54<44:39, 54.68s/it]
[36m(task, pid=6382)[0m   6%|▌         | 3/50 [00:54<11:08, 14.21s/it]
[36m(task, pid=6382)[0m  10%|█         | 5/50 [00:54<05:12,  6.94s/it]
[36m(task, pid=6382)[0m  16%|█▌        | 8/50 [00:55<02:19,  3.33s/it]
[36m(task, pid=6382)[0m  20%|██        | 10/50 [00:55<01:29,  2.25s/it]
[36m(task, pid=6382)[0m  26%|██▌       | 13/50 [00:55<00:49,  1.33s/it]
[36m(task, pid=6382)[0m  30%|███       | 15/50 [00:55<00:34,  1.03it/s]
[36m(task, pid=6382)[0m  36%|███▌      | 18/50 [00:55<00:19,  1.61it/s]
[36m(task, pid=6382)[0m  40%|████      | 20/50 [00:55<00:14,  2.11it/s]
[36m(task, pid=6382)[0m  46%|████▌     | 23/50 [00:55<00:08,  3.09it/s]
[36m(task, pid=6382)[0m  52%|█████▏    | 26/50 [00:56<00:05,  4.38it/s]
[36m(task, pid=6382)[0m  56%|█████▌    | 28/50 [00:56<00:04,  5.35it/s]
[36m(task, pid=6382)[0m  62%|██████▏   | 31/50 [00:56<00:02,  7.35it/s]
[36m(task, pid=6382)[0m  68%|██████▊   | 34/50 [00:56<00:01,  9.26it/s]
[36m(task, pid=6382)[0m  74%|███████▍  | 37/50 [00:56<00:01, 10.72it/s]
[36m(task, pid=6382)[0m  80%|████████  | 40/50 [00:56<00:00, 12.80it/s]
[36m(task, pid=6382)[0m  84%|████████▍ | 42/50 [00:56<00:00, 13.23it/s]
[36m(task, pid=6382)[0m  90%|█████████ | 45/50 [00:57<00:00, 15.43it/s]
[36m(task, pid=6382)[0m  98%|█████████▊| 49/50 [00:57<00:00, 19.11it/s]
100%|██████████| 50/50 [00:57<00:00,  1.14s/it]
[36m(task, pid=6382)[0m 
[36m(task, pid=6382)[0m ============ Serving Benchmark Result ============
[36m(task, pid=6382)[0m Backend:                                 vllm      
[36m(task, pid=6382)[0m Traffic request rate:                    10.0      
[36m(task, pid=6382)[0m Max reqeuest concurrency:                not set   
[36m(task, pid=6382)[0m Successful requests:                     50        
[36m(task, pid=6382)[0m Benchmark duration (s):                  57.25     
[36m(task, pid=6382)[0m Total input tokens:                      500000    
[36m(task, pid=6382)[0m Total generated tokens:                  25000     
[36m(task, pid=6382)[0m Total generated tokens (retokenized):    24816     
[36m(task, pid=6382)[0m Request throughput (req/s):              0.87      
[36m(task, pid=6382)[0m Input token throughput (tok/s):          8734.00   
[36m(task, pid=6382)[0m Output token throughput (tok/s):         436.70    
[36m(task, pid=6382)[0m Total token throughput (tok/s):          9170.70   
[36m(task, pid=6382)[0m Concurrency:                             47.16     
[36m(task, pid=6382)[0m ----------------End-to-End Latency----------------
[36m(task, pid=6382)[0m Mean E2E Latency (ms):                   54001.24  
[36m(task, pid=6382)[0m Median E2E Latency (ms):                 54146.39  
[36m(task, pid=6382)[0m ---------------Time to First Token----------------
[36m(task, pid=6382)[0m Mean TTFT (ms):                          14620.56  
[36m(task, pid=6382)[0m Median TTFT (ms):                        14881.48  
[36m(task, pid=6382)[0m P99 TTFT (ms):                           27253.65  
[36m(task, pid=6382)[0m ---------------Inter-Token Latency----------------
[36m(task, pid=6382)[0m Mean ITL (ms):                           79.45     
[36m(task, pid=6382)[0m Median ITL (ms):                         51.01     
[36m(task, pid=6382)[0m P95 ITL (ms):                            520.39    
[36m(task, pid=6382)[0m P99 ITL (ms):                            538.48    
[36m(task, pid=6382)[0m Max ITL (ms):                            1596.90   
[36m(task, pid=6382)[0m ==================================================
[36m(task, pid=6382)[0m | Input Tokens | Output Tokens | Output Token Throughput (tok/s) |
[36m(task, pid=6382)[0m |1000 | 2000 | 1052.20 |
[36m(task, pid=6382)[0m |5000 | 1000 | 799.60 |
[36m(task, pid=6382)[0m |10000 | 500 | 436.70 |
[36m(task, pid=6382)[0m benchmark_args=Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=None, dataset_name='random', dataset_path='', model=None, tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=30000, random_output_len=100, random_range_ratio=1.0, request_rate=10.0, max_concurrency=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_seperated=False, flush_cache=False, warmup_requests=1, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
[36m(task, pid=6382)[0m Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='', model='deepseek-ai/DeepSeek-R1', tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=30000, random_output_len=100, random_range_ratio=1.0, request_rate=10.0, max_concurrency=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_seperated=False, flush_cache=False, warmup_requests=1, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
[36m(task, pid=6382)[0m 
[36m(task, pid=6382)[0m If you do not want to randomly sample from a dataset, please use --dataset-name random-ids.
[36m(task, pid=6382)[0m #Input tokens: 1500000
[36m(task, pid=6382)[0m #Output tokens: 5000
[36m(task, pid=6382)[0m Starting warmup with 1 sequences...
[36m(task, pid=6382)[0m Warmup completed with 1 sequences. Starting main benchmark run...
[36m(task, pid=6382)[0m 
[36m(task, pid=6382)[0m   0%|          | 0/50 [00:00<?, ?it/s]
[36m(task, pid=6382)[0m   2%|▏         | 1/50 [00:45<37:31, 45.95s/it]
[36m(task, pid=6382)[0m   4%|▍         | 2/50 [00:48<16:18, 20.39s/it]
[36m(task, pid=6382)[0m   6%|▌         | 3/50 [00:50<09:20, 11.92s/it]
[36m(task, pid=6382)[0m   8%|▊         | 4/50 [00:53<06:22,  8.30s/it]
[36m(task, pid=6382)[0m  10%|█         | 5/50 [00:55<04:44,  6.31s/it]
[36m(task, pid=6382)[0m  12%|█▏        | 6/50 [00:57<03:33,  4.85s/it]
[36m(task, pid=6382)[0m  14%|█▍        | 7/50 [01:00<02:59,  4.16s/it]
[36m(task, pid=6382)[0m  16%|█▌        | 8/50 [01:03<02:36,  3.72s/it]
[36m(task, pid=6382)[0m  18%|█▊        | 9/50 [01:05<02:10,  3.19s/it]
[36m(task, pid=6382)[0m  20%|██        | 10/50 [01:08<02:02,  3.05s/it]
[36m(task, pid=6382)[0m  22%|██▏       | 11/50 [01:10<01:55,  2.97s/it]
[36m(task, pid=6382)[0m  24%|██▍       | 12/50 [01:12<01:41,  2.68s/it]
[36m(task, pid=6382)[0m  26%|██▌       | 13/50 [01:15<01:39,  2.70s/it]
[36m(task, pid=6382)[0m  28%|██▊       | 14/50 [01:18<01:38,  2.72s/it]
[36m(task, pid=6382)[0m  30%|███       | 15/50 [01:20<01:27,  2.51s/it]
[36m(task, pid=6382)[0m  32%|███▏      | 16/50 [01:23<01:27,  2.58s/it]
[36m(task, pid=6382)[0m  34%|███▍      | 17/50 [01:26<01:27,  2.65s/it]
[36m(task, pid=6382)[0m  36%|███▌      | 18/50 [01:28<01:18,  2.45s/it]
[36m(task, pid=6382)[0m  38%|███▊      | 19/50 [01:32<01:37,  3.15s/it]
[36m(task, pid=6382)[0m  40%|████      | 20/50 [01:35<01:28,  2.95s/it]
[36m(task, pid=6382)[0m  42%|████▏     | 21/50 [01:37<01:21,  2.81s/it]
[36m(task, pid=6382)[0m  44%|████▍     | 22/50 [01:40<01:15,  2.71s/it]
[36m(task, pid=6382)[0m  46%|████▌     | 23/50 [01:42<01:06,  2.46s/it]
[36m(task, pid=6382)[0m  48%|████▊     | 24/50 [01:44<01:06,  2.55s/it]
[36m(task, pid=6382)[0m  50%|█████     | 25/50 [01:47<01:05,  2.62s/it]
[36m(task, pid=6382)[0m  52%|█████▏    | 26/50 [01:49<00:58,  2.44s/it]
[36m(task, pid=6382)[0m  54%|█████▍    | 27/50 [01:52<00:58,  2.53s/it]
[36m(task, pid=6382)[0m  56%|█████▌    | 28/50 [01:55<00:57,  2.61s/it]
[36m(task, pid=6382)[0m  58%|█████▊    | 29/50 [01:57<00:51,  2.43s/it]
[36m(task, pid=6382)[0m  60%|██████    | 30/50 [01:59<00:50,  2.53s/it]
[36m(task, pid=6382)[0m  62%|██████▏   | 31/50 [02:02<00:49,  2.60s/it]
[36m(task, pid=6382)[0m  64%|██████▍   | 32/50 [02:04<00:43,  2.42s/it]
[36m(task, pid=6382)[0m  66%|██████▌   | 33/50 [02:07<00:42,  2.52s/it]
[36m(task, pid=6382)[0m  68%|██████▊   | 34/50 [02:08<00:30,  1.94s/it]
[36m(task, pid=6382)[0m  70%|███████   | 35/50 [02:08<00:21,  1.40s/it]
[36m(task, pid=6382)[0m  72%|███████▏  | 36/50 [02:08<00:14,  1.04s/it]
[36m(task, pid=6382)[0m  74%|███████▍  | 37/50 [02:10<00:17,  1.31s/it]
[36m(task, pid=6382)[0m  76%|███████▌  | 38/50 [02:10<00:11,  1.03it/s]
[36m(task, pid=6382)[0m  78%|███████▊  | 39/50 [02:10<00:08,  1.36it/s]
[36m(task, pid=6382)[0m  80%|████████  | 40/50 [02:10<00:05,  1.75it/s]
[36m(task, pid=6382)[0m  82%|████████▏ | 41/50 [02:11<00:04,  2.19it/s]
[36m(task, pid=6382)[0m  84%|████████▍ | 42/50 [02:11<00:03,  2.66it/s]
[36m(task, pid=6382)[0m  86%|████████▌ | 43/50 [02:11<00:02,  3.41it/s]
[36m(task, pid=6382)[0m  88%|████████▊ | 44/50 [02:11<00:01,  4.11it/s]
[36m(task, pid=6382)[0m  90%|█████████ | 45/50 [02:11<00:01,  4.79it/s]
[36m(task, pid=6382)[0m  94%|█████████▍| 47/50 [02:11<00:00,  6.12it/s]
[36m(task, pid=6382)[0m  96%|█████████▌| 48/50 [02:12<00:00,  6.49it/s]
100%|██████████| 50/50 [02:12<00:00,  7.36it/s]
100%|██████████| 50/50 [02:12<00:00,  2.64s/it]
[36m(task, pid=6382)[0m 
[36m(task, pid=6382)[0m ============ Serving Benchmark Result ============
[36m(task, pid=6382)[0m Backend:                                 vllm      
[36m(task, pid=6382)[0m Traffic request rate:                    10.0      
[36m(task, pid=6382)[0m Max reqeuest concurrency:                not set   
[36m(task, pid=6382)[0m Successful requests:                     50        
[36m(task, pid=6382)[0m Benchmark duration (s):                  132.22    
[36m(task, pid=6382)[0m Total input tokens:                      1500000   
[36m(task, pid=6382)[0m Total generated tokens:                  5000      
[36m(task, pid=6382)[0m Total generated tokens (retokenized):    4960      
[36m(task, pid=6382)[0m Request throughput (req/s):              0.38      
[36m(task, pid=6382)[0m Input token throughput (tok/s):          11344.50  
[36m(task, pid=6382)[0m Output token throughput (tok/s):         37.82     
[36m(task, pid=6382)[0m Total token throughput (tok/s):          11382.32  
[36m(task, pid=6382)[0m Concurrency:                             37.64     
[36m(task, pid=6382)[0m ----------------End-to-End Latency----------------
[36m(task, pid=6382)[0m Mean E2E Latency (ms):                   99548.06  
[36m(task, pid=6382)[0m Median E2E Latency (ms):                 106739.05 
[36m(task, pid=6382)[0m ---------------Time to First Token----------------
[36m(task, pid=6382)[0m Mean TTFT (ms):                          62988.26  
[36m(task, pid=6382)[0m Median TTFT (ms):                        63173.23  
[36m(task, pid=6382)[0m P99 TTFT (ms):                           122054.24 
[36m(task, pid=6382)[0m ---------------Inter-Token Latency----------------
[36m(task, pid=6382)[0m Mean ITL (ms):                           371.47    
[36m(task, pid=6382)[0m Median ITL (ms):                         495.43    
[36m(task, pid=6382)[0m P95 ITL (ms):                            782.85    
[36m(task, pid=6382)[0m P99 ITL (ms):                            822.89    
[36m(task, pid=6382)[0m Max ITL (ms):                            2133.89   
[36m(task, pid=6382)[0m ==================================================
[36m(task, pid=6382)[0m | Input Tokens | Output Tokens | Output Token Throughput (tok/s) |
[36m(task, pid=6382)[0m |1000 | 2000 | 1052.20 |
[36m(task, pid=6382)[0m |5000 | 1000 | 799.60 |
[36m(task, pid=6382)[0m |10000 | 500 | 436.70 |
[36m(task, pid=6382)[0m |30000 | 100 | 37.82 |
[36m(task, pid=6382)[0m Benchmark results for vllm on deepseek-ai/DeepSeek-R1:
[36m(task, pid=6382)[0m | Input Tokens | Output Tokens | Output Token Throughput (tok/s) |
[36m(task, pid=6382)[0m |1000 | 2000 | 1052.20 |
[36m(task, pid=6382)[0m |5000 | 1000 | 799.60 |
[36m(task, pid=6382)[0m |10000 | 500 | 436.70 |
[36m(task, pid=6382)[0m |30000 | 100 | 37.82 |
[0m[32m✓ Job finished (status: SUCCEEDED).[0m
