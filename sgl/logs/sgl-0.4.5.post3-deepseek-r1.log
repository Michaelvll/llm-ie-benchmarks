[33mTailing logs of job 45 on cluster 'benchmark'...[0m
[2m├── [0m[2mWaiting for task resources on 1 node.[0m
[2m└── [0mJob started. Streaming logs... [2m(Ctrl-C to exit log streaming; job will not be killed)[0m
[36m(task, pid=6695)[0m Using Python 3.10.13 environment at: /home/ubuntu/miniconda3
[36m(task, pid=6695)[0m Resolved 138 packages in 490ms
[36m(task, pid=6695)[0m Downloading sglang (1.2MiB)
[36m(task, pid=6695)[0m  Downloaded sglang
[36m(task, pid=6695)[0m Prepared 1 package in 139ms
[36m(task, pid=6695)[0m Uninstalled 2 packages in 55ms
[36m(task, pid=6695)[0m Installed 2 packages in 21ms
[36m(task, pid=6695)[0m  - sglang==0.4.5.post2
[36m(task, pid=6695)[0m  + sglang==0.4.5.post3
[36m(task, pid=6695)[0m  - xgrammar==0.1.18
[36m(task, pid=6695)[0m  + xgrammar==0.1.17
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m Waiting for sgl server to start...
[36m(task, pid=6695)[0m sgl server started
[36m(task, pid=6695)[0m == Running retry 1 ==
[36m(task, pid=6695)[0m benchmark_args=Namespace(backend='sglang-oai', base_url=None, host='0.0.0.0', port=None, dataset_name='random', dataset_path='', model=None, tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1000, random_output_len=2000, random_range_ratio=1.0, request_rate=10.0, max_concurrency=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_seperated=False, flush_cache=False, warmup_requests=1, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
[36m(task, pid=6695)[0m Namespace(backend='sglang-oai', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='', model='deepseek-ai/DeepSeek-R1', tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1000, random_output_len=2000, random_range_ratio=1.0, request_rate=10.0, max_concurrency=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_seperated=False, flush_cache=False, warmup_requests=1, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m Downloading from https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json to /tmp/ShareGPT_V3_unfiltered_cleaned_split.json
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   0%|          | 0.00/642M [00:00<?, ?B/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   1%|          | 4.18M/642M [00:00<00:15, 43.6MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   2%|▏         | 11.2M/642M [00:00<00:10, 60.9MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   3%|▎         | 19.3M/642M [00:00<00:09, 72.1MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   4%|▍         | 27.3M/642M [00:00<00:08, 76.4MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   6%|▌         | 35.4M/642M [00:00<00:07, 79.7MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   7%|▋         | 43.8M/642M [00:00<00:07, 82.5MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   8%|▊         | 52.4M/642M [00:00<00:07, 84.7MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  10%|▉         | 61.1M/642M [00:00<00:07, 86.9MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  11%|█         | 69.9M/642M [00:00<00:06, 88.3MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  12%|█▏        | 78.3M/642M [00:01<00:06, 87.4MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  13%|█▎        | 86.6M/642M [00:01<00:06, 87.1MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  15%|█▍        | 95.0M/642M [00:01<00:06, 87.3MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  16%|█▌        | 103M/642M [00:01<00:06, 86.7MB/s] 
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  17%|█▋        | 112M/642M [00:01<00:06, 86.7MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  19%|█▊        | 120M/642M [00:01<00:06, 87.4MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  20%|██        | 129M/642M [00:01<00:06, 87.3MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  21%|██▏       | 137M/642M [00:01<00:06, 86.2MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  23%|██▎       | 145M/642M [00:01<00:05, 87.4MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  24%|██▍       | 154M/642M [00:01<00:05, 87.4MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  25%|██▌       | 162M/642M [00:02<00:05, 85.1MB/s]
/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  27%|██▋       | 171M/642M [00:02<00:05, 86.5MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  28%|██▊       | 179M/642M [00:02<00:05, 87.6MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  29%|██▉       | 188M/642M [00:02<00:05, 86.3MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  31%|███       | 196M/642M [00:02<00:05, 87.5MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  32%|███▏      | 205M/642M [00:02<00:05, 86.9MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  33%|███▎      | 213M/642M [00:02<00:05, 87.9MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  35%|███▍      | 222M/642M [00:02<00:04, 89.2MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  36%|███▌      | 231M/642M [00:02<00:04, 89.9MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  37%|███▋      | 239M/642M [00:02<00:04, 89.4MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  39%|███▊      | 248M/642M [00:03<00:04, 88.7MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  40%|███▉      | 256M/642M [00:03<00:04, 88.2MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  41%|████▏     | 265M/642M [00:03<00:04, 87.4MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  43%|████▎     | 273M/642M [00:03<00:04, 87.2MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  44%|████▍     | 282M/642M [00:03<00:04, 88.0MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  45%|████▌     | 290M/642M [00:03<00:04, 87.2MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  46%|████▋     | 298M/642M [00:03<00:04, 86.9MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  48%|████▊     | 307M/642M [00:03<00:04, 86.0MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  49%|████▉     | 315M/642M [00:03<00:03, 86.5MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  50%|█████     | 323M/642M [00:03<00:03, 86.7MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  52%|█████▏    | 332M/642M [00:04<00:04, 80.6MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  53%|█████▎    | 340M/642M [00:04<00:03, 81.5MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  54%|█████▍    | 347M/642M [00:04<00:03, 80.1MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  55%|█████▌    | 356M/642M [00:04<00:03, 81.3MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  57%|█████▋    | 364M/642M [00:04<00:03, 82.6MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  58%|█████▊    | 372M/642M [00:04<00:03, 82.6MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  59%|█████▉    | 380M/642M [00:04<00:03, 81.3MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  61%|██████    | 388M/642M [00:04<00:03, 84.1MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  62%|██████▏   | 397M/642M [00:04<00:03, 85.4MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  63%|██████▎   | 405M/642M [00:05<00:03, 82.5MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  64%|██████▍   | 413M/642M [00:05<00:02, 84.6MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  66%|██████▌   | 422M/642M [00:05<00:02, 85.6MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  67%|██████▋   | 430M/642M [00:05<00:02, 86.9MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  68%|██████▊   | 439M/642M [00:05<00:02, 86.8MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  70%|██████▉   | 447M/642M [00:05<00:02, 87.2MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  71%|███████   | 456M/642M [00:05<00:02, 86.7MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  72%|███████▏  | 464M/642M [00:05<00:02, 87.5MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  74%|███████▎  | 472M/642M [00:05<00:02, 87.4MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  75%|███████▍  | 481M/642M [00:05<00:01, 88.1MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  76%|███████▋  | 489M/642M [00:06<00:01, 87.8MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  78%|███████▊  | 498M/642M [00:06<00:01, 87.5MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  79%|███████▉  | 506M/642M [00:06<00:01, 85.8MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  80%|████████  | 514M/642M [00:06<00:01, 85.7MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  81%|████████▏ | 523M/642M [00:06<00:01, 86.8MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  83%|████████▎ | 531M/642M [00:06<00:01, 86.5MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  84%|████████▍ | 540M/642M [00:06<00:01, 87.3MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  85%|████████▌ | 548M/642M [00:06<00:01, 87.2MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  87%|████████▋ | 556M/642M [00:06<00:01, 87.4MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  88%|████████▊ | 565M/642M [00:06<00:00, 87.1MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  89%|████████▉ | 573M/642M [00:07<00:00, 86.5MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  91%|█████████ | 582M/642M [00:07<00:00, 87.7MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  92%|█████████▏| 590M/642M [00:07<00:00, 87.1MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  93%|█████████▎| 599M/642M [00:07<00:00, 87.9MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  95%|█████████▍| 607M/642M [00:07<00:00, 88.4MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  96%|█████████▌| 616M/642M [00:07<00:00, 88.9MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  97%|█████████▋| 624M/642M [00:07<00:00, 86.5MB/s]
[36m(task, pid=6695)[0m /tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  99%|█████████▊| 633M/642M [00:07<00:00, 87.5MB/s]
/tmp/ShareGPT_V3_unfiltered_cleaned_split.json: 100%|█████████▉| 641M/642M [00:07<00:00, 87.2MB/s]
/tmp/ShareGPT_V3_unfiltered_cleaned_split.json: 100%|██████████| 642M/642M [00:07<00:00, 85.8MB/s]
[36m(task, pid=6695)[0m #Input tokens: 50000
[36m(task, pid=6695)[0m #Output tokens: 100000
[36m(task, pid=6695)[0m Starting warmup with 1 sequences...
[36m(task, pid=6695)[0m Warmup completed with 1 sequences. Starting main benchmark run...
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m   0%|          | 0/50 [00:00<?, ?it/s]
  2%|▏         | 1/50 [01:17<1:03:14, 77.45s/it]
100%|██████████| 50/50 [01:17<00:00,  1.55s/it] 
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m ============ Serving Benchmark Result ============
[36m(task, pid=6695)[0m Backend:                                 sglang-oai
[36m(task, pid=6695)[0m Traffic request rate:                    10.0      
[36m(task, pid=6695)[0m Max reqeuest concurrency:                not set   
[36m(task, pid=6695)[0m Successful requests:                     50        
[36m(task, pid=6695)[0m Benchmark duration (s):                  77.52     
[36m(task, pid=6695)[0m Total input tokens:                      50000     
[36m(task, pid=6695)[0m Total generated tokens:                  100000    
[36m(task, pid=6695)[0m Total generated tokens (retokenized):    99638     
[36m(task, pid=6695)[0m Request throughput (req/s):              0.65      
[36m(task, pid=6695)[0m Input token throughput (tok/s):          645.01    
[36m(task, pid=6695)[0m Output token throughput (tok/s):         1290.03   
[36m(task, pid=6695)[0m Total token throughput (tok/s):          1935.04   
[36m(task, pid=6695)[0m Concurrency:                             48.67     
[36m(task, pid=6695)[0m ----------------End-to-End Latency----------------
[36m(task, pid=6695)[0m Mean E2E Latency (ms):                   75457.80  
[36m(task, pid=6695)[0m Median E2E Latency (ms):                 75573.51  
[36m(task, pid=6695)[0m ---------------Time to First Token----------------
[36m(task, pid=6695)[0m Mean TTFT (ms):                          2114.78   
[36m(task, pid=6695)[0m Median TTFT (ms):                        2068.77   
[36m(task, pid=6695)[0m P99 TTFT (ms):                           5372.63   
[36m(task, pid=6695)[0m ---------------Inter-Token Latency----------------
[36m(task, pid=6695)[0m Mean ITL (ms):                           36.81     
[36m(task, pid=6695)[0m Median ITL (ms):                         35.53     
[36m(task, pid=6695)[0m P95 ITL (ms):                            36.38     
[36m(task, pid=6695)[0m P99 ITL (ms):                            37.09     
[36m(task, pid=6695)[0m Max ITL (ms):                            6409.64   
[36m(task, pid=6695)[0m ==================================================
[36m(task, pid=6695)[0m benchmark_args=Namespace(backend='sglang-oai', base_url=None, host='0.0.0.0', port=None, dataset_name='random', dataset_path='', model=None, tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=5000, random_output_len=1000, random_range_ratio=1.0, request_rate=10.0, max_concurrency=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_seperated=False, flush_cache=False, warmup_requests=1, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
[36m(task, pid=6695)[0m Namespace(backend='sglang-oai', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='', model='deepseek-ai/DeepSeek-R1', tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=5000, random_output_len=1000, random_range_ratio=1.0, request_rate=10.0, max_concurrency=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_seperated=False, flush_cache=False, warmup_requests=1, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m #Input tokens: 250000
[36m(task, pid=6695)[0m #Output tokens: 50000
[36m(task, pid=6695)[0m Starting warmup with 1 sequences...
[36m(task, pid=6695)[0m Warmup completed with 1 sequences. Starting main benchmark run...
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m   0%|          | 0/50 [00:00<?, ?it/s]
[36m(task, pid=6695)[0m   2%|▏         | 1/50 [00:52<42:45, 52.35s/it]
100%|██████████| 50/50 [00:52<00:00,  1.05s/it]
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m ============ Serving Benchmark Result ============
[36m(task, pid=6695)[0m Backend:                                 sglang-oai
[36m(task, pid=6695)[0m Traffic request rate:                    10.0      
[36m(task, pid=6695)[0m Max reqeuest concurrency:                not set   
[36m(task, pid=6695)[0m Successful requests:                     50        
[36m(task, pid=6695)[0m Benchmark duration (s):                  52.43     
[36m(task, pid=6695)[0m Total input tokens:                      250000    
[36m(task, pid=6695)[0m Total generated tokens:                  50000     
[36m(task, pid=6695)[0m Total generated tokens (retokenized):    49850     
[36m(task, pid=6695)[0m Request throughput (req/s):              0.95      
[36m(task, pid=6695)[0m Input token throughput (tok/s):          4768.41   
[36m(task, pid=6695)[0m Output token throughput (tok/s):         953.68    
[36m(task, pid=6695)[0m Total token throughput (tok/s):          5722.09   
[36m(task, pid=6695)[0m Concurrency:                             48.03     
[36m(task, pid=6695)[0m ----------------End-to-End Latency----------------
[36m(task, pid=6695)[0m Mean E2E Latency (ms):                   50367.48  
[36m(task, pid=6695)[0m Median E2E Latency (ms):                 50484.17  
[36m(task, pid=6695)[0m ---------------Time to First Token----------------
[36m(task, pid=6695)[0m Mean TTFT (ms):                          5119.10   
[36m(task, pid=6695)[0m Median TTFT (ms):                        5149.77   
[36m(task, pid=6695)[0m P99 TTFT (ms):                           9858.42   
[36m(task, pid=6695)[0m ---------------Inter-Token Latency----------------
[36m(task, pid=6695)[0m Mean ITL (ms):                           45.42     
[36m(task, pid=6695)[0m Median ITL (ms):                         39.63     
[36m(task, pid=6695)[0m P95 ITL (ms):                            40.37     
[36m(task, pid=6695)[0m P99 ITL (ms):                            41.46     
[36m(task, pid=6695)[0m Max ITL (ms):                            12435.10  
[36m(task, pid=6695)[0m ==================================================
[36m(task, pid=6695)[0m benchmark_args=Namespace(backend='sglang-oai', base_url=None, host='0.0.0.0', port=None, dataset_name='random', dataset_path='', model=None, tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=10000, random_output_len=500, random_range_ratio=1.0, request_rate=10.0, max_concurrency=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_seperated=False, flush_cache=False, warmup_requests=1, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
[36m(task, pid=6695)[0m Namespace(backend='sglang-oai', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='', model='deepseek-ai/DeepSeek-R1', tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=10000, random_output_len=500, random_range_ratio=1.0, request_rate=10.0, max_concurrency=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_seperated=False, flush_cache=False, warmup_requests=1, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m #Input tokens: 500000
[36m(task, pid=6695)[0m #Output tokens: 25000
[36m(task, pid=6695)[0m Starting warmup with 1 sequences...
[36m(task, pid=6695)[0m Warmup completed with 1 sequences. Starting main benchmark run...
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m   0%|          | 0/50 [00:00<?, ?it/s]
  2%|▏         | 1/50 [00:52<42:28, 52.02s/it]
100%|██████████| 50/50 [00:52<00:00,  1.04s/it]
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m ============ Serving Benchmark Result ============
[36m(task, pid=6695)[0m Backend:                                 sglang-oai
[36m(task, pid=6695)[0m Traffic request rate:                    10.0      
[36m(task, pid=6695)[0m Max reqeuest concurrency:                not set   
[36m(task, pid=6695)[0m Successful requests:                     50        
[36m(task, pid=6695)[0m Benchmark duration (s):                  52.05     
[36m(task, pid=6695)[0m Total input tokens:                      500000    
[36m(task, pid=6695)[0m Total generated tokens:                  25000     
[36m(task, pid=6695)[0m Total generated tokens (retokenized):    24822     
[36m(task, pid=6695)[0m Request throughput (req/s):              0.96      
[36m(task, pid=6695)[0m Input token throughput (tok/s):          9605.42   
[36m(task, pid=6695)[0m Output token throughput (tok/s):         480.27    
[36m(task, pid=6695)[0m Total token throughput (tok/s):          10085.69  
[36m(task, pid=6695)[0m Concurrency:                             48.02     
[36m(task, pid=6695)[0m ----------------End-to-End Latency----------------
[36m(task, pid=6695)[0m Mean E2E Latency (ms):                   49988.17  
[36m(task, pid=6695)[0m Median E2E Latency (ms):                 50102.98  
[36m(task, pid=6695)[0m ---------------Time to First Token----------------
[36m(task, pid=6695)[0m Mean TTFT (ms):                          13593.77  
[36m(task, pid=6695)[0m Median TTFT (ms):                        13722.02  
[36m(task, pid=6695)[0m P99 TTFT (ms):                           26323.54  
[36m(task, pid=6695)[0m ---------------Inter-Token Latency----------------
[36m(task, pid=6695)[0m Mean ITL (ms):                           73.38     
[36m(task, pid=6695)[0m Median ITL (ms):                         45.34     
[36m(task, pid=6695)[0m P95 ITL (ms):                            46.24     
[36m(task, pid=6695)[0m P99 ITL (ms):                            48.33     
[36m(task, pid=6695)[0m Max ITL (ms):                            28610.82  
[36m(task, pid=6695)[0m ==================================================
[36m(task, pid=6695)[0m benchmark_args=Namespace(backend='sglang-oai', base_url=None, host='0.0.0.0', port=None, dataset_name='random', dataset_path='', model=None, tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=30000, random_output_len=100, random_range_ratio=1.0, request_rate=10.0, max_concurrency=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_seperated=False, flush_cache=False, warmup_requests=1, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
[36m(task, pid=6695)[0m Namespace(backend='sglang-oai', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='', model='deepseek-ai/DeepSeek-R1', tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=30000, random_output_len=100, random_range_ratio=1.0, request_rate=10.0, max_concurrency=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_seperated=False, flush_cache=False, warmup_requests=1, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m #Input tokens: 1500000
[36m(task, pid=6695)[0m #Output tokens: 5000
[36m(task, pid=6695)[0m Starting warmup with 1 sequences...
[36m(task, pid=6695)[0m Warmup completed with 1 sequences. Starting main benchmark run...
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m   0%|          | 0/50 [00:00<?, ?it/s]
[36m(task, pid=6695)[0m   2%|▏         | 1/50 [00:43<35:46, 43.81s/it]
[36m(task, pid=6695)[0m  44%|████▍     | 22/50 [01:27<01:36,  3.44s/it]
 86%|████████▌ | 43/50 [01:45<00:13,  1.93s/it]
100%|██████████| 50/50 [01:45<00:00,  2.11s/it]
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m ============ Serving Benchmark Result ============
[36m(task, pid=6695)[0m Backend:                                 sglang-oai
[36m(task, pid=6695)[0m Traffic request rate:                    10.0      
[36m(task, pid=6695)[0m Max reqeuest concurrency:                not set   
[36m(task, pid=6695)[0m Successful requests:                     50        
[36m(task, pid=6695)[0m Benchmark duration (s):                  105.55    
[36m(task, pid=6695)[0m Total input tokens:                      1500000   
[36m(task, pid=6695)[0m Total generated tokens:                  5000      
[36m(task, pid=6695)[0m Total generated tokens (retokenized):    4956      
[36m(task, pid=6695)[0m Request throughput (req/s):              0.47      
[36m(task, pid=6695)[0m Input token throughput (tok/s):          14211.69  
[36m(task, pid=6695)[0m Output token throughput (tok/s):         47.37     
[36m(task, pid=6695)[0m Total token throughput (tok/s):          14259.06  
[36m(task, pid=6695)[0m Concurrency:                             33.21     
[36m(task, pid=6695)[0m ----------------End-to-End Latency----------------
[36m(task, pid=6695)[0m Mean E2E Latency (ms):                   70100.44  
[36m(task, pid=6695)[0m Median E2E Latency (ms):                 84409.66  
[36m(task, pid=6695)[0m ---------------Time to First Token----------------
[36m(task, pid=6695)[0m Mean TTFT (ms):                          49953.58  
[36m(task, pid=6695)[0m Median TTFT (ms):                        50960.71  
[36m(task, pid=6695)[0m P99 TTFT (ms):                           97717.95  
[36m(task, pid=6695)[0m ---------------Inter-Token Latency----------------
[36m(task, pid=6695)[0m Mean ITL (ms):                           204.74    
[36m(task, pid=6695)[0m Median ITL (ms):                         36.26     
[36m(task, pid=6695)[0m P95 ITL (ms):                            37.36     
[36m(task, pid=6695)[0m P99 ITL (ms):                            109.47    
[36m(task, pid=6695)[0m Max ITL (ms):                            38097.89  
[36m(task, pid=6695)[0m ==================================================
[36m(task, pid=6695)[0m == Running retry 2 ==
[36m(task, pid=6695)[0m benchmark_args=Namespace(backend='sglang-oai', base_url=None, host='0.0.0.0', port=None, dataset_name='random', dataset_path='', model=None, tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1000, random_output_len=2000, random_range_ratio=1.0, request_rate=10.0, max_concurrency=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_seperated=False, flush_cache=False, warmup_requests=1, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
[36m(task, pid=6695)[0m Namespace(backend='sglang-oai', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='', model='deepseek-ai/DeepSeek-R1', tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1000, random_output_len=2000, random_range_ratio=1.0, request_rate=10.0, max_concurrency=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_seperated=False, flush_cache=False, warmup_requests=1, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m #Input tokens: 50000
[36m(task, pid=6695)[0m #Output tokens: 100000
[36m(task, pid=6695)[0m Starting warmup with 1 sequences...
[36m(task, pid=6695)[0m Warmup completed with 1 sequences. Starting main benchmark run...
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m   0%|          | 0/50 [00:00<?, ?it/s]
[36m(task, pid=6695)[0m   2%|▏         | 1/50 [01:14<1:00:31, 74.11s/it]
[36m(task, pid=6695)[0m  46%|████▌     | 23/50 [01:14<01:01,  2.30s/it] 
[36m(task, pid=6695)[0m  68%|██████▊   | 34/50 [01:14<00:21,  1.36s/it]
[36m(task, pid=6695)[0m  86%|████████▌ | 43/50 [01:14<00:06,  1.07it/s]
100%|██████████| 50/50 [01:14<00:00,  1.50s/it]
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m ============ Serving Benchmark Result ============
[36m(task, pid=6695)[0m Backend:                                 sglang-oai
[36m(task, pid=6695)[0m Traffic request rate:                    10.0      
[36m(task, pid=6695)[0m Max reqeuest concurrency:                not set   
[36m(task, pid=6695)[0m Successful requests:                     50        
[36m(task, pid=6695)[0m Benchmark duration (s):                  74.83     
[36m(task, pid=6695)[0m Total input tokens:                      50000     
[36m(task, pid=6695)[0m Total generated tokens:                  100000    
[36m(task, pid=6695)[0m Total generated tokens (retokenized):    99731     
[36m(task, pid=6695)[0m Request throughput (req/s):              0.67      
[36m(task, pid=6695)[0m Input token throughput (tok/s):          668.18    
[36m(task, pid=6695)[0m Output token throughput (tok/s):         1336.36   
[36m(task, pid=6695)[0m Total token throughput (tok/s):          2004.54   
[36m(task, pid=6695)[0m Concurrency:                             48.34     
[36m(task, pid=6695)[0m ----------------End-to-End Latency----------------
[36m(task, pid=6695)[0m Mean E2E Latency (ms):                   72347.34  
[36m(task, pid=6695)[0m Median E2E Latency (ms):                 72421.53  
[36m(task, pid=6695)[0m ---------------Time to First Token----------------
[36m(task, pid=6695)[0m Mean TTFT (ms):                          319.14    
[36m(task, pid=6695)[0m Median TTFT (ms):                        294.48    
[36m(task, pid=6695)[0m P99 TTFT (ms):                           676.05    
[36m(task, pid=6695)[0m ---------------Inter-Token Latency----------------
[36m(task, pid=6695)[0m Mean ITL (ms):                           36.12     
[36m(task, pid=6695)[0m Median ITL (ms):                         35.12     
[36m(task, pid=6695)[0m P95 ITL (ms):                            36.06     
[36m(task, pid=6695)[0m P99 ITL (ms):                            36.77     
[36m(task, pid=6695)[0m Max ITL (ms):                            888.17    
[36m(task, pid=6695)[0m ==================================================
[36m(task, pid=6695)[0m benchmark_args=Namespace(backend='sglang-oai', base_url=None, host='0.0.0.0', port=None, dataset_name='random', dataset_path='', model=None, tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=5000, random_output_len=1000, random_range_ratio=1.0, request_rate=10.0, max_concurrency=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_seperated=False, flush_cache=False, warmup_requests=1, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
[36m(task, pid=6695)[0m Namespace(backend='sglang-oai', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='', model='deepseek-ai/DeepSeek-R1', tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=5000, random_output_len=1000, random_range_ratio=1.0, request_rate=10.0, max_concurrency=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_seperated=False, flush_cache=False, warmup_requests=1, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m #Input tokens: 250000
[36m(task, pid=6695)[0m #Output tokens: 50000
[36m(task, pid=6695)[0m Starting warmup with 1 sequences...
[36m(task, pid=6695)[0m Warmup completed with 1 sequences. Starting main benchmark run...
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m   0%|          | 0/50 [00:00<?, ?it/s]
  2%|▏         | 1/50 [00:52<42:48, 52.43s/it]
100%|██████████| 50/50 [00:52<00:00,  1.05s/it]
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m ============ Serving Benchmark Result ============
[36m(task, pid=6695)[0m Backend:                                 sglang-oai
[36m(task, pid=6695)[0m Traffic request rate:                    10.0      
[36m(task, pid=6695)[0m Max reqeuest concurrency:                not set   
[36m(task, pid=6695)[0m Successful requests:                     50        
[36m(task, pid=6695)[0m Benchmark duration (s):                  52.51     
[36m(task, pid=6695)[0m Total input tokens:                      250000    
[36m(task, pid=6695)[0m Total generated tokens:                  50000     
[36m(task, pid=6695)[0m Total generated tokens (retokenized):    49850     
[36m(task, pid=6695)[0m Request throughput (req/s):              0.95      
[36m(task, pid=6695)[0m Input token throughput (tok/s):          4761.41   
[36m(task, pid=6695)[0m Output token throughput (tok/s):         952.28    
[36m(task, pid=6695)[0m Total token throughput (tok/s):          5713.70   
[36m(task, pid=6695)[0m Concurrency:                             48.04     
[36m(task, pid=6695)[0m ----------------End-to-End Latency----------------
[36m(task, pid=6695)[0m Mean E2E Latency (ms):                   50443.68  
[36m(task, pid=6695)[0m Median E2E Latency (ms):                 50561.11  
[36m(task, pid=6695)[0m ---------------Time to First Token----------------
[36m(task, pid=6695)[0m Mean TTFT (ms):                          5145.22   
[36m(task, pid=6695)[0m Median TTFT (ms):                        5176.45   
[36m(task, pid=6695)[0m P99 TTFT (ms):                           9901.87   
[36m(task, pid=6695)[0m ---------------Inter-Token Latency----------------
[36m(task, pid=6695)[0m Mean ITL (ms):                           45.47     
[36m(task, pid=6695)[0m Median ITL (ms):                         39.65     
[36m(task, pid=6695)[0m P95 ITL (ms):                            40.44     
[36m(task, pid=6695)[0m P99 ITL (ms):                            41.38     
[36m(task, pid=6695)[0m Max ITL (ms):                            12475.01  
[36m(task, pid=6695)[0m ==================================================
[36m(task, pid=6695)[0m benchmark_args=Namespace(backend='sglang-oai', base_url=None, host='0.0.0.0', port=None, dataset_name='random', dataset_path='', model=None, tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=10000, random_output_len=500, random_range_ratio=1.0, request_rate=10.0, max_concurrency=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_seperated=False, flush_cache=False, warmup_requests=1, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
[36m(task, pid=6695)[0m Namespace(backend='sglang-oai', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='', model='deepseek-ai/DeepSeek-R1', tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=10000, random_output_len=500, random_range_ratio=1.0, request_rate=10.0, max_concurrency=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_seperated=False, flush_cache=False, warmup_requests=1, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m #Input tokens: 500000
[36m(task, pid=6695)[0m #Output tokens: 25000
[36m(task, pid=6695)[0m Starting warmup with 1 sequences...
[36m(task, pid=6695)[0m Warmup completed with 1 sequences. Starting main benchmark run...
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m   0%|          | 0/50 [00:00<?, ?it/s]
  2%|▏         | 1/50 [00:52<42:29, 52.04s/it]
100%|██████████| 50/50 [00:52<00:00,  1.04s/it]
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m ============ Serving Benchmark Result ============
[36m(task, pid=6695)[0m Backend:                                 sglang-oai
[36m(task, pid=6695)[0m Traffic request rate:                    10.0      
[36m(task, pid=6695)[0m Max reqeuest concurrency:                not set   
[36m(task, pid=6695)[0m Successful requests:                     50        
[36m(task, pid=6695)[0m Benchmark duration (s):                  52.07     
[36m(task, pid=6695)[0m Total input tokens:                      500000    
[36m(task, pid=6695)[0m Total generated tokens:                  25000     
[36m(task, pid=6695)[0m Total generated tokens (retokenized):    24822     
[36m(task, pid=6695)[0m Request throughput (req/s):              0.96      
[36m(task, pid=6695)[0m Input token throughput (tok/s):          9601.66   
[36m(task, pid=6695)[0m Output token throughput (tok/s):         480.08    
[36m(task, pid=6695)[0m Total token throughput (tok/s):          10081.74  
[36m(task, pid=6695)[0m Concurrency:                             48.02     
[36m(task, pid=6695)[0m ----------------End-to-End Latency----------------
[36m(task, pid=6695)[0m Mean E2E Latency (ms):                   50010.41  
[36m(task, pid=6695)[0m Median E2E Latency (ms):                 50124.85  
[36m(task, pid=6695)[0m ---------------Time to First Token----------------
[36m(task, pid=6695)[0m Mean TTFT (ms):                          13707.32  
[36m(task, pid=6695)[0m Median TTFT (ms):                        13851.43  
[36m(task, pid=6695)[0m P99 TTFT (ms):                           26312.30  
[36m(task, pid=6695)[0m ---------------Inter-Token Latency----------------
[36m(task, pid=6695)[0m Mean ITL (ms):                           73.19     
[36m(task, pid=6695)[0m Median ITL (ms):                         45.38     
[36m(task, pid=6695)[0m P95 ITL (ms):                            46.33     
[36m(task, pid=6695)[0m P99 ITL (ms):                            48.79     
[36m(task, pid=6695)[0m Max ITL (ms):                            28593.52  
[36m(task, pid=6695)[0m ==================================================
[36m(task, pid=6695)[0m benchmark_args=Namespace(backend='sglang-oai', base_url=None, host='0.0.0.0', port=None, dataset_name='random', dataset_path='', model=None, tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=30000, random_output_len=100, random_range_ratio=1.0, request_rate=10.0, max_concurrency=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_seperated=False, flush_cache=False, warmup_requests=1, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
[36m(task, pid=6695)[0m Namespace(backend='sglang-oai', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='', model='deepseek-ai/DeepSeek-R1', tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=30000, random_output_len=100, random_range_ratio=1.0, request_rate=10.0, max_concurrency=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_seperated=False, flush_cache=False, warmup_requests=1, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m #Input tokens: 1500000
[36m(task, pid=6695)[0m #Output tokens: 5000
[36m(task, pid=6695)[0m Starting warmup with 1 sequences...
[36m(task, pid=6695)[0m Warmup completed with 1 sequences. Starting main benchmark run...
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m   0%|          | 0/50 [00:00<?, ?it/s]
[36m(task, pid=6695)[0m   2%|▏         | 1/50 [00:43<35:52, 43.92s/it]
[36m(task, pid=6695)[0m  44%|████▍     | 22/50 [01:27<01:36,  3.44s/it]
 86%|████████▌ | 43/50 [01:45<00:13,  1.93s/it]
100%|██████████| 50/50 [01:45<00:00,  2.11s/it]
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m ============ Serving Benchmark Result ============
[36m(task, pid=6695)[0m Backend:                                 sglang-oai
[36m(task, pid=6695)[0m Traffic request rate:                    10.0      
[36m(task, pid=6695)[0m Max reqeuest concurrency:                not set   
[36m(task, pid=6695)[0m Successful requests:                     50        
[36m(task, pid=6695)[0m Benchmark duration (s):                  105.48    
[36m(task, pid=6695)[0m Total input tokens:                      1500000   
[36m(task, pid=6695)[0m Total generated tokens:                  5000      
[36m(task, pid=6695)[0m Total generated tokens (retokenized):    4956      
[36m(task, pid=6695)[0m Request throughput (req/s):              0.47      
[36m(task, pid=6695)[0m Input token throughput (tok/s):          14220.54  
[36m(task, pid=6695)[0m Output token throughput (tok/s):         47.40     
[36m(task, pid=6695)[0m Total token throughput (tok/s):          14267.94  
[36m(task, pid=6695)[0m Concurrency:                             33.27     
[36m(task, pid=6695)[0m ----------------End-to-End Latency----------------
[36m(task, pid=6695)[0m Mean E2E Latency (ms):                   70184.04  
[36m(task, pid=6695)[0m Median E2E Latency (ms):                 84522.06  
[36m(task, pid=6695)[0m ---------------Time to First Token----------------
[36m(task, pid=6695)[0m Mean TTFT (ms):                          50038.02  
[36m(task, pid=6695)[0m Median TTFT (ms):                        51076.19  
[36m(task, pid=6695)[0m P99 TTFT (ms):                           97740.06  
[36m(task, pid=6695)[0m ---------------Inter-Token Latency----------------
[36m(task, pid=6695)[0m Mean ITL (ms):                           204.74    
[36m(task, pid=6695)[0m Median ITL (ms):                         36.31     
[36m(task, pid=6695)[0m P95 ITL (ms):                            37.72     
[36m(task, pid=6695)[0m P99 ITL (ms):                            109.43    
[36m(task, pid=6695)[0m Max ITL (ms):                            38089.19  
[36m(task, pid=6695)[0m ==================================================
[36m(task, pid=6695)[0m == Running retry 3 ==
[36m(task, pid=6695)[0m benchmark_args=Namespace(backend='sglang-oai', base_url=None, host='0.0.0.0', port=None, dataset_name='random', dataset_path='', model=None, tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1000, random_output_len=2000, random_range_ratio=1.0, request_rate=10.0, max_concurrency=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_seperated=False, flush_cache=False, warmup_requests=1, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
[36m(task, pid=6695)[0m Namespace(backend='sglang-oai', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='', model='deepseek-ai/DeepSeek-R1', tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1000, random_output_len=2000, random_range_ratio=1.0, request_rate=10.0, max_concurrency=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_seperated=False, flush_cache=False, warmup_requests=1, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m #Input tokens: 50000
[36m(task, pid=6695)[0m #Output tokens: 100000
[36m(task, pid=6695)[0m Starting warmup with 1 sequences...
[36m(task, pid=6695)[0m Warmup completed with 1 sequences. Starting main benchmark run...
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m   0%|          | 0/50 [00:00<?, ?it/s]
[36m(task, pid=6695)[0m   2%|▏         | 1/50 [01:14<1:00:49, 74.47s/it]
[36m(task, pid=6695)[0m  38%|███▊      | 19/50 [01:14<01:26,  2.79s/it] 
[36m(task, pid=6695)[0m  74%|███████▍  | 37/50 [01:14<00:15,  1.19s/it]
 96%|█████████▌| 48/50 [01:15<00:01,  1.25it/s]
100%|██████████| 50/50 [01:15<00:00,  1.50s/it]
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m ============ Serving Benchmark Result ============
[36m(task, pid=6695)[0m Backend:                                 sglang-oai
[36m(task, pid=6695)[0m Traffic request rate:                    10.0      
[36m(task, pid=6695)[0m Max reqeuest concurrency:                not set   
[36m(task, pid=6695)[0m Successful requests:                     50        
[36m(task, pid=6695)[0m Benchmark duration (s):                  75.24     
[36m(task, pid=6695)[0m Total input tokens:                      50000     
[36m(task, pid=6695)[0m Total generated tokens:                  100000    
[36m(task, pid=6695)[0m Total generated tokens (retokenized):    99677     
[36m(task, pid=6695)[0m Request throughput (req/s):              0.66      
[36m(task, pid=6695)[0m Input token throughput (tok/s):          664.57    
[36m(task, pid=6695)[0m Output token throughput (tok/s):         1329.14   
[36m(task, pid=6695)[0m Total token throughput (tok/s):          1993.72   
[36m(task, pid=6695)[0m Concurrency:                             48.35     
[36m(task, pid=6695)[0m ----------------End-to-End Latency----------------
[36m(task, pid=6695)[0m Mean E2E Latency (ms):                   72748.91  
[36m(task, pid=6695)[0m Median E2E Latency (ms):                 72878.62  
[36m(task, pid=6695)[0m ---------------Time to First Token----------------
[36m(task, pid=6695)[0m Mean TTFT (ms):                          309.30    
[36m(task, pid=6695)[0m Median TTFT (ms):                        285.92    
[36m(task, pid=6695)[0m P99 TTFT (ms):                           642.25    
[36m(task, pid=6695)[0m ---------------Inter-Token Latency----------------
[36m(task, pid=6695)[0m Mean ITL (ms):                           36.34     
[36m(task, pid=6695)[0m Median ITL (ms):                         35.36     
[36m(task, pid=6695)[0m P95 ITL (ms):                            36.21     
[36m(task, pid=6695)[0m P99 ITL (ms):                            36.84     
[36m(task, pid=6695)[0m Max ITL (ms):                            772.52    
[36m(task, pid=6695)[0m ==================================================
[36m(task, pid=6695)[0m benchmark_args=Namespace(backend='sglang-oai', base_url=None, host='0.0.0.0', port=None, dataset_name='random', dataset_path='', model=None, tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=5000, random_output_len=1000, random_range_ratio=1.0, request_rate=10.0, max_concurrency=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_seperated=False, flush_cache=False, warmup_requests=1, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
[36m(task, pid=6695)[0m Namespace(backend='sglang-oai', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='', model='deepseek-ai/DeepSeek-R1', tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=5000, random_output_len=1000, random_range_ratio=1.0, request_rate=10.0, max_concurrency=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_seperated=False, flush_cache=False, warmup_requests=1, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m #Input tokens: 250000
[36m(task, pid=6695)[0m #Output tokens: 50000
[36m(task, pid=6695)[0m Starting warmup with 1 sequences...
[36m(task, pid=6695)[0m Warmup completed with 1 sequences. Starting main benchmark run...
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m   0%|          | 0/50 [00:00<?, ?it/s]
  2%|▏         | 1/50 [00:52<42:50, 52.46s/it]
100%|██████████| 50/50 [00:52<00:00,  1.05s/it]
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m ============ Serving Benchmark Result ============
[36m(task, pid=6695)[0m Backend:                                 sglang-oai
[36m(task, pid=6695)[0m Traffic request rate:                    10.0      
[36m(task, pid=6695)[0m Max reqeuest concurrency:                not set   
[36m(task, pid=6695)[0m Successful requests:                     50        
[36m(task, pid=6695)[0m Benchmark duration (s):                  52.54     
[36m(task, pid=6695)[0m Total input tokens:                      250000    
[36m(task, pid=6695)[0m Total generated tokens:                  50000     
[36m(task, pid=6695)[0m Total generated tokens (retokenized):    49850     
[36m(task, pid=6695)[0m Request throughput (req/s):              0.95      
[36m(task, pid=6695)[0m Input token throughput (tok/s):          4758.19   
[36m(task, pid=6695)[0m Output token throughput (tok/s):         951.64    
[36m(task, pid=6695)[0m Total token throughput (tok/s):          5709.83   
[36m(task, pid=6695)[0m Concurrency:                             48.04     
[36m(task, pid=6695)[0m ----------------End-to-End Latency----------------
[36m(task, pid=6695)[0m Mean E2E Latency (ms):                   50479.70  
[36m(task, pid=6695)[0m Median E2E Latency (ms):                 50596.08  
[36m(task, pid=6695)[0m ---------------Time to First Token----------------
[36m(task, pid=6695)[0m Mean TTFT (ms):                          5150.47   
[36m(task, pid=6695)[0m Median TTFT (ms):                        5180.38   
[36m(task, pid=6695)[0m P99 TTFT (ms):                           9911.70   
[36m(task, pid=6695)[0m ---------------Inter-Token Latency----------------
[36m(task, pid=6695)[0m Mean ITL (ms):                           45.50     
[36m(task, pid=6695)[0m Median ITL (ms):                         39.69     
[36m(task, pid=6695)[0m P95 ITL (ms):                            40.44     
[36m(task, pid=6695)[0m P99 ITL (ms):                            41.38     
[36m(task, pid=6695)[0m Max ITL (ms):                            12483.98  
[36m(task, pid=6695)[0m ==================================================
[36m(task, pid=6695)[0m benchmark_args=Namespace(backend='sglang-oai', base_url=None, host='0.0.0.0', port=None, dataset_name='random', dataset_path='', model=None, tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=10000, random_output_len=500, random_range_ratio=1.0, request_rate=10.0, max_concurrency=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_seperated=False, flush_cache=False, warmup_requests=1, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
[36m(task, pid=6695)[0m Namespace(backend='sglang-oai', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='', model='deepseek-ai/DeepSeek-R1', tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=10000, random_output_len=500, random_range_ratio=1.0, request_rate=10.0, max_concurrency=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_seperated=False, flush_cache=False, warmup_requests=1, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m #Input tokens: 500000
[36m(task, pid=6695)[0m #Output tokens: 25000
[36m(task, pid=6695)[0m Starting warmup with 1 sequences...
[36m(task, pid=6695)[0m Warmup completed with 1 sequences. Starting main benchmark run...
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m   0%|          | 0/50 [00:00<?, ?it/s]
  2%|▏         | 1/50 [00:52<42:32, 52.09s/it]
100%|██████████| 50/50 [00:52<00:00,  1.04s/it]
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m ============ Serving Benchmark Result ============
[36m(task, pid=6695)[0m Backend:                                 sglang-oai
[36m(task, pid=6695)[0m Traffic request rate:                    10.0      
[36m(task, pid=6695)[0m Max reqeuest concurrency:                not set   
[36m(task, pid=6695)[0m Successful requests:                     50        
[36m(task, pid=6695)[0m Benchmark duration (s):                  52.12     
[36m(task, pid=6695)[0m Total input tokens:                      500000    
[36m(task, pid=6695)[0m Total generated tokens:                  25000     
[36m(task, pid=6695)[0m Total generated tokens (retokenized):    24822     
[36m(task, pid=6695)[0m Request throughput (req/s):              0.96      
[36m(task, pid=6695)[0m Input token throughput (tok/s):          9593.77   
[36m(task, pid=6695)[0m Output token throughput (tok/s):         479.69    
[36m(task, pid=6695)[0m Total token throughput (tok/s):          10073.46  
[36m(task, pid=6695)[0m Concurrency:                             48.02     
[36m(task, pid=6695)[0m ----------------End-to-End Latency----------------
[36m(task, pid=6695)[0m Mean E2E Latency (ms):                   50056.13  
[36m(task, pid=6695)[0m Median E2E Latency (ms):                 50170.62  
[36m(task, pid=6695)[0m ---------------Time to First Token----------------
[36m(task, pid=6695)[0m Mean TTFT (ms):                          13724.04  
[36m(task, pid=6695)[0m Median TTFT (ms):                        13873.26  
[36m(task, pid=6695)[0m P99 TTFT (ms):                           26325.59  
[36m(task, pid=6695)[0m ---------------Inter-Token Latency----------------
[36m(task, pid=6695)[0m Mean ITL (ms):                           73.25     
[36m(task, pid=6695)[0m Median ITL (ms):                         45.45     
[36m(task, pid=6695)[0m P95 ITL (ms):                            46.52     
[36m(task, pid=6695)[0m P99 ITL (ms):                            48.87     
[36m(task, pid=6695)[0m Max ITL (ms):                            28611.36  
[36m(task, pid=6695)[0m ==================================================
[36m(task, pid=6695)[0m benchmark_args=Namespace(backend='sglang-oai', base_url=None, host='0.0.0.0', port=None, dataset_name='random', dataset_path='', model=None, tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=30000, random_output_len=100, random_range_ratio=1.0, request_rate=10.0, max_concurrency=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_seperated=False, flush_cache=False, warmup_requests=1, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
[36m(task, pid=6695)[0m Namespace(backend='sglang-oai', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='', model='deepseek-ai/DeepSeek-R1', tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=30000, random_output_len=100, random_range_ratio=1.0, request_rate=10.0, max_concurrency=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_seperated=False, flush_cache=False, warmup_requests=1, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m #Input tokens: 1500000
[36m(task, pid=6695)[0m #Output tokens: 5000
[36m(task, pid=6695)[0m Starting warmup with 1 sequences...
[36m(task, pid=6695)[0m Warmup completed with 1 sequences. Starting main benchmark run...
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m   0%|          | 0/50 [00:00<?, ?it/s]
[36m(task, pid=6695)[0m   2%|▏         | 1/50 [00:43<35:54, 43.96s/it]
[36m(task, pid=6695)[0m  44%|████▍     | 22/50 [01:27<01:36,  3.44s/it]
 86%|████████▌ | 43/50 [01:45<00:13,  1.93s/it]
100%|██████████| 50/50 [01:45<00:00,  2.11s/it]
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m ============ Serving Benchmark Result ============
[36m(task, pid=6695)[0m Backend:                                 sglang-oai
[36m(task, pid=6695)[0m Traffic request rate:                    10.0      
[36m(task, pid=6695)[0m Max reqeuest concurrency:                not set   
[36m(task, pid=6695)[0m Successful requests:                     50        
[36m(task, pid=6695)[0m Benchmark duration (s):                  105.54    
[36m(task, pid=6695)[0m Total input tokens:                      1500000   
[36m(task, pid=6695)[0m Total generated tokens:                  5000      
[36m(task, pid=6695)[0m Total generated tokens (retokenized):    4956      
[36m(task, pid=6695)[0m Request throughput (req/s):              0.47      
[36m(task, pid=6695)[0m Input token throughput (tok/s):          14212.78  
[36m(task, pid=6695)[0m Output token throughput (tok/s):         47.38     
[36m(task, pid=6695)[0m Total token throughput (tok/s):          14260.16  
[36m(task, pid=6695)[0m Concurrency:                             33.27     
[36m(task, pid=6695)[0m ----------------End-to-End Latency----------------
[36m(task, pid=6695)[0m Mean E2E Latency (ms):                   70233.78  
[36m(task, pid=6695)[0m Median E2E Latency (ms):                 84575.33  
[36m(task, pid=6695)[0m ---------------Time to First Token----------------
[36m(task, pid=6695)[0m Mean TTFT (ms):                          50071.37  
[36m(task, pid=6695)[0m Median TTFT (ms):                        51118.12  
[36m(task, pid=6695)[0m P99 TTFT (ms):                           97794.82  
[36m(task, pid=6695)[0m ---------------Inter-Token Latency----------------
[36m(task, pid=6695)[0m Mean ITL (ms):                           204.90    
[36m(task, pid=6695)[0m Median ITL (ms):                         36.38     
[36m(task, pid=6695)[0m P95 ITL (ms):                            37.50     
[36m(task, pid=6695)[0m P99 ITL (ms):                            109.34    
[36m(task, pid=6695)[0m Max ITL (ms):                            38095.44  
[36m(task, pid=6695)[0m ==================================================
[36m(task, pid=6695)[0m == Server logs for sgl on deepseek-ai/DeepSeek-R1 ==
[36m(task, pid=6695)[0m INFO 04-22 05:23:59 [__init__.py:239] Automatically detected platform cuda.
[36m(task, pid=6695)[0m [2025-04-22 05:24:07] server_args=ServerArgs(model_path='deepseek-ai/DeepSeek-R1', tokenizer_path='deepseek-ai/DeepSeek-R1', tokenizer_mode='auto', skip_tokenizer_init=False, enable_tokenizer_batch_encode=False, load_format='auto', trust_remote_code=True, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=None, device='cuda', served_model_name='deepseek-ai/DeepSeek-R1', chat_template=None, completion_template=None, is_embedding=False, revision=None, host='127.0.0.1', port=30000, mem_fraction_static=0.9074083090470262, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, tp_size=8, stream_interval=1, stream_output=False, random_seed=192063012, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=True, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_llama4_multimodal=None, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_deepep_moe=False, deepep_mode='auto', enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=None, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through_selective', flashinfer_mla_disable_ragged=False, warmups=None, moe_dense_tp_size=None, n_share_experts_fusion=0, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_bootstrap_port=8998, disaggregation_transfer_backend='mooncake', disaggregation_ib_device=None)
[36m(task, pid=6695)[0m INFO 04-22 05:24:11 [__init__.py:239] Automatically detected platform cuda.
[36m(task, pid=6695)[0m INFO 04-22 05:24:12 [__init__.py:239] Automatically detected platform cuda.
[36m(task, pid=6695)[0m INFO 04-22 05:24:12 [__init__.py:239] Automatically detected platform cuda.
[36m(task, pid=6695)[0m INFO 04-22 05:24:12 [__init__.py:239] Automatically detected platform cuda.
[36m(task, pid=6695)[0m INFO 04-22 05:24:12 [__init__.py:239] Automatically detected platform cuda.
[36m(task, pid=6695)[0m INFO 04-22 05:24:12 [__init__.py:239] Automatically detected platform cuda.
[36m(task, pid=6695)[0m INFO 04-22 05:24:12 [__init__.py:239] Automatically detected platform cuda.
[36m(task, pid=6695)[0m INFO 04-22 05:24:12 [__init__.py:239] Automatically detected platform cuda.
[36m(task, pid=6695)[0m INFO 04-22 05:24:12 [__init__.py:239] Automatically detected platform cuda.
[36m(task, pid=6695)[0m [2025-04-22 05:24:22 TP5] Attention backend not set. Use fa3 backend by default.
[36m(task, pid=6695)[0m [2025-04-22 05:24:22 TP5] Chunked prefix cache is turned on.
[36m(task, pid=6695)[0m [2025-04-22 05:24:22 TP5] Init torch distributed begin.
[36m(task, pid=6695)[0m [2025-04-22 05:24:23 TP2] Attention backend not set. Use fa3 backend by default.
[36m(task, pid=6695)[0m [2025-04-22 05:24:23 TP2] Chunked prefix cache is turned on.
[36m(task, pid=6695)[0m [2025-04-22 05:24:23 TP2] Init torch distributed begin.
[36m(task, pid=6695)[0m [2025-04-22 05:24:26 TP4] Attention backend not set. Use fa3 backend by default.
[36m(task, pid=6695)[0m [2025-04-22 05:24:26 TP4] Chunked prefix cache is turned on.
[36m(task, pid=6695)[0m [2025-04-22 05:24:26 TP4] Init torch distributed begin.
[36m(task, pid=6695)[0m [2025-04-22 05:24:26 TP6] Attention backend not set. Use fa3 backend by default.
[36m(task, pid=6695)[0m [2025-04-22 05:24:26 TP6] Chunked prefix cache is turned on.
[36m(task, pid=6695)[0m [2025-04-22 05:24:26 TP6] Init torch distributed begin.
[36m(task, pid=6695)[0m [2025-04-22 05:24:27 TP0] Attention backend not set. Use fa3 backend by default.
[36m(task, pid=6695)[0m [2025-04-22 05:24:27 TP0] Chunked prefix cache is turned on.
[36m(task, pid=6695)[0m [2025-04-22 05:24:27 TP0] Init torch distributed begin.
[36m(task, pid=6695)[0m [2025-04-22 05:24:27 TP7] Attention backend not set. Use fa3 backend by default.
[36m(task, pid=6695)[0m [2025-04-22 05:24:27 TP7] Chunked prefix cache is turned on.
[36m(task, pid=6695)[0m [2025-04-22 05:24:27 TP7] Init torch distributed begin.
[36m(task, pid=6695)[0m [2025-04-22 05:24:27 TP3] Attention backend not set. Use fa3 backend by default.
[36m(task, pid=6695)[0m [2025-04-22 05:24:27 TP3] Chunked prefix cache is turned on.
[36m(task, pid=6695)[0m [2025-04-22 05:24:27 TP3] Init torch distributed begin.
[36m(task, pid=6695)[0m [2025-04-22 05:24:27 TP1] Attention backend not set. Use fa3 backend by default.
[36m(task, pid=6695)[0m [2025-04-22 05:24:27 TP1] Chunked prefix cache is turned on.
[36m(task, pid=6695)[0m [2025-04-22 05:24:27 TP1] Init torch distributed begin.
[36m(task, pid=6695)[0m [2025-04-22 05:24:28 TP1] sglang is using nccl==2.21.5
[36m(task, pid=6695)[0m [2025-04-22 05:24:28 TP3] sglang is using nccl==2.21.5
[36m(task, pid=6695)[0m [2025-04-22 05:24:28 TP7] sglang is using nccl==2.21.5
[36m(task, pid=6695)[0m [2025-04-22 05:24:28 TP0] sglang is using nccl==2.21.5
[36m(task, pid=6695)[0m [2025-04-22 05:24:28 TP2] sglang is using nccl==2.21.5
[36m(task, pid=6695)[0m [2025-04-22 05:24:28 TP4] sglang is using nccl==2.21.5
[36m(task, pid=6695)[0m [2025-04-22 05:24:28 TP5] sglang is using nccl==2.21.5
[36m(task, pid=6695)[0m [2025-04-22 05:24:28 TP6] sglang is using nccl==2.21.5
[36m(task, pid=6695)[0m [2025-04-22 05:24:41 TP1] Init torch distributed ends. mem usage=1.96 GB
[36m(task, pid=6695)[0m [2025-04-22 05:24:41 TP7] Init torch distributed ends. mem usage=1.49 GB
[36m(task, pid=6695)[0m [2025-04-22 05:24:41 TP1] Load weight begin. avail mem=137.24 GB
[36m(task, pid=6695)[0m [2025-04-22 05:24:41 TP3] Init torch distributed ends. mem usage=1.96 GB
[36m(task, pid=6695)[0m [2025-04-22 05:24:41 TP6] Init torch distributed ends. mem usage=1.96 GB
[36m(task, pid=6695)[0m [2025-04-22 05:24:41 TP4] Init torch distributed ends. mem usage=1.96 GB
[36m(task, pid=6695)[0m [2025-04-22 05:24:41 TP0] Init torch distributed ends. mem usage=1.87 GB
[36m(task, pid=6695)[0m [2025-04-22 05:24:41 TP5] Init torch distributed ends. mem usage=1.96 GB
[36m(task, pid=6695)[0m [2025-04-22 05:24:41 TP2] Init torch distributed ends. mem usage=1.96 GB
[36m(task, pid=6695)[0m [2025-04-22 05:24:41 TP7] Load weight begin. avail mem=137.71 GB
[36m(task, pid=6695)[0m [2025-04-22 05:24:41 TP3] Load weight begin. avail mem=137.24 GB
[36m(task, pid=6695)[0m [2025-04-22 05:24:41 TP6] Load weight begin. avail mem=137.24 GB
[36m(task, pid=6695)[0m [2025-04-22 05:24:41 TP0] Load weight begin. avail mem=137.33 GB
[36m(task, pid=6695)[0m [2025-04-22 05:24:41 TP4] Load weight begin. avail mem=137.24 GB
[36m(task, pid=6695)[0m [2025-04-22 05:24:41 TP2] Load weight begin. avail mem=137.24 GB
[36m(task, pid=6695)[0m [2025-04-22 05:24:41 TP5] Load weight begin. avail mem=137.24 GB
[36m(task, pid=6695)[0m [2025-04-22 05:24:43 TP5] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.
[36m(task, pid=6695)[0m [2025-04-22 05:24:43 TP4] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.
[36m(task, pid=6695)[0m [2025-04-22 05:24:43 TP5] Deepseek V3/R1 with fp8 can use shared experts fusion optimization when SM version >=90. Shared experts fusion optimization is enabled.
[36m(task, pid=6695)[0m [2025-04-22 05:24:43 TP4] Deepseek V3/R1 with fp8 can use shared experts fusion optimization when SM version >=90. Shared experts fusion optimization is enabled.
[36m(task, pid=6695)[0m [2025-04-22 05:24:43 TP7] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.
[36m(task, pid=6695)[0m [2025-04-22 05:24:43 TP2] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.
[36m(task, pid=6695)[0m [2025-04-22 05:24:43 TP7] Deepseek V3/R1 with fp8 can use shared experts fusion optimization when SM version >=90. Shared experts fusion optimization is enabled.
[36m(task, pid=6695)[0m [2025-04-22 05:24:43 TP2] Deepseek V3/R1 with fp8 can use shared experts fusion optimization when SM version >=90. Shared experts fusion optimization is enabled.
[36m(task, pid=6695)[0m [2025-04-22 05:24:43 TP1] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.
[36m(task, pid=6695)[0m [2025-04-22 05:24:43 TP1] Deepseek V3/R1 with fp8 can use shared experts fusion optimization when SM version >=90. Shared experts fusion optimization is enabled.
[36m(task, pid=6695)[0m [2025-04-22 05:24:43 TP3] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.
[36m(task, pid=6695)[0m [2025-04-22 05:24:43 TP3] Deepseek V3/R1 with fp8 can use shared experts fusion optimization when SM version >=90. Shared experts fusion optimization is enabled.
[36m(task, pid=6695)[0m [2025-04-22 05:24:43 TP0] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.
[36m(task, pid=6695)[0m [2025-04-22 05:24:43 TP6] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.
[36m(task, pid=6695)[0m [2025-04-22 05:24:43 TP6] Deepseek V3/R1 with fp8 can use shared experts fusion optimization when SM version >=90. Shared experts fusion optimization is enabled.
[36m(task, pid=6695)[0m [2025-04-22 05:24:43 TP0] Deepseek V3/R1 with fp8 can use shared experts fusion optimization when SM version >=90. Shared experts fusion optimization is enabled.
[36m(task, pid=6695)[0m [2025-04-22 05:24:43 TP7] Using model weights format ['*.safetensors']
[36m(task, pid=6695)[0m [2025-04-22 05:24:43 TP4] Using model weights format ['*.safetensors']
[36m(task, pid=6695)[0m [2025-04-22 05:24:43 TP5] Using model weights format ['*.safetensors']
[36m(task, pid=6695)[0m [2025-04-22 05:24:43 TP1] Using model weights format ['*.safetensors']
[36m(task, pid=6695)[0m [2025-04-22 05:24:43 TP3] Using model weights format ['*.safetensors']
[36m(task, pid=6695)[0m [2025-04-22 05:24:43 TP2] Using model weights format ['*.safetensors']
[36m(task, pid=6695)[0m [2025-04-22 05:24:44 TP6] Using model weights format ['*.safetensors']
[36m(task, pid=6695)[0m [2025-04-22 05:24:44 TP0] Using model weights format ['*.safetensors']
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:00<00:12, 12.50it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:01<00:35,  4.38it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:01<00:43,  3.60it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:02<00:54,  2.83it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:02<01:06,  2.33it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:03<01:14,  2.05it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:03<01:04,  2.36it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:04<01:14,  2.02it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:05<01:21,  1.84it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:05<01:26,  1.72it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:06<01:29,  1.65it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  10% Completed | 16/163 [00:07<01:30,  1.63it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:07<01:14,  1.96it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:08<01:21,  1.79it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:08<01:23,  1.72it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:09<01:25,  1.68it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:09<01:26,  1.64it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:10<01:26,  1.63it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:11<01:27,  1.59it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:11<01:28,  1.57it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  15% Completed | 25/163 [00:12<01:26,  1.59it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:13<01:27,  1.56it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  17% Completed | 27/163 [00:13<01:26,  1.56it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:14<01:11,  1.89it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  18% Completed | 29/163 [00:14<01:15,  1.77it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  18% Completed | 30/163 [00:15<01:18,  1.70it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  19% Completed | 31/163 [00:15<01:19,  1.65it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  20% Completed | 32/163 [00:16<01:21,  1.61it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  20% Completed | 33/163 [00:17<01:19,  1.63it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  21% Completed | 34/163 [00:17<01:19,  1.61it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  21% Completed | 35/163 [00:18<01:20,  1.58it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  22% Completed | 36/163 [00:19<01:20,  1.58it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  23% Completed | 37/163 [00:19<01:19,  1.58it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  23% Completed | 38/163 [00:20<01:18,  1.60it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  24% Completed | 39/163 [00:21<01:19,  1.56it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  25% Completed | 40/163 [00:21<01:18,  1.57it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  25% Completed | 41/163 [00:22<01:15,  1.61it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  26% Completed | 42/163 [00:22<01:06,  1.82it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  26% Completed | 43/163 [00:23<01:09,  1.73it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  27% Completed | 44/163 [00:23<01:11,  1.67it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  28% Completed | 45/163 [00:24<01:12,  1.63it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  28% Completed | 46/163 [00:25<01:13,  1.59it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  29% Completed | 47/163 [00:25<01:12,  1.59it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  29% Completed | 48/163 [00:26<01:12,  1.58it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  30% Completed | 49/163 [00:27<01:13,  1.55it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  31% Completed | 50/163 [00:27<01:12,  1.55it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  31% Completed | 51/163 [00:28<00:59,  1.89it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  32% Completed | 52/163 [00:28<01:03,  1.75it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  33% Completed | 53/163 [00:29<01:05,  1.69it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  33% Completed | 54/163 [00:30<01:05,  1.65it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  34% Completed | 55/163 [00:30<01:06,  1.62it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  34% Completed | 56/163 [00:31<01:07,  1.58it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  35% Completed | 57/163 [00:32<01:07,  1.57it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  36% Completed | 58/163 [00:32<01:06,  1.57it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  36% Completed | 59/163 [00:33<01:07,  1.55it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  37% Completed | 60/163 [00:34<01:12,  1.41it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  37% Completed | 61/163 [00:34<01:03,  1.60it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  38% Completed | 62/163 [00:35<01:03,  1.60it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  39% Completed | 63/163 [00:35<00:50,  2.00it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  39% Completed | 64/163 [00:36<00:55,  1.80it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  40% Completed | 65/163 [00:36<00:57,  1.70it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  40% Completed | 66/163 [00:37<00:59,  1.63it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  41% Completed | 67/163 [00:38<01:00,  1.59it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  42% Completed | 68/163 [00:38<00:59,  1.58it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  42% Completed | 69/163 [00:39<01:00,  1.56it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  43% Completed | 70/163 [00:40<00:59,  1.55it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  44% Completed | 71/163 [00:40<00:59,  1.54it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  44% Completed | 72/163 [00:41<00:58,  1.55it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  45% Completed | 73/163 [00:42<00:57,  1.58it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  45% Completed | 74/163 [00:42<00:57,  1.55it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  46% Completed | 75/163 [00:43<00:56,  1.54it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  47% Completed | 76/163 [00:43<00:54,  1.59it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  47% Completed | 77/163 [00:44<00:54,  1.57it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  48% Completed | 78/163 [00:45<00:54,  1.55it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  48% Completed | 79/163 [00:45<00:54,  1.54it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  49% Completed | 80/163 [00:46<00:54,  1.52it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  50% Completed | 81/163 [00:47<00:54,  1.51it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  50% Completed | 82/163 [00:47<00:53,  1.50it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  51% Completed | 83/163 [00:48<00:48,  1.64it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  52% Completed | 84/163 [00:49<00:48,  1.62it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  52% Completed | 85/163 [00:49<00:49,  1.59it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  53% Completed | 86/163 [00:50<00:47,  1.62it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  53% Completed | 87/163 [00:50<00:46,  1.62it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  54% Completed | 88/163 [00:51<00:46,  1.60it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  55% Completed | 89/163 [00:52<00:46,  1.59it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  55% Completed | 90/163 [00:52<00:46,  1.56it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  56% Completed | 91/163 [00:53<00:46,  1.54it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  56% Completed | 92/163 [00:54<00:46,  1.54it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  57% Completed | 93/163 [00:54<00:44,  1.58it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  58% Completed | 94/163 [00:55<00:44,  1.57it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  58% Completed | 95/163 [00:56<00:43,  1.57it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  59% Completed | 96/163 [00:56<00:42,  1.59it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  60% Completed | 97/163 [00:57<00:41,  1.58it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  60% Completed | 98/163 [00:57<00:41,  1.56it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  61% Completed | 99/163 [00:58<00:41,  1.55it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  61% Completed | 100/163 [00:59<00:40,  1.56it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  62% Completed | 101/163 [00:59<00:40,  1.55it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  63% Completed | 102/163 [01:00<00:39,  1.54it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  63% Completed | 103/163 [01:01<00:39,  1.54it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  64% Completed | 104/163 [01:01<00:38,  1.52it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  64% Completed | 105/163 [01:02<00:37,  1.53it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  65% Completed | 106/163 [01:03<00:37,  1.54it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  66% Completed | 107/163 [01:03<00:36,  1.55it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  66% Completed | 108/163 [01:04<00:35,  1.53it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  67% Completed | 109/163 [01:05<00:34,  1.55it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  67% Completed | 110/163 [01:05<00:28,  1.89it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  68% Completed | 111/163 [01:05<00:28,  1.83it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  69% Completed | 112/163 [01:06<00:29,  1.76it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  69% Completed | 113/163 [01:07<00:29,  1.70it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  70% Completed | 114/163 [01:07<00:29,  1.64it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  71% Completed | 115/163 [01:08<00:29,  1.62it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  71% Completed | 116/163 [01:09<00:29,  1.62it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  72% Completed | 117/163 [01:09<00:28,  1.62it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  72% Completed | 118/163 [01:10<00:27,  1.61it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  73% Completed | 119/163 [01:11<00:27,  1.57it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  74% Completed | 120/163 [01:11<00:27,  1.56it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  74% Completed | 121/163 [01:12<00:26,  1.59it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  75% Completed | 122/163 [01:12<00:25,  1.58it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  75% Completed | 123/163 [01:13<00:25,  1.58it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  76% Completed | 124/163 [01:14<00:24,  1.60it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  77% Completed | 125/163 [01:14<00:24,  1.57it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  77% Completed | 126/163 [01:15<00:22,  1.65it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  78% Completed | 127/163 [01:16<00:22,  1.62it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  79% Completed | 128/163 [01:16<00:22,  1.58it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  79% Completed | 129/163 [01:17<00:22,  1.54it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  80% Completed | 130/163 [01:18<00:21,  1.55it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  80% Completed | 131/163 [01:18<00:16,  1.89it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  81% Completed | 132/163 [01:18<00:17,  1.80it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  82% Completed | 133/163 [01:19<00:17,  1.72it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  82% Completed | 134/163 [01:20<00:17,  1.67it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  83% Completed | 135/163 [01:20<00:17,  1.64it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  83% Completed | 136/163 [01:21<00:16,  1.61it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  84% Completed | 137/163 [01:22<00:16,  1.59it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  85% Completed | 138/163 [01:22<00:15,  1.56it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  85% Completed | 139/163 [01:23<00:15,  1.56it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  86% Completed | 140/163 [01:24<00:14,  1.55it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  87% Completed | 141/163 [01:24<00:14,  1.56it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  87% Completed | 142/163 [01:25<00:13,  1.55it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  88% Completed | 143/163 [01:26<00:13,  1.53it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  88% Completed | 144/163 [01:26<00:12,  1.55it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  89% Completed | 145/163 [01:27<00:11,  1.57it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  90% Completed | 146/163 [01:27<00:11,  1.54it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  90% Completed | 147/163 [01:28<00:10,  1.57it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  91% Completed | 148/163 [01:29<00:09,  1.57it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  91% Completed | 149/163 [01:29<00:08,  1.57it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  92% Completed | 150/163 [01:30<00:08,  1.53it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  93% Completed | 151/163 [01:31<00:07,  1.53it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  93% Completed | 152/163 [01:31<00:07,  1.53it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  94% Completed | 153/163 [01:32<00:06,  1.54it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  94% Completed | 154/163 [01:33<00:05,  1.56it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  95% Completed | 155/163 [01:33<00:05,  1.56it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  96% Completed | 156/163 [01:34<00:05,  1.37it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  96% Completed | 157/163 [01:35<00:03,  1.60it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  97% Completed | 158/163 [01:35<00:03,  1.64it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  98% Completed | 159/163 [01:36<00:02,  1.60it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  98% Completed | 160/163 [01:36<00:01,  1.56it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  99% Completed | 161/163 [01:37<00:01,  1.56it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards:  99% Completed | 162/163 [01:38<00:00,  1.57it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards: 100% Completed | 163/163 [01:38<00:00,  1.56it/s]
[36m(task, pid=6695)[0m 
Loading safetensors checkpoint shards: 100% Completed | 163/163 [01:38<00:00,  1.65it/s]
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m 
Cloning 8 replicas of the shared expert into MoE:   0%|          | 0/58 [00:00<?, ?it/s]
Cloning 8 replicas of the shared expert into MoE:   0%|          | 0/58 [00:00<?, ?it/s]
Cloning 8 replicas of the shared expert into MoE:   0%|          | 0/58 [00:00<?, ?it/s]
Cloning 8 replicas of the shared expert into MoE:   0%|          | 0/58 [00:00<?, ?it/s]
Cloning 8 replicas of the shared expert into MoE:   0%|          | 0/58 [00:00<?, ?it/s]
Cloning 8 replicas of the shared expert into MoE:   0%|          | 0/58 [00:00<?, ?it/s]
Cloning 8 replicas of the shared expert into MoE:   0%|          | 0/58 [00:00<?, ?it/s]
Cloning 8 replicas of the shared expert into MoE:   0%|          | 0/58 [00:00<?, ?it/s]
Cloning 8 replicas of the shared expert into MoE: 100%|██████████| 58/58 [00:00<00:00, 11387.96it/s]
[36m(task, pid=6695)[0m 
Cloning 8 replicas of the shared expert into MoE: 100%|██████████| 58/58 [00:00<00:00, 11011.66it/s]
[36m(task, pid=6695)[0m 
Cloning 8 replicas of the shared expert into MoE: 100%|██████████| 58/58 [00:00<00:00, 10519.31it/s]
Cloning 8 replicas of the shared expert into MoE: 100%|██████████| 58/58 [00:00<00:00, 10819.68it/s]
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m 
Cloning 8 replicas of the shared expert into MoE: 100%|██████████| 58/58 [00:00<00:00, 10617.56it/s]
Cloning 8 replicas of the shared expert into MoE: 100%|██████████| 58/58 [00:00<00:00, 10521.59it/s]
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m 
Cloning 8 replicas of the shared expert into MoE: 100%|██████████| 58/58 [00:00<00:00, 10374.86it/s]
[36m(task, pid=6695)[0m 
Cloning 8 replicas of the shared expert into MoE: 100%|██████████| 58/58 [00:00<00:00, 9637.88it/s]
[36m(task, pid=6695)[0m [2025-04-22 05:53:40 TP6] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=55.60 GB, mem usage=81.64 GB.
[36m(task, pid=6695)[0m [2025-04-22 05:54:45 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=55.60 GB, mem usage=81.64 GB.
[36m(task, pid=6695)[0m [2025-04-22 05:54:45 TP7] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=56.07 GB, mem usage=81.64 GB.
[36m(task, pid=6695)[0m [2025-04-22 05:54:45 TP5] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=55.60 GB, mem usage=81.64 GB.
[36m(task, pid=6695)[0m [2025-04-22 05:54:45 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=55.60 GB, mem usage=81.64 GB.
[36m(task, pid=6695)[0m [2025-04-22 05:54:45 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=55.70 GB, mem usage=81.64 GB.
[36m(task, pid=6695)[0m [2025-04-22 05:54:45 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=55.60 GB, mem usage=81.64 GB.
[36m(task, pid=6695)[0m [2025-04-22 05:54:45 TP4] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=55.60 GB, mem usage=81.64 GB.
[36m(task, pid=6695)[0m [2025-04-22 05:54:45 TP3] KV Cache is allocated. #tokens: 654280, KV size: 42.82 GB
[36m(task, pid=6695)[0m [2025-04-22 05:54:45 TP7] KV Cache is allocated. #tokens: 654280, KV size: 42.82 GB
[36m(task, pid=6695)[0m [2025-04-22 05:54:45 TP3] Memory pool end. avail mem=11.46 GB
[36m(task, pid=6695)[0m [2025-04-22 05:54:45 TP7] Memory pool end. avail mem=11.93 GB
[36m(task, pid=6695)[0m [2025-04-22 05:54:45 TP4] KV Cache is allocated. #tokens: 654280, KV size: 42.82 GB
[36m(task, pid=6695)[0m [2025-04-22 05:54:45 TP6] KV Cache is allocated. #tokens: 654280, KV size: 42.82 GB
[36m(task, pid=6695)[0m [2025-04-22 05:54:45 TP4] Memory pool end. avail mem=11.46 GB
[36m(task, pid=6695)[0m [2025-04-22 05:54:45 TP6] Memory pool end. avail mem=11.46 GB
[36m(task, pid=6695)[0m [2025-04-22 05:54:45 TP0] KV Cache is allocated. #tokens: 654280, KV size: 42.82 GB
[36m(task, pid=6695)[0m [2025-04-22 05:54:45 TP2] KV Cache is allocated. #tokens: 654280, KV size: 42.82 GB
[36m(task, pid=6695)[0m [2025-04-22 05:54:45 TP2] Memory pool end. avail mem=11.46 GB
[36m(task, pid=6695)[0m [2025-04-22 05:54:45 TP0] Memory pool end. avail mem=11.55 GB
[36m(task, pid=6695)[0m [2025-04-22 05:54:45 TP1] KV Cache is allocated. #tokens: 654280, KV size: 42.82 GB
[36m(task, pid=6695)[0m [2025-04-22 05:54:45 TP1] Memory pool end. avail mem=11.46 GB
[36m(task, pid=6695)[0m [2025-04-22 05:54:45 TP5] KV Cache is allocated. #tokens: 654280, KV size: 42.82 GB
[36m(task, pid=6695)[0m [2025-04-22 05:54:45 TP5] Memory pool end. avail mem=11.46 GB
[36m(task, pid=6695)[0m [2025-04-22 05:54:45 TP5] Capture cuda graph begin. This can take up to several minutes. avail mem=11.37 GB
[36m(task, pid=6695)[0m [2025-04-22 05:54:45 TP3] Capture cuda graph begin. This can take up to several minutes. avail mem=11.37 GB
[36m(task, pid=6695)[0m [2025-04-22 05:54:45 TP2] Capture cuda graph begin. This can take up to several minutes. avail mem=11.37 GB
[36m(task, pid=6695)[0m [2025-04-22 05:54:45 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=11.46 GB
[36m(task, pid=6695)[0m [2025-04-22 05:54:45 TP4] Capture cuda graph begin. This can take up to several minutes. avail mem=11.37 GB
[36m(task, pid=6695)[0m [2025-04-22 05:54:45 TP6] Capture cuda graph begin. This can take up to several minutes. avail mem=11.37 GB
[36m(task, pid=6695)[0m [2025-04-22 05:54:45 TP7] Capture cuda graph begin. This can take up to several minutes. avail mem=11.84 GB
[36m(task, pid=6695)[0m [2025-04-22 05:54:45 TP1] Capture cuda graph begin. This can take up to several minutes. avail mem=11.37 GB
[36m(task, pid=6695)[0m 
  0%|          | 0/35 [00:00<?, ?it/s]
Capturing batches (avail_mem=10.99 GB):   0%|          | 0/35 [00:00<?, ?it/s][2025-04-22 05:54:46 TP1] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=1536,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:54:46 TP5] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=1536,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:54:46 TP6] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=1536,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:54:46 TP0] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=1536,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:54:46 TP4] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=1536,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:54:46 TP2] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=1536,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:54:46 TP3] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=1536,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:54:46 TP0] Entering DeepGEMM JIT Pre-Complie session. And it may takes a long time(Typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. Recommand to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:54:46 TP0] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=1536, K=7168, num_groups=1 with all Ms. It only takes a litte time(Typically 1 sec) if you have run `sglang.compile_deep_gemm`. 
[36m(task, pid=6695)[0m [2025-04-22 05:54:46 TP7] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=1536,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:54:46 TP1] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:54:46 TP3] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:54:46 TP5] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:54:46 TP2] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:54:46 TP7] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:54:46 TP5] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=1536, K=7168. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:54:46 TP6] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:54:46 TP4] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:54:46 TP1] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=1536, K=7168. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:54:46 TP3] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=1536, K=7168. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:54:46 TP2] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=1536, K=7168. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:54:46 TP7] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=1536, K=7168. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:54:46 TP6] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=1536, K=7168. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:54:46 TP4] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=1536, K=7168. Please wait.
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m 
  0%|          | 0/62 [00:00<?, ?it/s][A[2025-04-22 05:54:54 TP4] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=3072,K=1536,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:54:54 TP4] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:54:54 TP4] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=3072, K=1536. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:54:54 TP2] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=3072,K=1536,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:54:54 TP2] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:54:54 TP2] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=3072, K=1536. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:54:54 TP3] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=3072,K=1536,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:54:54 TP3] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:54:54 TP3] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=3072, K=1536. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:54:54 TP6] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=3072,K=1536,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:54:54 TP6] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:54:54 TP6] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=3072, K=1536. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:54:54 TP5] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=3072,K=1536,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:54:54 TP1] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=3072,K=1536,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:54:54 TP7] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=3072,K=1536,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:54:54 TP5] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:54:54 TP5] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=3072, K=1536. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:54:54 TP1] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:54:54 TP1] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=3072, K=1536. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:54:54 TP7] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:54:54 TP7] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=3072, K=1536. Please wait.
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m 
  2%|▏         | 1/62 [00:07<07:25,  7.30s/it][A
[36m(task, pid=6695)[0m 
  3%|▎         | 2/62 [00:07<03:12,  3.21s/it][A
[36m(task, pid=6695)[0m 
  8%|▊         | 5/62 [00:13<02:17,  2.41s/it][A
[36m(task, pid=6695)[0m 
 10%|▉         | 6/62 [00:15<01:57,  2.11s/it][A[2025-04-22 05:55:03 TP1] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:03 TP1] DeepGEMM JIT Compiling for <m_grouped_gemm_fp8_fp8_bf16_nt_masked> M=256, N=512, K=128. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:03 TP3] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:03 TP3] DeepGEMM JIT Compiling for <m_grouped_gemm_fp8_fp8_bf16_nt_masked> M=256, N=512, K=128. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:03 TP7] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:03 TP7] DeepGEMM JIT Compiling for <m_grouped_gemm_fp8_fp8_bf16_nt_masked> M=256, N=512, K=128. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:03 TP2] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:03 TP2] DeepGEMM JIT Compiling for <m_grouped_gemm_fp8_fp8_bf16_nt_masked> M=256, N=512, K=128. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:03 TP6] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:03 TP6] DeepGEMM JIT Compiling for <m_grouped_gemm_fp8_fp8_bf16_nt_masked> M=256, N=512, K=128. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:03 TP5] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:03 TP5] DeepGEMM JIT Compiling for <m_grouped_gemm_fp8_fp8_bf16_nt_masked> M=256, N=512, K=128. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:03 TP4] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:03 TP4] DeepGEMM JIT Compiling for <m_grouped_gemm_fp8_fp8_bf16_nt_masked> M=256, N=512, K=128. Please wait.
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m 
 15%|█▍        | 9/62 [00:21<01:49,  2.07s/it][A
[36m(task, pid=6695)[0m 
 16%|█▌        | 10/62 [00:21<01:30,  1.74s/it][A
[36m(task, pid=6695)[0m 
 18%|█▊        | 11/62 [00:22<01:16,  1.50s/it][A[2025-04-22 05:55:10 TP7] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=576,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:55:10 TP7] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:10 TP7] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=576, K=7168. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:10 TP5] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=576,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:55:10 TP5] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:10 TP5] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=576, K=7168. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:10 TP1] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=576,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:55:10 TP1] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:10 TP1] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=576, K=7168. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:10 TP3] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=576,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:55:10 TP3] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:10 TP3] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=576, K=7168. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:10 TP6] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=576,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:55:10 TP6] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:10 TP6] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=576, K=7168. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:10 TP4] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=576,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:55:10 TP4] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:10 TP4] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=576, K=7168. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:10 TP2] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=576,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:55:10 TP2] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:10 TP2] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=576, K=7168. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:17 TP7] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:17 TP7] DeepGEMM JIT Compiling for <m_grouped_gemm_fp8_fp8_bf16_nt_masked> M=256, N=128, K=512. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:17 TP1] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:17 TP1] DeepGEMM JIT Compiling for <m_grouped_gemm_fp8_fp8_bf16_nt_masked> M=256, N=128, K=512. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:17 TP5] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:17 TP5] DeepGEMM JIT Compiling for <m_grouped_gemm_fp8_fp8_bf16_nt_masked> M=256, N=128, K=512. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:17 TP3] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:17 TP3] DeepGEMM JIT Compiling for <m_grouped_gemm_fp8_fp8_bf16_nt_masked> M=256, N=128, K=512. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:17 TP6] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:17 TP6] DeepGEMM JIT Compiling for <m_grouped_gemm_fp8_fp8_bf16_nt_masked> M=256, N=128, K=512. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:17 TP4] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:17 TP4] DeepGEMM JIT Compiling for <m_grouped_gemm_fp8_fp8_bf16_nt_masked> M=256, N=128, K=512. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:17 TP2] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:17 TP2] DeepGEMM JIT Compiling for <m_grouped_gemm_fp8_fp8_bf16_nt_masked> M=256, N=128, K=512. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:23 TP7] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=7168,K=2048,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:55:23 TP7] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:23 TP7] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=7168, K=2048. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:23 TP1] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=7168,K=2048,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:55:23 TP1] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:23 TP1] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=7168, K=2048. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:23 TP5] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=7168,K=2048,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:55:23 TP5] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:23 TP5] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=7168, K=2048. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:23 TP3] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=7168,K=2048,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:55:23 TP3] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:23 TP3] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=7168, K=2048. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:24 TP6] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=7168,K=2048,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:55:24 TP6] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:24 TP6] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=7168, K=2048. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:24 TP4] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=7168,K=2048,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:55:24 TP4] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:24 TP4] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=7168, K=2048. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:24 TP2] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=7168,K=2048,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:55:24 TP2] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:24 TP2] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=7168, K=2048. Please wait.
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m 
 21%|██        | 13/62 [00:37<03:10,  3.90s/it][A[2025-04-22 05:55:30 TP7] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=4608,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:55:30 TP7] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:30 TP7] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=4608, K=7168. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:30 TP1] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=4608,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:55:30 TP1] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:30 TP1] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=4608, K=7168. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:31 TP5] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=4608,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:55:31 TP5] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:31 TP5] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=4608, K=7168. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:31 TP3] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=4608,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:55:31 TP3] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:31 TP3] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=4608, K=7168. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:31 TP6] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=4608,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:55:31 TP6] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:31 TP6] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=4608, K=7168. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:31 TP4] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=4608,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:55:31 TP4] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:31 TP4] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=4608, K=7168. Please wait.
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m 
 40%|████      | 25/62 [00:44<00:49,  1.33s/it][A[2025-04-22 05:55:31 TP2] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=4608,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:55:31 TP2] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:31 TP2] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=4608, K=7168. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:38 TP1] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=7168,K=2304,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:55:38 TP7] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=7168,K=2304,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:55:38 TP7] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:38 TP1] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:38 TP7] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=7168, K=2304. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:38 TP1] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=7168, K=2304. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:38 TP5] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=7168,K=2304,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:55:38 TP5] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:38 TP5] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=7168, K=2304. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:38 TP3] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=7168,K=2304,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:55:38 TP3] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:38 TP3] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=7168, K=2304. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:39 TP4] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=7168,K=2304,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:55:39 TP4] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:39 TP4] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=7168, K=2304. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:39 TP6] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=7168,K=2304,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:55:39 TP6] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:39 TP6] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=7168, K=2304. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 05:55:39 TP2] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=7168,K=2304,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:55:39 TP2] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:55:39 TP2] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=256, N=7168, K=2304. Please wait.
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m 
 42%|████▏     | 26/62 [00:52<01:07,  1.88s/it][A
[36m(task, pid=6695)[0m 
 48%|████▊     | 30/62 [00:54<00:46,  1.44s/it][A[2025-04-22 05:55:45 TP1] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/E=264,N=256,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for MoE layer.
[36m(task, pid=6695)[0m [2025-04-22 05:55:45 TP7] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/E=264,N=256,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for MoE layer.
[36m(task, pid=6695)[0m [2025-04-22 05:55:45 TP5] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/E=264,N=256,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for MoE layer.
[36m(task, pid=6695)[0m [2025-04-22 05:55:45 TP3] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/E=264,N=256,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for MoE layer.
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m 
 58%|█████▊    | 36/62 [00:58<00:29,  1.14s/it][A[2025-04-22 05:55:46 TP4] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/E=264,N=256,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for MoE layer.
[36m(task, pid=6695)[0m [2025-04-22 05:55:46 TP6] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/E=264,N=256,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for MoE layer.
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m 
 65%|██████▍   | 40/62 [00:59<00:18,  1.17it/s][A[2025-04-22 05:55:46 TP2] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/E=264,N=256,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for MoE layer.
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m 
 66%|██████▌   | 41/62 [01:01<00:20,  1.04it/s][A
[36m(task, pid=6695)[0m 
 69%|██████▉   | 43/62 [01:05<00:22,  1.19s/it][A
[36m(task, pid=6695)[0m 
 73%|███████▎  | 45/62 [01:05<00:17,  1.01s/it][A
[36m(task, pid=6695)[0m 
 77%|███████▋  | 48/62 [01:08<00:13,  1.06it/s][A
[36m(task, pid=6695)[0m 
 79%|███████▉  | 49/62 [01:21<00:34,  2.63s/it][A
[36m(task, pid=6695)[0m 
 94%|█████████▎| 58/62 [01:22<00:04,  1.01s/it][A
[36m(task, pid=6695)[0m 
 95%|█████████▌| 59/62 [01:27<00:04,  1.40s/it][A
100%|██████████| 62/62 [01:27<00:00,  1.42s/it]
[36m(task, pid=6695)[0m [2025-04-22 05:56:15 TP0] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=3072,K=1536,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:56:15 TP0] Entering DeepGEMM JIT Pre-Complie session. And it may takes a long time(Typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. Recommand to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:56:15 TP0] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a litte time(Typically 1 sec) if you have run `sglang.compile_deep_gemm`. 
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m 
  0%|          | 0/36 [00:00<?, ?it/s][A
[36m(task, pid=6695)[0m 
  3%|▎         | 1/36 [00:06<04:04,  6.97s/it][A
[36m(task, pid=6695)[0m 
  6%|▌         | 2/36 [00:07<01:47,  3.16s/it][A
[36m(task, pid=6695)[0m 
 14%|█▍        | 5/36 [00:14<01:18,  2.52s/it][A
[36m(task, pid=6695)[0m 
 22%|██▏       | 8/36 [00:14<00:35,  1.27s/it][A
[36m(task, pid=6695)[0m 
 28%|██▊       | 10/36 [00:22<00:58,  2.25s/it][A
[36m(task, pid=6695)[0m 
 31%|███       | 11/36 [00:23<00:49,  1.97s/it][A
[36m(task, pid=6695)[0m 
 36%|███▌      | 13/36 [00:31<01:01,  2.68s/it][A
[36m(task, pid=6695)[0m 
 44%|████▍     | 16/36 [00:39<00:53,  2.66s/it][A
[36m(task, pid=6695)[0m 
 92%|█████████▏| 33/36 [00:43<00:02,  1.33it/s][A
[36m(task, pid=6695)[0m 
 97%|█████████▋| 35/36 [00:44<00:00,  1.35it/s][A
[36m(task, pid=6695)[0m 
100%|██████████| 36/36 [00:46<00:00,  1.21it/s][A
100%|██████████| 36/36 [00:46<00:00,  1.28s/it]
[36m(task, pid=6695)[0m [2025-04-22 05:57:02 TP0] Entering DeepGEMM JIT Pre-Complie session. And it may takes a long time(Typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. Recommand to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:57:02 TP0] Try DeepGEMM JIT Compiling for <m_grouped_gemm_fp8_fp8_bf16_nt_masked> N=512, K=128, num_groups=16 with all Ms. It only takes a litte time(Typically 1 sec) if you have run `sglang.compile_deep_gemm`. 
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m 
  0%|          | 0/14 [00:00<?, ?it/s][A
[36m(task, pid=6695)[0m 
  7%|▋         | 1/14 [00:06<01:26,  6.64s/it][A
[36m(task, pid=6695)[0m 
 64%|██████▍   | 9/14 [00:13<00:06,  1.28s/it][A
100%|██████████| 14/14 [00:13<00:00,  1.06it/s]
[36m(task, pid=6695)[0m [2025-04-22 05:57:16 TP0] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=576,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:57:16 TP0] Entering DeepGEMM JIT Pre-Complie session. And it may takes a long time(Typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. Recommand to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:57:16 TP0] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=576, K=7168, num_groups=1 with all Ms. It only takes a litte time(Typically 1 sec) if you have run `sglang.compile_deep_gemm`. 
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m 
  0%|          | 0/103 [00:00<?, ?it/s][A
[36m(task, pid=6695)[0m 
  1%|          | 1/103 [00:06<11:21,  6.68s/it][A
[36m(task, pid=6695)[0m 
  3%|▎         | 3/103 [00:06<02:58,  1.79s/it][A
[36m(task, pid=6695)[0m 
  4%|▍         | 4/103 [00:07<02:10,  1.31s/it][A
[36m(task, pid=6695)[0m 
  5%|▍         | 5/103 [00:13<04:47,  2.93s/it][A
[36m(task, pid=6695)[0m 
  7%|▋         | 7/103 [00:14<02:46,  1.74s/it][A
[36m(task, pid=6695)[0m 
  9%|▊         | 9/103 [00:20<03:29,  2.23s/it][A
[36m(task, pid=6695)[0m 
 11%|█         | 11/103 [00:21<02:29,  1.62s/it][A
[36m(task, pid=6695)[0m 
 12%|█▏        | 12/103 [00:21<02:02,  1.34s/it][A
[36m(task, pid=6695)[0m 
 13%|█▎        | 13/103 [00:27<03:38,  2.43s/it][A
[36m(task, pid=6695)[0m 
 15%|█▍        | 15/103 [00:29<02:31,  1.72s/it][A
[36m(task, pid=6695)[0m 
 17%|█▋        | 17/103 [00:33<02:47,  1.95s/it][A
[36m(task, pid=6695)[0m 
 17%|█▋        | 18/103 [00:34<02:23,  1.69s/it][A
[36m(task, pid=6695)[0m 
 19%|█▉        | 20/103 [00:36<01:59,  1.44s/it][A
[36m(task, pid=6695)[0m 
 23%|██▎       | 24/103 [00:41<01:42,  1.30s/it][A
[36m(task, pid=6695)[0m 
 25%|██▌       | 26/103 [00:49<02:36,  2.04s/it][A
[36m(task, pid=6695)[0m 
 27%|██▋       | 28/103 [00:51<02:12,  1.76s/it][A
[36m(task, pid=6695)[0m 
 33%|███▎      | 34/103 [00:55<01:18,  1.14s/it][A
[36m(task, pid=6695)[0m 
 36%|███▌      | 37/103 [00:55<00:59,  1.11it/s][A
[36m(task, pid=6695)[0m 
 37%|███▋      | 38/103 [00:56<00:56,  1.15it/s][A
[36m(task, pid=6695)[0m 
 38%|███▊      | 39/103 [00:58<01:02,  1.03it/s][A
[36m(task, pid=6695)[0m 
 50%|█████     | 52/103 [01:01<00:24,  2.09it/s][A
[36m(task, pid=6695)[0m 
 51%|█████▏    | 53/103 [01:02<00:24,  2.03it/s][A
[36m(task, pid=6695)[0m 
 53%|█████▎    | 55/103 [01:03<00:22,  2.15it/s][A
[36m(task, pid=6695)[0m 
 54%|█████▍    | 56/103 [01:04<00:27,  1.70it/s][A
[36m(task, pid=6695)[0m 
 56%|█████▋    | 58/103 [01:08<00:40,  1.10it/s][A
[36m(task, pid=6695)[0m 
 59%|█████▉    | 61/103 [01:09<00:28,  1.46it/s][A
[36m(task, pid=6695)[0m 
 62%|██████▏   | 64/103 [01:10<00:23,  1.68it/s][A
[36m(task, pid=6695)[0m 
 63%|██████▎   | 65/103 [01:11<00:24,  1.56it/s][A
[36m(task, pid=6695)[0m 
 66%|██████▌   | 68/103 [01:16<00:35,  1.01s/it][A
[36m(task, pid=6695)[0m 
 75%|███████▍  | 77/103 [01:18<00:13,  1.91it/s][A
[36m(task, pid=6695)[0m 
 77%|███████▋  | 79/103 [01:19<00:12,  1.95it/s][A
[36m(task, pid=6695)[0m 
 84%|████████▍ | 87/103 [01:23<00:08,  1.85it/s][A
[36m(task, pid=6695)[0m 
 86%|████████▋ | 89/103 [01:24<00:06,  2.05it/s][A
[36m(task, pid=6695)[0m 
 88%|████████▊ | 91/103 [01:25<00:05,  2.09it/s][A
[36m(task, pid=6695)[0m 
 89%|████████▉ | 92/103 [01:26<00:05,  1.91it/s][A
[36m(task, pid=6695)[0m 
 95%|█████████▌| 98/103 [01:30<00:03,  1.54it/s][A
[36m(task, pid=6695)[0m 
 96%|█████████▌| 99/103 [01:30<00:02,  1.67it/s][A
[36m(task, pid=6695)[0m 
 97%|█████████▋| 100/103 [01:31<00:01,  1.54it/s][A
[36m(task, pid=6695)[0m 
100%|██████████| 103/103 [01:32<00:00,  1.93it/s][A
100%|██████████| 103/103 [01:32<00:00,  1.11it/s]
[36m(task, pid=6695)[0m [2025-04-22 05:58:50 TP0] Entering DeepGEMM JIT Pre-Complie session. And it may takes a long time(Typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. Recommand to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:58:50 TP0] Try DeepGEMM JIT Compiling for <m_grouped_gemm_fp8_fp8_bf16_nt_masked> N=128, K=512, num_groups=16 with all Ms. It only takes a litte time(Typically 1 sec) if you have run `sglang.compile_deep_gemm`. 
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m 
  0%|          | 0/37 [00:00<?, ?it/s][A
[36m(task, pid=6695)[0m 
  5%|▌         | 2/37 [00:06<01:58,  3.38s/it][A
[36m(task, pid=6695)[0m 
  8%|▊         | 3/37 [00:07<01:16,  2.26s/it][A
[36m(task, pid=6695)[0m 
 16%|█▌        | 6/37 [00:14<01:09,  2.25s/it][A
[36m(task, pid=6695)[0m 
 68%|██████▊   | 25/37 [00:20<00:07,  1.61it/s][A
[36m(task, pid=6695)[0m 
 92%|█████████▏| 34/37 [00:20<00:01,  2.47it/s][A
[36m(task, pid=6695)[0m 
 97%|█████████▋| 36/37 [00:27<00:00,  1.44it/s][A
100%|██████████| 37/37 [00:27<00:00,  1.35it/s]
[36m(task, pid=6695)[0m [2025-04-22 05:59:18 TP0] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=7168,K=2048,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:59:18 TP0] Entering DeepGEMM JIT Pre-Complie session. And it may takes a long time(Typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. Recommand to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:59:18 TP0] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a litte time(Typically 1 sec) if you have run `sglang.compile_deep_gemm`. 
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m 
  0%|          | 0/21 [00:00<?, ?it/s][A
[36m(task, pid=6695)[0m 
  5%|▍         | 1/21 [00:07<02:26,  7.32s/it][A
[36m(task, pid=6695)[0m 
 24%|██▍       | 5/21 [00:14<00:40,  2.54s/it][A
[36m(task, pid=6695)[0m 
 29%|██▊       | 6/21 [00:14<00:30,  2.05s/it][A
[36m(task, pid=6695)[0m 
 52%|█████▏    | 11/21 [00:21<00:16,  1.60s/it][A
[36m(task, pid=6695)[0m 
 67%|██████▋   | 14/21 [00:26<00:11,  1.63s/it][A
[36m(task, pid=6695)[0m 
 95%|█████████▌| 20/21 [00:28<00:00,  1.04it/s][A
100%|██████████| 21/21 [00:28<00:00,  1.34s/it]
[36m(task, pid=6695)[0m [2025-04-22 05:59:47 TP0] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=4608,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 05:59:47 TP0] Entering DeepGEMM JIT Pre-Complie session. And it may takes a long time(Typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. Recommand to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 05:59:47 TP0] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=4608, K=7168, num_groups=1 with all Ms. It only takes a litte time(Typically 1 sec) if you have run `sglang.compile_deep_gemm`. 
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m 
  0%|          | 0/28 [00:00<?, ?it/s][A
[36m(task, pid=6695)[0m 
  7%|▋         | 2/28 [00:06<01:29,  3.43s/it][A
[36m(task, pid=6695)[0m 
 11%|█         | 3/28 [00:16<02:28,  5.95s/it][A
[36m(task, pid=6695)[0m 
 36%|███▌      | 10/28 [00:20<00:30,  1.68s/it][A
[36m(task, pid=6695)[0m 
 50%|█████     | 14/28 [00:21<00:15,  1.09s/it][A
[36m(task, pid=6695)[0m 
 54%|█████▎    | 15/28 [00:22<00:13,  1.01s/it][A
[36m(task, pid=6695)[0m 
 61%|██████    | 17/28 [00:31<00:21,  1.93s/it][A
[36m(task, pid=6695)[0m 
 82%|████████▏ | 23/28 [00:35<00:06,  1.31s/it][A
[36m(task, pid=6695)[0m 
 96%|█████████▋| 27/28 [00:36<00:00,  1.09it/s][A
[36m(task, pid=6695)[0m 
100%|██████████| 28/28 [00:43<00:00,  1.62s/it][A
100%|██████████| 28/28 [00:43<00:00,  1.56s/it]
[36m(task, pid=6695)[0m [2025-04-22 06:00:32 TP0] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=7168,K=2304,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 06:00:32 TP0] Entering DeepGEMM JIT Pre-Complie session. And it may takes a long time(Typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. Recommand to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 06:00:32 TP0] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2304, num_groups=1 with all Ms. It only takes a litte time(Typically 1 sec) if you have run `sglang.compile_deep_gemm`. 
[36m(task, pid=6695)[0m 
[36m(task, pid=6695)[0m 
  0%|          | 0/21 [00:00<?, ?it/s][A
[36m(task, pid=6695)[0m 
  5%|▍         | 1/21 [00:06<02:17,  6.86s/it][A
[36m(task, pid=6695)[0m 
 10%|▉         | 2/21 [00:09<01:23,  4.41s/it][A
[36m(task, pid=6695)[0m 
 29%|██▊       | 6/21 [00:13<00:27,  1.83s/it][A
[36m(task, pid=6695)[0m 
 33%|███▎      | 7/21 [00:14<00:20,  1.49s/it][A
[36m(task, pid=6695)[0m 
 43%|████▎     | 9/21 [00:17<00:17,  1.50s/it][A
[36m(task, pid=6695)[0m 
 48%|████▊     | 10/21 [00:20<00:21,  1.98s/it][A
[36m(task, pid=6695)[0m 
 57%|█████▋    | 12/21 [00:21<00:11,  1.26s/it][A
[36m(task, pid=6695)[0m 
 67%|██████▋   | 14/21 [00:21<00:06,  1.06it/s][A
[36m(task, pid=6695)[0m 
 71%|███████▏  | 15/21 [00:24<00:07,  1.23s/it][A
[36m(task, pid=6695)[0m 
 76%|███████▌  | 16/21 [00:27<00:08,  1.78s/it][A
[36m(task, pid=6695)[0m 
 95%|█████████▌| 20/21 [00:28<00:00,  1.19it/s][A
100%|██████████| 21/21 [00:28<00:00,  1.34s/it]
[36m(task, pid=6695)[0m [2025-04-22 06:01:01 TP0] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/E=264,N=256,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for MoE layer.
[36m(task, pid=6695)[0m 
Capturing batches (avail_mem=10.99 GB):   3%|▎         | 1/35 [06:17<3:33:52, 377.42s/it]
Capturing batches (avail_mem=9.06 GB):   3%|▎         | 1/35 [06:17<3:33:52, 377.42s/it] 
Capturing batches (avail_mem=9.06 GB):   6%|▌         | 2/35 [06:19<1:26:03, 156.48s/it]
Capturing batches (avail_mem=8.21 GB):   6%|▌         | 2/35 [06:19<1:26:03, 156.48s/it]
Capturing batches (avail_mem=8.21 GB):   9%|▊         | 3/35 [06:20<45:32, 85.38s/it]   
Capturing batches (avail_mem=8.18 GB):   9%|▊         | 3/35 [06:20<45:32, 85.38s/it]
Capturing batches (avail_mem=8.18 GB):  11%|█▏        | 4/35 [06:20<26:49, 51.91s/it]
Capturing batches (avail_mem=8.16 GB):  11%|█▏        | 4/35 [06:20<26:49, 51.91s/it]
Capturing batches (avail_mem=8.16 GB):  14%|█▍        | 5/35 [06:21<16:42, 33.41s/it]
Capturing batches (avail_mem=8.13 GB):  14%|█▍        | 5/35 [06:21<16:42, 33.41s/it]
Capturing batches (avail_mem=8.13 GB):  17%|█▋        | 6/35 [06:21<10:45, 22.26s/it]
Capturing batches (avail_mem=8.11 GB):  17%|█▋        | 6/35 [06:21<10:45, 22.26s/it]
Capturing batches (avail_mem=8.11 GB):  20%|██        | 7/35 [06:22<07:04, 15.16s/it]
Capturing batches (avail_mem=7.96 GB):  20%|██        | 7/35 [06:22<07:04, 15.16s/it]
Capturing batches (avail_mem=7.96 GB):  23%|██▎       | 8/35 [06:22<04:44, 10.52s/it]
Capturing batches (avail_mem=7.81 GB):  23%|██▎       | 8/35 [06:22<04:44, 10.52s/it]
Capturing batches (avail_mem=7.81 GB):  26%|██▌       | 9/35 [06:23<03:15,  7.52s/it]
Capturing batches (avail_mem=7.03 GB):  26%|██▌       | 9/35 [06:23<03:15,  7.52s/it]
Capturing batches (avail_mem=7.03 GB):  29%|██▊       | 10/35 [06:24<02:14,  5.39s/it]
Capturing batches (avail_mem=7.01 GB):  29%|██▊       | 10/35 [06:24<02:14,  5.39s/it]
Capturing batches (avail_mem=7.01 GB):  31%|███▏      | 11/35 [06:25<01:34,  3.92s/it]
Capturing batches (avail_mem=6.87 GB):  31%|███▏      | 11/35 [06:25<01:34,  3.92s/it]
Capturing batches (avail_mem=6.87 GB):  34%|███▍      | 12/35 [06:25<01:07,  2.94s/it]
Capturing batches (avail_mem=6.74 GB):  34%|███▍      | 12/35 [06:25<01:07,  2.94s/it]
Capturing batches (avail_mem=6.74 GB):  37%|███▋      | 13/35 [06:26<00:48,  2.23s/it]
Capturing batches (avail_mem=6.62 GB):  37%|███▋      | 13/35 [06:26<00:48,  2.23s/it]
Capturing batches (avail_mem=6.62 GB):  40%|████      | 14/35 [06:26<00:36,  1.73s/it]
Capturing batches (avail_mem=6.50 GB):  40%|████      | 14/35 [06:26<00:36,  1.73s/it]
Capturing batches (avail_mem=6.50 GB):  43%|████▎     | 15/35 [06:27<00:27,  1.39s/it]
Capturing batches (avail_mem=6.39 GB):  43%|████▎     | 15/35 [06:27<00:27,  1.39s/it]
Capturing batches (avail_mem=6.39 GB):  46%|████▌     | 16/35 [06:28<00:21,  1.15s/it]
Capturing batches (avail_mem=6.28 GB):  46%|████▌     | 16/35 [06:28<00:21,  1.15s/it]
Capturing batches (avail_mem=6.28 GB):  49%|████▊     | 17/35 [06:29<00:20,  1.12s/it]
Capturing batches (avail_mem=6.18 GB):  49%|████▊     | 17/35 [06:29<00:20,  1.12s/it]
Capturing batches (avail_mem=6.18 GB):  51%|█████▏    | 18/35 [06:29<00:16,  1.05it/s]
Capturing batches (avail_mem=6.08 GB):  51%|█████▏    | 18/35 [06:29<00:16,  1.05it/s]
Capturing batches (avail_mem=6.08 GB):  54%|█████▍    | 19/35 [06:31<00:19,  1.19s/it]
Capturing batches (avail_mem=5.99 GB):  54%|█████▍    | 19/35 [06:31<00:19,  1.19s/it]
Capturing batches (avail_mem=5.99 GB):  57%|█████▋    | 20/35 [06:33<00:20,  1.36s/it]
Capturing batches (avail_mem=5.90 GB):  57%|█████▋    | 20/35 [06:33<00:20,  1.36s/it]
Capturing batches (avail_mem=5.90 GB):  60%|██████    | 21/35 [06:33<00:15,  1.14s/it]
Capturing batches (avail_mem=5.81 GB):  60%|██████    | 21/35 [06:33<00:15,  1.14s/it]
Capturing batches (avail_mem=5.81 GB):  63%|██████▎   | 22/35 [06:34<00:12,  1.02it/s]
Capturing batches (avail_mem=5.74 GB):  63%|██████▎   | 22/35 [06:34<00:12,  1.02it/s]
Capturing batches (avail_mem=5.74 GB):  66%|██████▌   | 23/35 [06:35<00:10,  1.14it/s]
Capturing batches (avail_mem=5.66 GB):  66%|██████▌   | 23/35 [06:35<00:10,  1.14it/s]
Capturing batches (avail_mem=5.66 GB):  69%|██████▊   | 24/35 [06:35<00:08,  1.27it/s]
Capturing batches (avail_mem=5.59 GB):  69%|██████▊   | 24/35 [06:35<00:08,  1.27it/s]
Capturing batches (avail_mem=5.59 GB):  71%|███████▏  | 25/35 [06:36<00:08,  1.16it/s]
Capturing batches (avail_mem=5.53 GB):  71%|███████▏  | 25/35 [06:36<00:08,  1.16it/s]
Capturing batches (avail_mem=5.53 GB):  74%|███████▍  | 26/35 [06:38<00:10,  1.13s/it]
Capturing batches (avail_mem=5.47 GB):  74%|███████▍  | 26/35 [06:38<00:10,  1.13s/it]
Capturing batches (avail_mem=5.47 GB):  77%|███████▋  | 27/35 [06:40<00:10,  1.33s/it]
Capturing batches (avail_mem=5.41 GB):  77%|███████▋  | 27/35 [06:40<00:10,  1.33s/it]
Capturing batches (avail_mem=5.41 GB):  80%|████████  | 28/35 [06:40<00:07,  1.11s/it]
Capturing batches (avail_mem=5.36 GB):  80%|████████  | 28/35 [06:40<00:07,  1.11s/it]
Capturing batches (avail_mem=5.36 GB):  83%|████████▎ | 29/35 [06:41<00:05,  1.04it/s]
Capturing batches (avail_mem=5.32 GB):  83%|████████▎ | 29/35 [06:41<00:05,  1.04it/s]
Capturing batches (avail_mem=5.32 GB):  86%|████████▌ | 30/35 [06:42<00:04,  1.18it/s]
Capturing batches (avail_mem=5.28 GB):  86%|████████▌ | 30/35 [06:42<00:04,  1.18it/s]
Capturing batches (avail_mem=5.28 GB):  89%|████████▊ | 31/35 [06:42<00:03,  1.30it/s]
Capturing batches (avail_mem=5.24 GB):  89%|████████▊ | 31/35 [06:42<00:03,  1.30it/s]
Capturing batches (avail_mem=5.24 GB):  91%|█████████▏| 32/35 [06:43<00:02,  1.39it/s]
Capturing batches (avail_mem=5.20 GB):  91%|█████████▏| 32/35 [06:43<00:02,  1.39it/s]
Capturing batches (avail_mem=5.20 GB):  94%|█████████▍| 33/35 [06:44<00:01,  1.08it/s]
Capturing batches (avail_mem=5.17 GB):  94%|█████████▍| 33/35 [06:44<00:01,  1.08it/s]
Capturing batches (avail_mem=5.17 GB):  97%|█████████▋| 34/35 [06:45<00:00,  1.00it/s]
Capturing batches (avail_mem=5.15 GB):  97%|█████████▋| 34/35 [06:45<00:00,  1.00it/s][2025-04-22 06:01:33 TP2] Registering 4305 cuda graph addresses
[36m(task, pid=6695)[0m [2025-04-22 06:01:33 TP5] Registering 4305 cuda graph addresses
[36m(task, pid=6695)[0m [2025-04-22 06:01:33 TP7] Registering 4305 cuda graph addresses
[36m(task, pid=6695)[0m [2025-04-22 06:01:33 TP3] Registering 4305 cuda graph addresses
[36m(task, pid=6695)[0m 
Capturing batches (avail_mem=5.15 GB): 100%|██████████| 35/35 [06:47<00:00,  1.12s/it]
Capturing batches (avail_mem=5.15 GB): 100%|██████████| 35/35 [06:47<00:00, 11.64s/it]
[36m(task, pid=6695)[0m [2025-04-22 06:01:33 TP4] Registering 4305 cuda graph addresses
[36m(task, pid=6695)[0m [2025-04-22 06:01:33 TP6] Registering 4305 cuda graph addresses
[36m(task, pid=6695)[0m [2025-04-22 06:01:33 TP0] Registering 4305 cuda graph addresses
[36m(task, pid=6695)[0m [2025-04-22 06:01:33 TP1] Registering 4305 cuda graph addresses
[36m(task, pid=6695)[0m [2025-04-22 06:01:33 TP7] Capture cuda graph end. Time elapsed: 408.13 s. avail mem=5.50 GB. mem usage=6.34 GB.
[36m(task, pid=6695)[0m [2025-04-22 06:01:33 TP2] Capture cuda graph end. Time elapsed: 408.13 s. avail mem=5.03 GB. mem usage=6.34 GB.
[36m(task, pid=6695)[0m [2025-04-22 06:01:33 TP1] Capture cuda graph end. Time elapsed: 408.14 s. avail mem=5.03 GB. mem usage=6.34 GB.
[36m(task, pid=6695)[0m [2025-04-22 06:01:33 TP5] Capture cuda graph end. Time elapsed: 408.14 s. avail mem=5.03 GB. mem usage=6.34 GB.
[36m(task, pid=6695)[0m [2025-04-22 06:01:33 TP3] Capture cuda graph end. Time elapsed: 408.14 s. avail mem=5.03 GB. mem usage=6.34 GB.
[36m(task, pid=6695)[0m [2025-04-22 06:01:33 TP0] Capture cuda graph end. Time elapsed: 408.14 s. avail mem=5.12 GB. mem usage=6.34 GB.
[36m(task, pid=6695)[0m [2025-04-22 06:01:33 TP6] Capture cuda graph end. Time elapsed: 408.14 s. avail mem=5.03 GB. mem usage=6.34 GB.
[36m(task, pid=6695)[0m [2025-04-22 06:01:33 TP4] Capture cuda graph end. Time elapsed: 408.14 s. avail mem=5.03 GB. mem usage=6.34 GB.
[36m(task, pid=6695)[0m [2025-04-22 06:01:34 TP1] max_total_num_tokens=654280, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2049, context_len=163840
[36m(task, pid=6695)[0m [2025-04-22 06:01:34 TP7] max_total_num_tokens=654280, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2049, context_len=163840
[36m(task, pid=6695)[0m [2025-04-22 06:01:34 TP3] max_total_num_tokens=654280, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2049, context_len=163840
[36m(task, pid=6695)[0m [2025-04-22 06:01:34 TP5] max_total_num_tokens=654280, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2049, context_len=163840
[36m(task, pid=6695)[0m [2025-04-22 06:01:34 TP4] max_total_num_tokens=654280, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2049, context_len=163840
[36m(task, pid=6695)[0m [2025-04-22 06:01:34 TP6] max_total_num_tokens=654280, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2049, context_len=163840
[36m(task, pid=6695)[0m [2025-04-22 06:01:34 TP2] max_total_num_tokens=654280, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2049, context_len=163840
[36m(task, pid=6695)[0m [2025-04-22 06:01:34 TP0] max_total_num_tokens=654280, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2049, context_len=163840
[36m(task, pid=6695)[0m [2025-04-22 06:01:34] INFO:     Started server process [16260]
[36m(task, pid=6695)[0m [2025-04-22 06:01:34] INFO:     Waiting for application startup.
[36m(task, pid=6695)[0m [2025-04-22 06:01:34] INFO:     Application startup complete.
[36m(task, pid=6695)[0m [2025-04-22 06:01:34] INFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)
[36m(task, pid=6695)[0m [2025-04-22 06:01:35] INFO:     127.0.0.1:59870 - "GET /get_model_info HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:01:35 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:01:45] INFO:     127.0.0.1:59876 - "POST /generate HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:01:45] The server is fired up and ready to roll!
[36m(task, pid=6695)[0m [2025-04-22 06:01:49] INFO:     127.0.0.1:40494 - "GET /v1/models HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:01] INFO:     127.0.0.1:55982 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:01 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:03 TP0] Decode batch. #running-req: 0, #token: 0, token usage: 0.00, gen throughput (token/s): 1.39, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:04] INFO:     127.0.0.1:55998 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:04 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:04] INFO:     127.0.0.1:56012 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:04 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:04] INFO:     127.0.0.1:56018 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:04] INFO:     127.0.0.1:56032 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:04] INFO:     127.0.0.1:56034 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:04] INFO:     127.0.0.1:56048 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:04] INFO:     127.0.0.1:56064 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:04] INFO:     127.0.0.1:56068 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:04 TP0] Prefill batch. #new-seq: 6, #new-token: 6006, #cached-token: 0, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:04] INFO:     127.0.0.1:56076 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:04] INFO:     127.0.0.1:56086 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:04] INFO:     127.0.0.1:56100 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:04] INFO:     127.0.0.1:56110 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:04 TP0] Prefill batch. #new-seq: 4, #new-token: 4004, #cached-token: 0, token usage: 0.01, #running-req: 8, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:04] INFO:     127.0.0.1:56124 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:04] INFO:     127.0.0.1:56128 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:05] INFO:     127.0.0.1:41744 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:05] INFO:     127.0.0.1:41760 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:05] INFO:     127.0.0.1:41764 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:05] INFO:     127.0.0.1:41776 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:05] INFO:     127.0.0.1:41792 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:05] INFO:     127.0.0.1:41798 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:05] INFO:     127.0.0.1:41806 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:05] INFO:     127.0.0.1:41810 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:05] INFO:     127.0.0.1:41820 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:05] INFO:     127.0.0.1:41822 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:05] INFO:     127.0.0.1:41830 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:06] INFO:     127.0.0.1:41842 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:06] INFO:     127.0.0.1:41856 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:06] INFO:     127.0.0.1:41862 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:06] INFO:     127.0.0.1:41878 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:06] INFO:     127.0.0.1:41886 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:06] INFO:     127.0.0.1:41892 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:06] INFO:     127.0.0.1:41908 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:06] INFO:     127.0.0.1:41912 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:07] INFO:     127.0.0.1:41922 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:07 TP0] Prefill batch. #new-seq: 9, #new-token: 8192, #cached-token: 0, token usage: 0.02, #running-req: 12, #queue-req: 13, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:07] INFO:     127.0.0.1:41926 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:07] INFO:     127.0.0.1:41938 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:07] INFO:     127.0.0.1:41944 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:07] INFO:     127.0.0.1:41956 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:07] INFO:     127.0.0.1:41960 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:07] INFO:     127.0.0.1:41972 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:07 TP0] Prefill batch. #new-seq: 9, #new-token: 8192, #cached-token: 0, token usage: 0.03, #running-req: 20, #queue-req: 9, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:07] INFO:     127.0.0.1:41984 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:07 TP0] Prefill batch. #new-seq: 9, #new-token: 8192, #cached-token: 0, token usage: 0.04, #running-req: 28, #queue-req: 4, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:08] INFO:     127.0.0.1:41992 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:08 TP0] Prefill batch. #new-seq: 6, #new-token: 5454, #cached-token: 0, token usage: 0.06, #running-req: 36, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:08] INFO:     127.0.0.1:42000 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:08] INFO:     127.0.0.1:42012 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:08] INFO:     127.0.0.1:42022 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:08] INFO:     127.0.0.1:42030 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:08 TP0] Prefill batch. #new-seq: 4, #new-token: 4004, #cached-token: 0, token usage: 0.06, #running-req: 42, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:08] INFO:     127.0.0.1:42036 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:08 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.07, #running-req: 46, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:08] INFO:     127.0.0.1:42050 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:08] INFO:     127.0.0.1:42066 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:08] INFO:     127.0.0.1:42068 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:02:08 TP0] Prefill batch. #new-seq: 3, #new-token: 3003, #cached-token: 0, token usage: 0.07, #running-req: 47, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:12 TP0] Decode batch. #running-req: 50, #token: 52051, token usage: 0.08, gen throughput (token/s): 218.57, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:13 TP0] Decode batch. #running-req: 50, #token: 54051, token usage: 0.08, gen throughput (token/s): 1484.32, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:14 TP0] Decode batch. #running-req: 50, #token: 56051, token usage: 0.09, gen throughput (token/s): 1467.49, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:16 TP0] Decode batch. #running-req: 50, #token: 58051, token usage: 0.09, gen throughput (token/s): 1452.57, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:17 TP0] Decode batch. #running-req: 50, #token: 60051, token usage: 0.09, gen throughput (token/s): 1442.57, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:18 TP0] Decode batch. #running-req: 50, #token: 62051, token usage: 0.09, gen throughput (token/s): 1433.40, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:20 TP0] Decode batch. #running-req: 50, #token: 64051, token usage: 0.10, gen throughput (token/s): 1434.55, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:21 TP0] Decode batch. #running-req: 50, #token: 66051, token usage: 0.10, gen throughput (token/s): 1426.96, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:23 TP0] Decode batch. #running-req: 50, #token: 68051, token usage: 0.10, gen throughput (token/s): 1430.50, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:24 TP0] Decode batch. #running-req: 50, #token: 70051, token usage: 0.11, gen throughput (token/s): 1426.40, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:25 TP0] Decode batch. #running-req: 50, #token: 72051, token usage: 0.11, gen throughput (token/s): 1425.49, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:27 TP0] Decode batch. #running-req: 50, #token: 74051, token usage: 0.11, gen throughput (token/s): 1421.66, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:28 TP0] Decode batch. #running-req: 50, #token: 76051, token usage: 0.12, gen throughput (token/s): 1412.39, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:30 TP0] Decode batch. #running-req: 50, #token: 78051, token usage: 0.12, gen throughput (token/s): 1412.64, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:31 TP0] Decode batch. #running-req: 50, #token: 80051, token usage: 0.12, gen throughput (token/s): 1419.82, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:33 TP0] Decode batch. #running-req: 50, #token: 82051, token usage: 0.13, gen throughput (token/s): 1420.12, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:34 TP0] Decode batch. #running-req: 50, #token: 84051, token usage: 0.13, gen throughput (token/s): 1413.68, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:35 TP0] Decode batch. #running-req: 50, #token: 86051, token usage: 0.13, gen throughput (token/s): 1410.47, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:37 TP0] Decode batch. #running-req: 50, #token: 88051, token usage: 0.13, gen throughput (token/s): 1415.17, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:38 TP0] Decode batch. #running-req: 50, #token: 90051, token usage: 0.14, gen throughput (token/s): 1413.75, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:40 TP0] Decode batch. #running-req: 50, #token: 92051, token usage: 0.14, gen throughput (token/s): 1409.09, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:41 TP0] Decode batch. #running-req: 50, #token: 94051, token usage: 0.14, gen throughput (token/s): 1405.70, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:42 TP0] Decode batch. #running-req: 50, #token: 96051, token usage: 0.15, gen throughput (token/s): 1402.92, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:44 TP0] Decode batch. #running-req: 50, #token: 98051, token usage: 0.15, gen throughput (token/s): 1404.83, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:45 TP0] Decode batch. #running-req: 50, #token: 100051, token usage: 0.15, gen throughput (token/s): 1402.82, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:47 TP0] Decode batch. #running-req: 50, #token: 102051, token usage: 0.16, gen throughput (token/s): 1403.31, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:48 TP0] Decode batch. #running-req: 50, #token: 104051, token usage: 0.16, gen throughput (token/s): 1397.85, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:50 TP0] Decode batch. #running-req: 50, #token: 106051, token usage: 0.16, gen throughput (token/s): 1402.46, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:51 TP0] Decode batch. #running-req: 50, #token: 108051, token usage: 0.17, gen throughput (token/s): 1400.99, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:52 TP0] Decode batch. #running-req: 50, #token: 110051, token usage: 0.17, gen throughput (token/s): 1400.15, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:54 TP0] Decode batch. #running-req: 50, #token: 112051, token usage: 0.17, gen throughput (token/s): 1397.24, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:55 TP0] Decode batch. #running-req: 50, #token: 114051, token usage: 0.17, gen throughput (token/s): 1400.69, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:57 TP0] Decode batch. #running-req: 50, #token: 116051, token usage: 0.18, gen throughput (token/s): 1397.32, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:02:58 TP0] Decode batch. #running-req: 50, #token: 118051, token usage: 0.18, gen throughput (token/s): 1395.06, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:00 TP0] Decode batch. #running-req: 50, #token: 120051, token usage: 0.18, gen throughput (token/s): 1397.71, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:01 TP0] Decode batch. #running-req: 50, #token: 122051, token usage: 0.19, gen throughput (token/s): 1394.98, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:02 TP0] Decode batch. #running-req: 50, #token: 124051, token usage: 0.19, gen throughput (token/s): 1399.03, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:04 TP0] Decode batch. #running-req: 50, #token: 126051, token usage: 0.19, gen throughput (token/s): 1396.92, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:05 TP0] Decode batch. #running-req: 50, #token: 128051, token usage: 0.20, gen throughput (token/s): 1404.13, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:07 TP0] Decode batch. #running-req: 50, #token: 130051, token usage: 0.20, gen throughput (token/s): 1396.09, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:08 TP0] Decode batch. #running-req: 50, #token: 132051, token usage: 0.20, gen throughput (token/s): 1395.84, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:10 TP0] Decode batch. #running-req: 50, #token: 134051, token usage: 0.20, gen throughput (token/s): 1394.27, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:11 TP0] Decode batch. #running-req: 50, #token: 136051, token usage: 0.21, gen throughput (token/s): 1392.89, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:12 TP0] Decode batch. #running-req: 50, #token: 138051, token usage: 0.21, gen throughput (token/s): 1391.41, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:14 TP0] Decode batch. #running-req: 50, #token: 140051, token usage: 0.21, gen throughput (token/s): 1391.07, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:15 TP0] Decode batch. #running-req: 50, #token: 142051, token usage: 0.22, gen throughput (token/s): 1391.90, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:17 TP0] Decode batch. #running-req: 50, #token: 144051, token usage: 0.22, gen throughput (token/s): 1388.04, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:18 TP0] Decode batch. #running-req: 50, #token: 146051, token usage: 0.22, gen throughput (token/s): 1388.37, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:20 TP0] Decode batch. #running-req: 50, #token: 148051, token usage: 0.23, gen throughput (token/s): 1381.64, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:21 TP0] Decode batch. #running-req: 49, #token: 49, token usage: 0.00, gen throughput (token/s): 1378.60, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:21] INFO:     127.0.0.1:57000 - "GET /get_server_info HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:24] INFO:     127.0.0.1:57002 - "GET /v1/models HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:28] INFO:     127.0.0.1:40324 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:28 TP0] Prefill batch. #new-seq: 1, #new-token: 5001, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:30] INFO:     127.0.0.1:40336 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:30 TP0] Prefill batch. #new-seq: 1, #new-token: 5001, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:30] INFO:     127.0.0.1:40350 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:30] INFO:     127.0.0.1:40364 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:30] INFO:     127.0.0.1:40366 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:30] INFO:     127.0.0.1:40368 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:30] INFO:     127.0.0.1:40382 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:30] INFO:     127.0.0.1:40398 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:30] INFO:     127.0.0.1:40404 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:30 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.01, #running-req: 1, #queue-req: 5, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:30] INFO:     127.0.0.1:40406 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:30] INFO:     127.0.0.1:40412 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:31] INFO:     127.0.0.1:40428 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:31] INFO:     127.0.0.1:40438 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:31 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.02, #running-req: 2, #queue-req: 7, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:31] INFO:     127.0.0.1:40442 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:31] INFO:     127.0.0.1:40444 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:31] INFO:     127.0.0.1:40450 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:31] INFO:     127.0.0.1:40462 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:31 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.03, #running-req: 4, #queue-req: 10, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:31] INFO:     127.0.0.1:40464 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:31] INFO:     127.0.0.1:40480 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:31] INFO:     127.0.0.1:40492 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:31] INFO:     127.0.0.1:40498 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:31] INFO:     127.0.0.1:40506 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:31] INFO:     127.0.0.1:40510 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:31 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.05, #running-req: 5, #queue-req: 14, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:32] INFO:     127.0.0.1:40518 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:32] INFO:     127.0.0.1:40526 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:32 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.06, #running-req: 7, #queue-req: 14, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:32] INFO:     127.0.0.1:40534 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:32] INFO:     127.0.0.1:40544 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:32 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.07, #running-req: 9, #queue-req: 15, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:32] INFO:     127.0.0.1:40548 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:32] INFO:     127.0.0.1:40554 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:32] INFO:     127.0.0.1:40564 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:32] INFO:     127.0.0.1:40578 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:33] INFO:     127.0.0.1:40584 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:33] INFO:     127.0.0.1:40594 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:33] INFO:     127.0.0.1:40604 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:33 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.08, #running-req: 10, #queue-req: 20, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:33] INFO:     127.0.0.1:40606 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:33] INFO:     127.0.0.1:40618 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:33 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.10, #running-req: 12, #queue-req: 20, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:33] INFO:     127.0.0.1:40626 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:33] INFO:     127.0.0.1:40632 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:33] INFO:     127.0.0.1:40648 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:33] INFO:     127.0.0.1:40652 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:33] INFO:     127.0.0.1:40664 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:34 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.11, #running-req: 14, #queue-req: 24, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:34] INFO:     127.0.0.1:40680 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:34 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.12, #running-req: 15, #queue-req: 23, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:34] INFO:     127.0.0.1:40682 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:34] INFO:     127.0.0.1:40684 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:34] INFO:     127.0.0.1:40690 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:34 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.13, #running-req: 17, #queue-req: 24, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:34] INFO:     127.0.0.1:40696 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:34] INFO:     127.0.0.1:40698 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:34] INFO:     127.0.0.1:48272 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:35] INFO:     127.0.0.1:48276 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:35] INFO:     127.0.0.1:48280 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:35] INFO:     127.0.0.1:48284 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:03:35 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.15, #running-req: 19, #queue-req: 29, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:35 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.16, #running-req: 20, #queue-req: 27, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:36 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.17, #running-req: 22, #queue-req: 26, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:36 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.18, #running-req: 23, #queue-req: 24, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:37 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.20, #running-req: 25, #queue-req: 22, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:37 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.21, #running-req: 27, #queue-req: 21, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:37 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.22, #running-req: 28, #queue-req: 19, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:38 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.23, #running-req: 30, #queue-req: 17, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:38 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.25, #running-req: 32, #queue-req: 16, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:39 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.26, #running-req: 33, #queue-req: 14, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:39 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.27, #running-req: 35, #queue-req: 12, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:39 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.28, #running-req: 37, #queue-req: 11, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:40 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.30, #running-req: 38, #queue-req: 9, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:40 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.31, #running-req: 40, #queue-req: 8, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:41 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.32, #running-req: 41, #queue-req: 6, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:41 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.33, #running-req: 43, #queue-req: 4, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:42 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.35, #running-req: 45, #queue-req: 3, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:42 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.36, #running-req: 46, #queue-req: 1, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:42 TP0] Prefill batch. #new-seq: 2, #new-token: 7481, #cached-token: 0, token usage: 0.37, #running-req: 48, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:43 TP0] Decode batch. #running-req: 50, #token: 250401, token usage: 0.38, gen throughput (token/s): 17.29, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:45 TP0] Decode batch. #running-req: 50, #token: 252401, token usage: 0.39, gen throughput (token/s): 1322.56, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:46 TP0] Decode batch. #running-req: 50, #token: 254401, token usage: 0.39, gen throughput (token/s): 1297.89, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:48 TP0] Decode batch. #running-req: 50, #token: 256401, token usage: 0.39, gen throughput (token/s): 1289.98, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:49 TP0] Decode batch. #running-req: 50, #token: 258401, token usage: 0.39, gen throughput (token/s): 1280.82, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:51 TP0] Decode batch. #running-req: 50, #token: 260401, token usage: 0.40, gen throughput (token/s): 1276.21, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:53 TP0] Decode batch. #running-req: 50, #token: 262401, token usage: 0.40, gen throughput (token/s): 1273.50, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:54 TP0] Decode batch. #running-req: 50, #token: 264401, token usage: 0.40, gen throughput (token/s): 1272.66, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:56 TP0] Decode batch. #running-req: 50, #token: 266401, token usage: 0.41, gen throughput (token/s): 1263.26, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:57 TP0] Decode batch. #running-req: 50, #token: 268401, token usage: 0.41, gen throughput (token/s): 1264.57, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:03:59 TP0] Decode batch. #running-req: 50, #token: 270401, token usage: 0.41, gen throughput (token/s): 1264.75, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:04:00 TP0] Decode batch. #running-req: 50, #token: 272401, token usage: 0.42, gen throughput (token/s): 1264.05, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:04:02 TP0] Decode batch. #running-req: 50, #token: 274401, token usage: 0.42, gen throughput (token/s): 1254.45, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:04:04 TP0] Decode batch. #running-req: 50, #token: 276401, token usage: 0.42, gen throughput (token/s): 1256.94, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:04:05 TP0] Decode batch. #running-req: 50, #token: 278401, token usage: 0.43, gen throughput (token/s): 1258.98, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:04:07 TP0] Decode batch. #running-req: 50, #token: 280401, token usage: 0.43, gen throughput (token/s): 1258.19, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:04:08 TP0] Decode batch. #running-req: 50, #token: 282401, token usage: 0.43, gen throughput (token/s): 1260.24, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:04:10 TP0] Decode batch. #running-req: 50, #token: 284401, token usage: 0.43, gen throughput (token/s): 1253.22, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:04:12 TP0] Decode batch. #running-req: 50, #token: 286401, token usage: 0.44, gen throughput (token/s): 1257.21, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:04:13 TP0] Decode batch. #running-req: 50, #token: 288401, token usage: 0.44, gen throughput (token/s): 1254.83, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:04:15 TP0] Decode batch. #running-req: 50, #token: 290401, token usage: 0.44, gen throughput (token/s): 1252.78, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:04:16 TP0] Decode batch. #running-req: 50, #token: 292401, token usage: 0.45, gen throughput (token/s): 1251.61, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:04:18 TP0] Decode batch. #running-req: 50, #token: 294401, token usage: 0.45, gen throughput (token/s): 1255.59, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:04:20 TP0] Decode batch. #running-req: 50, #token: 296401, token usage: 0.45, gen throughput (token/s): 1249.60, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:04:21 TP0] Decode batch. #running-req: 50, #token: 298401, token usage: 0.46, gen throughput (token/s): 1254.90, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:04:23] INFO:     127.0.0.1:59252 - "GET /get_server_info HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:04:25] INFO:     127.0.0.1:60908 - "GET /v1/models HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:04:29] INFO:     127.0.0.1:60916 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:04:29 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:04:29 TP0] Prefill batch. #new-seq: 1, #new-token: 1809, #cached-token: 0, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:04:30 TP0] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=4096,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 06:04:30 TP3] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=4096,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 06:04:30 TP7] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=4096,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 06:04:30 TP1] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=4096,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 06:04:30 TP2] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=4096,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 06:04:30 TP5] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=4096,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 06:04:30 TP6] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=4096,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 06:04:30 TP4] Using configuration from /home/ubuntu/miniconda3/lib/python3.10/site-packages/sglang/srt/layers/quantization/configs/N=4096,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json for W8A8 Block FP8 kernel.
[36m(task, pid=6695)[0m [2025-04-22 06:04:30 TP0] Entering DeepGEMM JIT Pre-Complie session. And it may takes a long time(Typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. Recommand to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 06:04:30 TP0] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=4096, K=512, num_groups=1 with all Ms. It only takes a litte time(Typically 1 sec) if you have run `sglang.compile_deep_gemm`. 
[36m(task, pid=6695)[0m [2025-04-22 06:04:30 TP3] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 06:04:30 TP7] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 06:04:30 TP3] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=1809, N=4096, K=512. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 06:04:30 TP1] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 06:04:30 TP7] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=1809, N=4096, K=512. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 06:04:30 TP2] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 06:04:30 TP1] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=1809, N=4096, K=512. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 06:04:30 TP5] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 06:04:30 TP6] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 06:04:30 TP2] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=1809, N=4096, K=512. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 06:04:30 TP4] Entering DeepGEMM JIT Single Kernel Complie session. And it will makes inference throughput becomes flaky. Please run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to solve this issue. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[36m(task, pid=6695)[0m [2025-04-22 06:04:30 TP5] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=1809, N=4096, K=512. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 06:04:30 TP6] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=1809, N=4096, K=512. Please wait.
[36m(task, pid=6695)[0m [2025-04-22 06:04:30 TP4] DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> M=1809, N=4096, K=512. Please wait.
[36m(task, pid=6695)[0m 
  0%|          | 0/33 [00:00<?, ?it/s]
  3%|▎         | 1/33 [00:06<03:32,  6.63s/it]
  6%|▌         | 2/33 [00:07<01:37,  3.14s/it]
  9%|▉         | 3/33 [00:07<00:54,  1.80s/it]
 15%|█▌        | 5/33 [00:14<01:14,  2.67s/it]
 21%|██        | 7/33 [00:23<01:34,  3.63s/it]
 33%|███▎      | 11/33 [00:27<00:45,  2.06s/it]
 42%|████▏     | 14/33 [00:38<00:50,  2.66s/it]
 64%|██████▎   | 21/33 [00:40<00:16,  1.37s/it]
 70%|██████▉   | 23/33 [00:52<00:22,  2.26s/it]
100%|██████████| 33/33 [00:52<00:00,  1.60s/it]
[36m(task, pid=6695)[0m [2025-04-22 06:05:25 TP0] Decode batch. #running-req: 1, #token: 10008, token usage: 0.02, gen throughput (token/s): 26.83, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:26] INFO:     127.0.0.1:60274 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:26 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:26 TP0] Prefill batch. #new-seq: 1, #new-token: 1809, #cached-token: 0, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:26] INFO:     127.0.0.1:60288 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:26] INFO:     127.0.0.1:60292 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:26] INFO:     127.0.0.1:60306 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:26] INFO:     127.0.0.1:60310 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:26] INFO:     127.0.0.1:60316 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:26] INFO:     127.0.0.1:60332 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:26] INFO:     127.0.0.1:60344 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:26] INFO:     127.0.0.1:60356 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:26] INFO:     127.0.0.1:60362 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:27] INFO:     127.0.0.1:60366 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:27] INFO:     127.0.0.1:60376 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:27 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.02, #running-req: 1, #queue-req: 10, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:27] INFO:     127.0.0.1:60380 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:27] INFO:     127.0.0.1:60394 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:27] INFO:     127.0.0.1:60410 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:27] INFO:     127.0.0.1:60414 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:27 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.03, #running-req: 1, #queue-req: 12, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:27] INFO:     127.0.0.1:60430 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:27] INFO:     127.0.0.1:60440 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:27] INFO:     127.0.0.1:60446 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:27] INFO:     127.0.0.1:60460 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:27] INFO:     127.0.0.1:60464 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:27] INFO:     127.0.0.1:60468 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:27 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.04, #running-req: 2, #queue-req: 17, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:28] INFO:     127.0.0.1:60482 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:28] INFO:     127.0.0.1:60492 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:28 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.05, #running-req: 3, #queue-req: 19, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:28] INFO:     127.0.0.1:60506 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:28] INFO:     127.0.0.1:60520 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:28 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.07, #running-req: 4, #queue-req: 20, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:28] INFO:     127.0.0.1:60530 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:28] INFO:     127.0.0.1:60538 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:28] INFO:     127.0.0.1:60552 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:28] INFO:     127.0.0.1:60562 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:29] INFO:     127.0.0.1:60570 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:29] INFO:     127.0.0.1:60584 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:29] INFO:     127.0.0.1:60600 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:29 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.08, #running-req: 5, #queue-req: 27, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:29] INFO:     127.0.0.1:60606 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:29] INFO:     127.0.0.1:60614 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:29] INFO:     127.0.0.1:60624 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:29] INFO:     127.0.0.1:60634 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:29] INFO:     127.0.0.1:60640 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:29 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.09, #running-req: 5, #queue-req: 30, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:29] INFO:     127.0.0.1:60644 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:29] INFO:     127.0.0.1:60646 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:30] INFO:     127.0.0.1:60660 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:30 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.10, #running-req: 6, #queue-req: 33, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:30] INFO:     127.0.0.1:60662 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:30 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.12, #running-req: 7, #queue-req: 33, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:30] INFO:     127.0.0.1:60678 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:30] INFO:     127.0.0.1:60680 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:30] INFO:     127.0.0.1:60686 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:30] INFO:     127.0.0.1:60702 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:30] INFO:     127.0.0.1:60704 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:31 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.13, #running-req: 8, #queue-req: 37, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:31] INFO:     127.0.0.1:60714 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:31] INFO:     127.0.0.1:60726 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:31] INFO:     127.0.0.1:60734 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:05:31 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.14, #running-req: 9, #queue-req: 39, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:32 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.15, #running-req: 10, #queue-req: 39, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:32 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.17, #running-req: 10, #queue-req: 38, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:33 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.18, #running-req: 11, #queue-req: 37, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:33 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.19, #running-req: 12, #queue-req: 36, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:34 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.20, #running-req: 13, #queue-req: 35, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:34 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.22, #running-req: 14, #queue-req: 35, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:35 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.23, #running-req: 14, #queue-req: 34, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:35 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.24, #running-req: 15, #queue-req: 33, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:35 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.25, #running-req: 16, #queue-req: 32, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:36 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.27, #running-req: 17, #queue-req: 31, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:36 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.28, #running-req: 18, #queue-req: 30, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:37 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.29, #running-req: 19, #queue-req: 30, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:37 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.30, #running-req: 19, #queue-req: 29, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:38 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.32, #running-req: 20, #queue-req: 28, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:38 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.33, #running-req: 21, #queue-req: 27, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:39 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.34, #running-req: 22, #queue-req: 26, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:39 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.35, #running-req: 23, #queue-req: 26, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:40 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.37, #running-req: 23, #queue-req: 25, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:40 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.38, #running-req: 24, #queue-req: 24, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:41 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.39, #running-req: 25, #queue-req: 23, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:41 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.40, #running-req: 26, #queue-req: 22, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:42 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.42, #running-req: 27, #queue-req: 21, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:42 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.43, #running-req: 28, #queue-req: 21, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:43 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.44, #running-req: 28, #queue-req: 20, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:43 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.45, #running-req: 29, #queue-req: 19, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:44 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.47, #running-req: 30, #queue-req: 18, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:44 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.48, #running-req: 31, #queue-req: 17, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:45 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.49, #running-req: 32, #queue-req: 17, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:45 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.50, #running-req: 32, #queue-req: 16, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:46 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.52, #running-req: 33, #queue-req: 15, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:46 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.53, #running-req: 34, #queue-req: 14, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:46 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.54, #running-req: 35, #queue-req: 13, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:47 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.55, #running-req: 36, #queue-req: 12, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:47 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.57, #running-req: 37, #queue-req: 12, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:48 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.58, #running-req: 37, #queue-req: 11, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:48 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.59, #running-req: 38, #queue-req: 10, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:49 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.60, #running-req: 39, #queue-req: 9, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:49 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.62, #running-req: 40, #queue-req: 8, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:50 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.63, #running-req: 41, #queue-req: 8, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:50 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.64, #running-req: 41, #queue-req: 7, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:51 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.65, #running-req: 42, #queue-req: 6, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:51 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.67, #running-req: 43, #queue-req: 5, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:52 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.68, #running-req: 44, #queue-req: 4, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:52 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.69, #running-req: 45, #queue-req: 3, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:53 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.70, #running-req: 46, #queue-req: 3, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:53 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.72, #running-req: 46, #queue-req: 2, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:54 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.73, #running-req: 47, #queue-req: 1, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:54 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.74, #running-req: 48, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:55 TP0] Prefill batch. #new-seq: 1, #new-token: 6721, #cached-token: 0, token usage: 0.75, #running-req: 49, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:56 TP0] Decode batch. #running-req: 50, #token: 500800, token usage: 0.77, gen throughput (token/s): 23.09, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:05:58 TP0] Decode batch. #running-req: 50, #token: 502800, token usage: 0.77, gen throughput (token/s): 1136.40, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:00 TP0] Decode batch. #running-req: 50, #token: 504800, token usage: 0.77, gen throughput (token/s): 1121.84, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:01 TP0] Decode batch. #running-req: 50, #token: 506800, token usage: 0.77, gen throughput (token/s): 1117.93, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:03 TP0] Decode batch. #running-req: 50, #token: 508800, token usage: 0.78, gen throughput (token/s): 1114.57, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:05 TP0] Decode batch. #running-req: 50, #token: 510800, token usage: 0.78, gen throughput (token/s): 1105.08, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:07 TP0] Decode batch. #running-req: 50, #token: 512800, token usage: 0.78, gen throughput (token/s): 1102.84, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:09 TP0] Decode batch. #running-req: 50, #token: 514800, token usage: 0.79, gen throughput (token/s): 1096.90, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:11 TP0] Decode batch. #running-req: 50, #token: 516800, token usage: 0.79, gen throughput (token/s): 1094.35, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:12 TP0] Decode batch. #running-req: 50, #token: 518800, token usage: 0.79, gen throughput (token/s): 1094.34, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:14 TP0] Decode batch. #running-req: 50, #token: 520800, token usage: 0.80, gen throughput (token/s): 1091.91, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:16 TP0] Decode batch. #running-req: 50, #token: 522800, token usage: 0.80, gen throughput (token/s): 1093.53, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:18 TP0] Decode batch. #running-req: 50, #token: 524800, token usage: 0.80, gen throughput (token/s): 1096.02, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:18] INFO:     127.0.0.1:34156 - "GET /get_server_info HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:21] INFO:     127.0.0.1:34164 - "GET /v1/models HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:25] INFO:     127.0.0.1:37260 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m Token indices sequence length is longer than the specified maximum sequence length for this model (30001 > 16384). Running this sequence through the model will result in indexing errors
[36m(task, pid=6695)[0m [2025-04-22 06:06:25 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:25 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:26 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.03, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:27 TP0] Prefill batch. #new-seq: 1, #new-token: 5425, #cached-token: 0, token usage: 0.04, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:29] INFO:     127.0.0.1:37274 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:29] INFO:     127.0.0.1:37290 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:29 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:29 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:29] INFO:     127.0.0.1:37292 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:29] INFO:     127.0.0.1:37300 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:29] INFO:     127.0.0.1:37304 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:29] INFO:     127.0.0.1:37318 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:29] INFO:     127.0.0.1:37332 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:29] INFO:     127.0.0.1:37336 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:29] INFO:     127.0.0.1:37346 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:29] INFO:     127.0.0.1:37362 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:29] INFO:     127.0.0.1:37366 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:29] INFO:     127.0.0.1:37380 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:29] INFO:     127.0.0.1:37388 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:30] INFO:     127.0.0.1:37398 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:30 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.03, #running-req: 0, #queue-req: 13, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:30] INFO:     127.0.0.1:37412 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:30] INFO:     127.0.0.1:37414 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:30] INFO:     127.0.0.1:37424 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:30] INFO:     127.0.0.1:37432 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:30] INFO:     127.0.0.1:37442 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:30] INFO:     127.0.0.1:37448 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:30] INFO:     127.0.0.1:37462 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:30] INFO:     127.0.0.1:37464 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:30 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.04, #running-req: 0, #queue-req: 19, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:30] INFO:     127.0.0.1:37480 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:31] INFO:     127.0.0.1:37490 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:31] INFO:     127.0.0.1:37506 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:31 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.05, #running-req: 1, #queue-req: 22, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:31] INFO:     127.0.0.1:37516 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:31] INFO:     127.0.0.1:37532 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:31] INFO:     127.0.0.1:37546 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:31 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.06, #running-req: 1, #queue-req: 25, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:31] INFO:     127.0.0.1:37550 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:31] INFO:     127.0.0.1:37564 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:31] INFO:     127.0.0.1:37568 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:31] INFO:     127.0.0.1:37580 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:31] INFO:     127.0.0.1:37596 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:32 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.08, #running-req: 1, #queue-req: 31, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:32] INFO:     127.0.0.1:37604 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:32] INFO:     127.0.0.1:37618 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:32] INFO:     127.0.0.1:37632 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:32] INFO:     127.0.0.1:37642 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:32] INFO:     127.0.0.1:37654 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:32 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.09, #running-req: 1, #queue-req: 35, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:32] INFO:     127.0.0.1:37666 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:32] INFO:     127.0.0.1:37680 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:32] INFO:     127.0.0.1:37682 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:33 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.10, #running-req: 2, #queue-req: 38, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:33] INFO:     127.0.0.1:37690 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:33] INFO:     127.0.0.1:37692 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:33] INFO:     127.0.0.1:37700 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:33] INFO:     127.0.0.1:37708 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:33] INFO:     127.0.0.1:37718 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:33 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.11, #running-req: 2, #queue-req: 42, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:33] INFO:     127.0.0.1:37722 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:33] INFO:     127.0.0.1:37732 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:33] INFO:     127.0.0.1:37734 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:34] INFO:     127.0.0.1:37748 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:06:34 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.13, #running-req: 2, #queue-req: 46, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:34 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.14, #running-req: 3, #queue-req: 46, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:35 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.15, #running-req: 3, #queue-req: 46, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:35 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.16, #running-req: 3, #queue-req: 46, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:36 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.18, #running-req: 3, #queue-req: 45, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:36 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.19, #running-req: 4, #queue-req: 45, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:37 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.20, #running-req: 4, #queue-req: 45, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:37 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.21, #running-req: 4, #queue-req: 45, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:38 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.23, #running-req: 4, #queue-req: 44, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:38 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.24, #running-req: 5, #queue-req: 44, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:39 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.25, #running-req: 5, #queue-req: 44, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:40 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.26, #running-req: 5, #queue-req: 43, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:40 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.28, #running-req: 6, #queue-req: 43, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:41 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.29, #running-req: 6, #queue-req: 43, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:41 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.30, #running-req: 6, #queue-req: 43, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:42 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.31, #running-req: 6, #queue-req: 42, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:42 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.33, #running-req: 7, #queue-req: 42, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:43 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.34, #running-req: 7, #queue-req: 42, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:43 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.35, #running-req: 7, #queue-req: 42, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:44 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.36, #running-req: 7, #queue-req: 41, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:44 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.38, #running-req: 8, #queue-req: 41, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:45 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.39, #running-req: 8, #queue-req: 41, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:45 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.40, #running-req: 8, #queue-req: 40, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:46 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.41, #running-req: 9, #queue-req: 40, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:46 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.43, #running-req: 9, #queue-req: 40, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:47 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.44, #running-req: 9, #queue-req: 40, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:47 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.45, #running-req: 9, #queue-req: 39, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:48 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.46, #running-req: 10, #queue-req: 39, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:48 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.48, #running-req: 10, #queue-req: 39, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:49 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.49, #running-req: 10, #queue-req: 39, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:49 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.50, #running-req: 10, #queue-req: 38, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:50 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.51, #running-req: 11, #queue-req: 38, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:50 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.53, #running-req: 11, #queue-req: 38, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:51 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.54, #running-req: 11, #queue-req: 37, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:52 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.55, #running-req: 12, #queue-req: 37, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:52 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.56, #running-req: 12, #queue-req: 37, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:53 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.58, #running-req: 12, #queue-req: 37, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:53 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.59, #running-req: 12, #queue-req: 36, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:54 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.60, #running-req: 13, #queue-req: 36, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:54 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.61, #running-req: 13, #queue-req: 36, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:55 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.63, #running-req: 13, #queue-req: 36, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:55 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.64, #running-req: 13, #queue-req: 35, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:56 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.65, #running-req: 14, #queue-req: 35, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:56 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.66, #running-req: 14, #queue-req: 35, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:57 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.68, #running-req: 14, #queue-req: 34, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:57 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.69, #running-req: 15, #queue-req: 34, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:58 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.70, #running-req: 15, #queue-req: 34, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:58 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.71, #running-req: 15, #queue-req: 34, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:59 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.73, #running-req: 15, #queue-req: 33, 
[36m(task, pid=6695)[0m [2025-04-22 06:06:59 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.74, #running-req: 16, #queue-req: 33, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:00 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.75, #running-req: 16, #queue-req: 33, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:00 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.76, #running-req: 16, #queue-req: 33, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:01 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.78, #running-req: 16, #queue-req: 32, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:01 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.79, #running-req: 17, #queue-req: 32, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:02 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.80, #running-req: 17, #queue-req: 32, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:02 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.81, #running-req: 17, #queue-req: 31, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:03 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.83, #running-req: 18, #queue-req: 31, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:04 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.84, #running-req: 18, #queue-req: 31, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:04 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.85, #running-req: 18, #queue-req: 31, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:05 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.86, #running-req: 18, #queue-req: 30, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:05 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.88, #running-req: 19, #queue-req: 30, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:06 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.89, #running-req: 19, #queue-req: 30, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:06 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.90, #running-req: 19, #queue-req: 30, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:07 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.91, #running-req: 19, #queue-req: 29, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:07 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.93, #running-req: 20, #queue-req: 29, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:08 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.94, #running-req: 20, #queue-req: 29, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:08 TP0] Prefill batch. #new-seq: 1, #new-token: 7429, #cached-token: 0, token usage: 0.95, #running-req: 20, #queue-req: 29, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:09 TP0] Decode batch. #running-req: 21, #token: 630084, token usage: 0.96, gen throughput (token/s): 7.30, #queue-req: 29, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:11 TP0] Decode batch. #running-req: 21, #token: 630924, token usage: 0.96, gen throughput (token/s): 580.24, #queue-req: 29, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:12 TP0] Decode batch. #running-req: 21, #token: 631764, token usage: 0.97, gen throughput (token/s): 572.58, #queue-req: 29, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:13 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 28, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:13 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.01, #running-req: 0, #queue-req: 28, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:13 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.03, #running-req: 0, #queue-req: 28, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:14 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.04, #running-req: 0, #queue-req: 27, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:14 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.05, #running-req: 1, #queue-req: 27, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:15 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.06, #running-req: 1, #queue-req: 27, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:15 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.08, #running-req: 1, #queue-req: 27, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:16 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.09, #running-req: 1, #queue-req: 26, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:16 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.10, #running-req: 2, #queue-req: 26, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:17 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.11, #running-req: 2, #queue-req: 26, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:18 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.13, #running-req: 2, #queue-req: 25, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:18 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.14, #running-req: 3, #queue-req: 25, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:19 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.15, #running-req: 3, #queue-req: 25, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:19 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.16, #running-req: 3, #queue-req: 25, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:20 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.18, #running-req: 3, #queue-req: 24, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:20 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.19, #running-req: 4, #queue-req: 24, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:21 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.20, #running-req: 4, #queue-req: 24, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:21 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.21, #running-req: 4, #queue-req: 24, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:22 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.23, #running-req: 4, #queue-req: 23, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:22 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.24, #running-req: 5, #queue-req: 23, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:23 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.25, #running-req: 5, #queue-req: 23, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:23 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.26, #running-req: 5, #queue-req: 22, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:24 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.28, #running-req: 6, #queue-req: 22, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:24 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.29, #running-req: 6, #queue-req: 22, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:25 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.30, #running-req: 6, #queue-req: 22, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:25 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.31, #running-req: 6, #queue-req: 21, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:26 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.33, #running-req: 7, #queue-req: 21, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:26 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.34, #running-req: 7, #queue-req: 21, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:27 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.35, #running-req: 7, #queue-req: 21, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:27 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.36, #running-req: 7, #queue-req: 20, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:28 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.38, #running-req: 8, #queue-req: 20, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:28 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.39, #running-req: 8, #queue-req: 20, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:29 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.40, #running-req: 8, #queue-req: 19, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:30 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.41, #running-req: 9, #queue-req: 19, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:30 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.43, #running-req: 9, #queue-req: 19, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:31 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.44, #running-req: 9, #queue-req: 19, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:31 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.45, #running-req: 9, #queue-req: 18, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:32 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.46, #running-req: 10, #queue-req: 18, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:32 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.48, #running-req: 10, #queue-req: 18, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:33 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.49, #running-req: 10, #queue-req: 18, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:33 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.50, #running-req: 10, #queue-req: 17, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:34 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.51, #running-req: 11, #queue-req: 17, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:34 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.53, #running-req: 11, #queue-req: 17, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:35 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.54, #running-req: 11, #queue-req: 16, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:35 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.55, #running-req: 12, #queue-req: 16, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:36 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.56, #running-req: 12, #queue-req: 16, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:36 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.58, #running-req: 12, #queue-req: 16, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:37 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.59, #running-req: 12, #queue-req: 15, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:37 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.60, #running-req: 13, #queue-req: 15, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:38 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.61, #running-req: 13, #queue-req: 15, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:38 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.63, #running-req: 13, #queue-req: 15, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:39 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.64, #running-req: 13, #queue-req: 14, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:39 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.65, #running-req: 14, #queue-req: 14, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:40 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.66, #running-req: 14, #queue-req: 14, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:41 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.68, #running-req: 14, #queue-req: 13, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:41 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.69, #running-req: 15, #queue-req: 13, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:42 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.70, #running-req: 15, #queue-req: 13, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:42 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.71, #running-req: 15, #queue-req: 13, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:43 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.73, #running-req: 15, #queue-req: 12, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:43 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.74, #running-req: 16, #queue-req: 12, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:44 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.75, #running-req: 16, #queue-req: 12, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:44 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.76, #running-req: 16, #queue-req: 12, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:45 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.78, #running-req: 16, #queue-req: 11, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:45 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.79, #running-req: 17, #queue-req: 11, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:46 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.80, #running-req: 17, #queue-req: 11, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:46 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.81, #running-req: 17, #queue-req: 10, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:47 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.83, #running-req: 18, #queue-req: 10, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:47 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.84, #running-req: 18, #queue-req: 10, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:48 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.85, #running-req: 18, #queue-req: 10, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:48 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.86, #running-req: 18, #queue-req: 9, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:49 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.88, #running-req: 19, #queue-req: 9, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:49 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.89, #running-req: 19, #queue-req: 9, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:50 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.90, #running-req: 19, #queue-req: 9, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:51 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.91, #running-req: 19, #queue-req: 8, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:51 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.93, #running-req: 20, #queue-req: 8, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:52 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.94, #running-req: 20, #queue-req: 8, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:52 TP0] Prefill batch. #new-seq: 1, #new-token: 7429, #cached-token: 0, token usage: 0.95, #running-req: 20, #queue-req: 8, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:54 TP0] Decode batch. #running-req: 21, #token: 630504, token usage: 0.96, gen throughput (token/s): 20.13, #queue-req: 8, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:55 TP0] Decode batch. #running-req: 21, #token: 631344, token usage: 0.96, gen throughput (token/s): 579.06, #queue-req: 8, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:57 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 7, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:57 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.01, #running-req: 0, #queue-req: 7, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:57 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.03, #running-req: 0, #queue-req: 7, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:58 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.04, #running-req: 0, #queue-req: 6, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:58 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.05, #running-req: 1, #queue-req: 6, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:59 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.06, #running-req: 1, #queue-req: 6, 
[36m(task, pid=6695)[0m [2025-04-22 06:07:59 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.08, #running-req: 1, #queue-req: 6, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:00 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.09, #running-req: 1, #queue-req: 5, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:00 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.10, #running-req: 2, #queue-req: 5, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:01 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.11, #running-req: 2, #queue-req: 5, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:02 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.13, #running-req: 2, #queue-req: 4, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:02 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.14, #running-req: 3, #queue-req: 4, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:03 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.15, #running-req: 3, #queue-req: 4, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:03 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.16, #running-req: 3, #queue-req: 4, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:04 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.18, #running-req: 3, #queue-req: 3, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:04 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.19, #running-req: 4, #queue-req: 3, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:05 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.20, #running-req: 4, #queue-req: 3, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:05 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.21, #running-req: 4, #queue-req: 3, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:06 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.23, #running-req: 4, #queue-req: 2, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:06 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.24, #running-req: 5, #queue-req: 2, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:07 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.25, #running-req: 5, #queue-req: 2, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:07 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.26, #running-req: 5, #queue-req: 1, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:08 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.28, #running-req: 6, #queue-req: 1, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:08 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.29, #running-req: 6, #queue-req: 1, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:09 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.30, #running-req: 6, #queue-req: 1, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:09 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.31, #running-req: 6, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:10 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.33, #running-req: 7, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:10 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.34, #running-req: 7, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:11 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.35, #running-req: 7, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:11 TP0] Prefill batch. #new-seq: 1, #new-token: 2440, #cached-token: 0, token usage: 0.36, #running-req: 7, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:12 TP0] Decode batch. #running-req: 8, #token: 240032, token usage: 0.37, gen throughput (token/s): 48.08, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:13 TP0] Decode batch. #running-req: 8, #token: 240352, token usage: 0.37, gen throughput (token/s): 349.32, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:14 TP0] Decode batch. #running-req: 8, #token: 240672, token usage: 0.37, gen throughput (token/s): 347.63, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:14] INFO:     127.0.0.1:45518 - "GET /get_server_info HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:17] INFO:     127.0.0.1:55320 - "GET /v1/models HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:21] INFO:     127.0.0.1:55326 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:21 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:22 TP0] Decode batch. #running-req: 1, #token: 1024, token usage: 0.00, gen throughput (token/s): 21.55, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:23] INFO:     127.0.0.1:55330 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:23 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:23] INFO:     127.0.0.1:55346 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:23 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:23] INFO:     127.0.0.1:55360 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:23] INFO:     127.0.0.1:55376 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:23] INFO:     127.0.0.1:55386 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:23] INFO:     127.0.0.1:55394 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:23] INFO:     127.0.0.1:55406 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:23] INFO:     127.0.0.1:55412 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:23 TP0] Prefill batch. #new-seq: 6, #new-token: 6006, #cached-token: 0, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:23] INFO:     127.0.0.1:55422 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:23] INFO:     127.0.0.1:55434 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:23] INFO:     127.0.0.1:55448 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:23] INFO:     127.0.0.1:55464 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:23 TP0] Prefill batch. #new-seq: 4, #new-token: 4004, #cached-token: 0, token usage: 0.01, #running-req: 8, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:23] INFO:     127.0.0.1:55472 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:23] INFO:     127.0.0.1:55474 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:23 TP0] Prefill batch. #new-seq: 2, #new-token: 2002, #cached-token: 0, token usage: 0.02, #running-req: 12, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:24] INFO:     127.0.0.1:55480 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:24] INFO:     127.0.0.1:55496 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:24] INFO:     127.0.0.1:55502 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:24 TP0] Prefill batch. #new-seq: 2, #new-token: 2002, #cached-token: 0, token usage: 0.02, #running-req: 14, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:24] INFO:     127.0.0.1:55518 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:24] INFO:     127.0.0.1:55526 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:24] INFO:     127.0.0.1:55528 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:24] INFO:     127.0.0.1:55544 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:24 TP0] Prefill batch. #new-seq: 4, #new-token: 4004, #cached-token: 0, token usage: 0.02, #running-req: 16, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:24 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.03, #running-req: 20, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:24] INFO:     127.0.0.1:55546 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:24 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.03, #running-req: 21, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:24] INFO:     127.0.0.1:55562 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:24 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.03, #running-req: 22, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:24] INFO:     127.0.0.1:37850 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:25] INFO:     127.0.0.1:37858 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:25 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.04, #running-req: 23, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:25 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.04, #running-req: 24, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:25] INFO:     127.0.0.1:37870 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:25 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.04, #running-req: 25, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:25] INFO:     127.0.0.1:37874 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:25] INFO:     127.0.0.1:37884 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:25] INFO:     127.0.0.1:37896 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:25 TP0] Prefill batch. #new-seq: 3, #new-token: 3003, #cached-token: 0, token usage: 0.04, #running-req: 26, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:25] INFO:     127.0.0.1:37910 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:25 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.04, #running-req: 29, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:25] INFO:     127.0.0.1:37916 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:25] INFO:     127.0.0.1:37924 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:25 TP0] Prefill batch. #new-seq: 2, #new-token: 2002, #cached-token: 0, token usage: 0.05, #running-req: 30, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:25] INFO:     127.0.0.1:37926 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:25 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.05, #running-req: 32, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:26] INFO:     127.0.0.1:37938 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:26 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.05, #running-req: 33, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:26] INFO:     127.0.0.1:37950 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:26 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.05, #running-req: 34, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:26] INFO:     127.0.0.1:37958 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:26] INFO:     127.0.0.1:37970 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:26 TP0] Prefill batch. #new-seq: 2, #new-token: 2002, #cached-token: 0, token usage: 0.05, #running-req: 35, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:26] INFO:     127.0.0.1:37972 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:26 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.06, #running-req: 37, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:26] INFO:     127.0.0.1:37986 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:26] INFO:     127.0.0.1:37996 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:26 TP0] Prefill batch. #new-seq: 2, #new-token: 2002, #cached-token: 0, token usage: 0.06, #running-req: 38, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:26] INFO:     127.0.0.1:38002 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:26 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.06, #running-req: 40, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:27] INFO:     127.0.0.1:38012 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:27 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.06, #running-req: 41, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:27] INFO:     127.0.0.1:38016 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:27 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.07, #running-req: 42, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:27] INFO:     127.0.0.1:38018 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:27 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.07, #running-req: 43, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:27] INFO:     127.0.0.1:38034 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:27] INFO:     127.0.0.1:38050 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:27] INFO:     127.0.0.1:38054 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:27 TP0] Prefill batch. #new-seq: 3, #new-token: 3003, #cached-token: 0, token usage: 0.07, #running-req: 44, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:27] INFO:     127.0.0.1:38060 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:27 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.07, #running-req: 47, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:27] INFO:     127.0.0.1:38074 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:27] INFO:     127.0.0.1:38080 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:08:28 TP0] Prefill batch. #new-seq: 2, #new-token: 2002, #cached-token: 0, token usage: 0.07, #running-req: 48, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:28 TP0] Decode batch. #running-req: 50, #token: 51142, token usage: 0.08, gen throughput (token/s): 170.23, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:29 TP0] Decode batch. #running-req: 50, #token: 53142, token usage: 0.08, gen throughput (token/s): 1498.28, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:30 TP0] Decode batch. #running-req: 50, #token: 55142, token usage: 0.08, gen throughput (token/s): 1485.44, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:32 TP0] Decode batch. #running-req: 50, #token: 57142, token usage: 0.09, gen throughput (token/s): 1470.14, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:33 TP0] Decode batch. #running-req: 50, #token: 59142, token usage: 0.09, gen throughput (token/s): 1457.06, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:35 TP0] Decode batch. #running-req: 50, #token: 61142, token usage: 0.09, gen throughput (token/s): 1444.35, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:36 TP0] Decode batch. #running-req: 50, #token: 63142, token usage: 0.10, gen throughput (token/s): 1445.45, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:37 TP0] Decode batch. #running-req: 50, #token: 65142, token usage: 0.10, gen throughput (token/s): 1439.90, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:39 TP0] Decode batch. #running-req: 50, #token: 67142, token usage: 0.10, gen throughput (token/s): 1438.67, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:40 TP0] Decode batch. #running-req: 50, #token: 69142, token usage: 0.11, gen throughput (token/s): 1444.80, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:42 TP0] Decode batch. #running-req: 50, #token: 71142, token usage: 0.11, gen throughput (token/s): 1441.07, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:43 TP0] Decode batch. #running-req: 50, #token: 73142, token usage: 0.11, gen throughput (token/s): 1436.87, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:44 TP0] Decode batch. #running-req: 50, #token: 75142, token usage: 0.11, gen throughput (token/s): 1432.37, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:46 TP0] Decode batch. #running-req: 50, #token: 77142, token usage: 0.12, gen throughput (token/s): 1432.20, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:47 TP0] Decode batch. #running-req: 50, #token: 79142, token usage: 0.12, gen throughput (token/s): 1439.42, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:49 TP0] Decode batch. #running-req: 50, #token: 81142, token usage: 0.12, gen throughput (token/s): 1433.33, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:50 TP0] Decode batch. #running-req: 50, #token: 83142, token usage: 0.13, gen throughput (token/s): 1432.20, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:51 TP0] Decode batch. #running-req: 50, #token: 85142, token usage: 0.13, gen throughput (token/s): 1430.73, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:53 TP0] Decode batch. #running-req: 50, #token: 87142, token usage: 0.13, gen throughput (token/s): 1430.02, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:54 TP0] Decode batch. #running-req: 50, #token: 89142, token usage: 0.14, gen throughput (token/s): 1429.23, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:56 TP0] Decode batch. #running-req: 50, #token: 91142, token usage: 0.14, gen throughput (token/s): 1425.17, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:57 TP0] Decode batch. #running-req: 50, #token: 93142, token usage: 0.14, gen throughput (token/s): 1421.35, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:08:58 TP0] Decode batch. #running-req: 50, #token: 95142, token usage: 0.15, gen throughput (token/s): 1425.98, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:00 TP0] Decode batch. #running-req: 50, #token: 97142, token usage: 0.15, gen throughput (token/s): 1419.80, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:01 TP0] Decode batch. #running-req: 50, #token: 99142, token usage: 0.15, gen throughput (token/s): 1423.48, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:03 TP0] Decode batch. #running-req: 50, #token: 101142, token usage: 0.15, gen throughput (token/s): 1416.25, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:04 TP0] Decode batch. #running-req: 50, #token: 103142, token usage: 0.16, gen throughput (token/s): 1420.64, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:05 TP0] Decode batch. #running-req: 50, #token: 105142, token usage: 0.16, gen throughput (token/s): 1412.52, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:07 TP0] Decode batch. #running-req: 50, #token: 107142, token usage: 0.16, gen throughput (token/s): 1423.42, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:08 TP0] Decode batch. #running-req: 50, #token: 109142, token usage: 0.17, gen throughput (token/s): 1420.10, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:10 TP0] Decode batch. #running-req: 50, #token: 111142, token usage: 0.17, gen throughput (token/s): 1421.35, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:11 TP0] Decode batch. #running-req: 50, #token: 113142, token usage: 0.17, gen throughput (token/s): 1419.06, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:12 TP0] Decode batch. #running-req: 50, #token: 115142, token usage: 0.18, gen throughput (token/s): 1419.73, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:14 TP0] Decode batch. #running-req: 50, #token: 117142, token usage: 0.18, gen throughput (token/s): 1421.82, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:15 TP0] Decode batch. #running-req: 50, #token: 119142, token usage: 0.18, gen throughput (token/s): 1415.95, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:17 TP0] Decode batch. #running-req: 50, #token: 121142, token usage: 0.19, gen throughput (token/s): 1413.60, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:18 TP0] Decode batch. #running-req: 50, #token: 123142, token usage: 0.19, gen throughput (token/s): 1409.24, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:19 TP0] Decode batch. #running-req: 50, #token: 125142, token usage: 0.19, gen throughput (token/s): 1411.01, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:21 TP0] Decode batch. #running-req: 50, #token: 127142, token usage: 0.19, gen throughput (token/s): 1419.73, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:22 TP0] Decode batch. #running-req: 50, #token: 129142, token usage: 0.20, gen throughput (token/s): 1412.51, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:24 TP0] Decode batch. #running-req: 50, #token: 131142, token usage: 0.20, gen throughput (token/s): 1411.04, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:25 TP0] Decode batch. #running-req: 50, #token: 133142, token usage: 0.20, gen throughput (token/s): 1404.14, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:27 TP0] Decode batch. #running-req: 50, #token: 135142, token usage: 0.21, gen throughput (token/s): 1410.09, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:28 TP0] Decode batch. #running-req: 50, #token: 137142, token usage: 0.21, gen throughput (token/s): 1406.55, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:29 TP0] Decode batch. #running-req: 50, #token: 139142, token usage: 0.21, gen throughput (token/s): 1406.65, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:31 TP0] Decode batch. #running-req: 50, #token: 141142, token usage: 0.22, gen throughput (token/s): 1406.79, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:32 TP0] Decode batch. #running-req: 50, #token: 143142, token usage: 0.22, gen throughput (token/s): 1401.40, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:34 TP0] Decode batch. #running-req: 50, #token: 145142, token usage: 0.22, gen throughput (token/s): 1399.25, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:35 TP0] Decode batch. #running-req: 50, #token: 147142, token usage: 0.22, gen throughput (token/s): 1401.18, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:37 TP0] Decode batch. #running-req: 50, #token: 149142, token usage: 0.23, gen throughput (token/s): 1394.30, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:38] INFO:     127.0.0.1:58690 - "GET /get_server_info HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:40] INFO:     127.0.0.1:58698 - "GET /v1/models HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:44] INFO:     127.0.0.1:51514 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:44 TP0] Prefill batch. #new-seq: 1, #new-token: 5001, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:45 TP0] Decode batch. #running-req: 1, #token: 5005, token usage: 0.01, gen throughput (token/s): 116.08, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:46] INFO:     127.0.0.1:51522 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:46 TP0] Prefill batch. #new-seq: 1, #new-token: 5001, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:46] INFO:     127.0.0.1:51526 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:46] INFO:     127.0.0.1:51542 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:46] INFO:     127.0.0.1:51558 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:46] INFO:     127.0.0.1:51574 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:46] INFO:     127.0.0.1:51584 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:46] INFO:     127.0.0.1:51598 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:47] INFO:     127.0.0.1:51602 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:47 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.01, #running-req: 1, #queue-req: 5, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:47] INFO:     127.0.0.1:51618 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:47] INFO:     127.0.0.1:51634 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:47] INFO:     127.0.0.1:51640 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:47] INFO:     127.0.0.1:51646 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:47 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.02, #running-req: 2, #queue-req: 7, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:47] INFO:     127.0.0.1:51648 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:47] INFO:     127.0.0.1:51654 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:47] INFO:     127.0.0.1:51670 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:47] INFO:     127.0.0.1:51682 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:47 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.03, #running-req: 4, #queue-req: 10, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:47] INFO:     127.0.0.1:51688 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:47] INFO:     127.0.0.1:51694 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:47] INFO:     127.0.0.1:51700 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:47] INFO:     127.0.0.1:51706 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:47] INFO:     127.0.0.1:51712 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:48] INFO:     127.0.0.1:51720 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:48 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.05, #running-req: 5, #queue-req: 14, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:48] INFO:     127.0.0.1:51724 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:48] INFO:     127.0.0.1:51728 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:48 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.06, #running-req: 7, #queue-req: 14, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:48] INFO:     127.0.0.1:51734 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:48] INFO:     127.0.0.1:51736 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:48 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.07, #running-req: 9, #queue-req: 15, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:48] INFO:     127.0.0.1:51748 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:49] INFO:     127.0.0.1:51762 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:49] INFO:     127.0.0.1:51770 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:49] INFO:     127.0.0.1:51780 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:49] INFO:     127.0.0.1:51782 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:49] INFO:     127.0.0.1:51790 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:49] INFO:     127.0.0.1:51800 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:49 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.08, #running-req: 10, #queue-req: 20, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:49] INFO:     127.0.0.1:51802 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:49] INFO:     127.0.0.1:51812 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:49] INFO:     127.0.0.1:51818 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:49 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.10, #running-req: 12, #queue-req: 20, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:49] INFO:     127.0.0.1:51826 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:49] INFO:     127.0.0.1:51830 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:50] INFO:     127.0.0.1:51844 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:50] INFO:     127.0.0.1:51860 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:50 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.11, #running-req: 14, #queue-req: 24, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:50] INFO:     127.0.0.1:51870 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:50 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.12, #running-req: 15, #queue-req: 23, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:50] INFO:     127.0.0.1:51874 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:50] INFO:     127.0.0.1:51890 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:50] INFO:     127.0.0.1:51894 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:51] INFO:     127.0.0.1:51902 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:51 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.13, #running-req: 17, #queue-req: 24, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:51] INFO:     127.0.0.1:51914 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:51] INFO:     127.0.0.1:51918 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:51] INFO:     127.0.0.1:51928 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:51] INFO:     127.0.0.1:51932 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:51] INFO:     127.0.0.1:51944 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:09:51 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.15, #running-req: 19, #queue-req: 29, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:51 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.16, #running-req: 20, #queue-req: 27, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:52 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.17, #running-req: 22, #queue-req: 26, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:52 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.18, #running-req: 23, #queue-req: 24, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:53 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.20, #running-req: 25, #queue-req: 22, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:53 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.21, #running-req: 27, #queue-req: 21, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:54 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.22, #running-req: 28, #queue-req: 19, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:54 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.23, #running-req: 30, #queue-req: 17, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:54 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.25, #running-req: 32, #queue-req: 16, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:55 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.26, #running-req: 33, #queue-req: 14, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:55 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.27, #running-req: 35, #queue-req: 12, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:56 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.28, #running-req: 37, #queue-req: 11, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:56 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.30, #running-req: 38, #queue-req: 9, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:57 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.31, #running-req: 40, #queue-req: 8, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:57 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.32, #running-req: 41, #queue-req: 6, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:57 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.33, #running-req: 43, #queue-req: 4, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:58 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.35, #running-req: 45, #queue-req: 3, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:58 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.36, #running-req: 46, #queue-req: 1, 
[36m(task, pid=6695)[0m [2025-04-22 06:09:59 TP0] Prefill batch. #new-seq: 2, #new-token: 7481, #cached-token: 0, token usage: 0.37, #running-req: 48, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:00 TP0] Decode batch. #running-req: 50, #token: 250601, token usage: 0.38, gen throughput (token/s): 35.88, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:01 TP0] Decode batch. #running-req: 50, #token: 252601, token usage: 0.39, gen throughput (token/s): 1318.41, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:03 TP0] Decode batch. #running-req: 50, #token: 254601, token usage: 0.39, gen throughput (token/s): 1295.75, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:04 TP0] Decode batch. #running-req: 50, #token: 256601, token usage: 0.39, gen throughput (token/s): 1286.29, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:06 TP0] Decode batch. #running-req: 50, #token: 258601, token usage: 0.40, gen throughput (token/s): 1279.67, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:07 TP0] Decode batch. #running-req: 50, #token: 260601, token usage: 0.40, gen throughput (token/s): 1274.93, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:09 TP0] Decode batch. #running-req: 50, #token: 262601, token usage: 0.40, gen throughput (token/s): 1273.21, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:10 TP0] Decode batch. #running-req: 50, #token: 264601, token usage: 0.40, gen throughput (token/s): 1269.65, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:12 TP0] Decode batch. #running-req: 50, #token: 266601, token usage: 0.41, gen throughput (token/s): 1262.69, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:14 TP0] Decode batch. #running-req: 50, #token: 268601, token usage: 0.41, gen throughput (token/s): 1264.24, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:15 TP0] Decode batch. #running-req: 50, #token: 270601, token usage: 0.41, gen throughput (token/s): 1264.03, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:17 TP0] Decode batch. #running-req: 50, #token: 272601, token usage: 0.42, gen throughput (token/s): 1261.28, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:18 TP0] Decode batch. #running-req: 50, #token: 274601, token usage: 0.42, gen throughput (token/s): 1254.21, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:20 TP0] Decode batch. #running-req: 50, #token: 276601, token usage: 0.42, gen throughput (token/s): 1256.85, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:22 TP0] Decode batch. #running-req: 50, #token: 278601, token usage: 0.43, gen throughput (token/s): 1257.48, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:23 TP0] Decode batch. #running-req: 50, #token: 280601, token usage: 0.43, gen throughput (token/s): 1257.66, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:25 TP0] Decode batch. #running-req: 50, #token: 282601, token usage: 0.43, gen throughput (token/s): 1257.48, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:26 TP0] Decode batch. #running-req: 50, #token: 284601, token usage: 0.43, gen throughput (token/s): 1252.09, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:28 TP0] Decode batch. #running-req: 50, #token: 286601, token usage: 0.44, gen throughput (token/s): 1256.64, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:30 TP0] Decode batch. #running-req: 50, #token: 288601, token usage: 0.44, gen throughput (token/s): 1252.35, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:31 TP0] Decode batch. #running-req: 50, #token: 290601, token usage: 0.44, gen throughput (token/s): 1252.18, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:33 TP0] Decode batch. #running-req: 50, #token: 292601, token usage: 0.45, gen throughput (token/s): 1250.57, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:34 TP0] Decode batch. #running-req: 50, #token: 294601, token usage: 0.45, gen throughput (token/s): 1253.09, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:36 TP0] Decode batch. #running-req: 50, #token: 296601, token usage: 0.45, gen throughput (token/s): 1249.53, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:38 TP0] Decode batch. #running-req: 50, #token: 298601, token usage: 0.46, gen throughput (token/s): 1252.33, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:39] INFO:     127.0.0.1:57010 - "GET /get_server_info HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:41] INFO:     127.0.0.1:57014 - "GET /v1/models HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:45] INFO:     127.0.0.1:44086 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:45 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:45 TP0] Prefill batch. #new-seq: 1, #new-token: 1809, #cached-token: 0, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:46 TP0] Decode batch. #running-req: 1, #token: 10012, token usage: 0.02, gen throughput (token/s): 176.65, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:47] INFO:     127.0.0.1:44098 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:47 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:47 TP0] Prefill batch. #new-seq: 1, #new-token: 1809, #cached-token: 0, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:47] INFO:     127.0.0.1:44106 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:48] INFO:     127.0.0.1:44108 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:48] INFO:     127.0.0.1:44120 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:48] INFO:     127.0.0.1:44134 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:48] INFO:     127.0.0.1:44142 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:48] INFO:     127.0.0.1:44152 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:48] INFO:     127.0.0.1:44156 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:48] INFO:     127.0.0.1:44166 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:48] INFO:     127.0.0.1:44174 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:48] INFO:     127.0.0.1:44176 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:48] INFO:     127.0.0.1:44188 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:48 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.02, #running-req: 1, #queue-req: 10, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:48] INFO:     127.0.0.1:44190 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:48] INFO:     127.0.0.1:44192 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:48] INFO:     127.0.0.1:44202 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:48] INFO:     127.0.0.1:44208 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:48 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.03, #running-req: 1, #queue-req: 12, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:48] INFO:     127.0.0.1:44210 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:48] INFO:     127.0.0.1:44224 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:49] INFO:     127.0.0.1:44234 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:49] INFO:     127.0.0.1:44250 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:49] INFO:     127.0.0.1:44252 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:49] INFO:     127.0.0.1:44258 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:49 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.04, #running-req: 2, #queue-req: 18, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:49] INFO:     127.0.0.1:44266 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:49] INFO:     127.0.0.1:44280 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:49 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.05, #running-req: 3, #queue-req: 19, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:49] INFO:     127.0.0.1:44288 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:49] INFO:     127.0.0.1:44290 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:50] INFO:     127.0.0.1:44292 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:50 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.07, #running-req: 4, #queue-req: 20, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:50] INFO:     127.0.0.1:44304 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:50] INFO:     127.0.0.1:44314 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:50] INFO:     127.0.0.1:44320 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:50] INFO:     127.0.0.1:44328 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:50] INFO:     127.0.0.1:44334 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:50] INFO:     127.0.0.1:44340 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:50 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.08, #running-req: 5, #queue-req: 27, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:50] INFO:     127.0.0.1:44356 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:50] INFO:     127.0.0.1:44360 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:50] INFO:     127.0.0.1:44368 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:51] INFO:     127.0.0.1:44376 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:51] INFO:     127.0.0.1:44386 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:51 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.09, #running-req: 5, #queue-req: 31, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:51] INFO:     127.0.0.1:44402 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:51] INFO:     127.0.0.1:44414 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:51] INFO:     127.0.0.1:44422 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:51 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.10, #running-req: 6, #queue-req: 33, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:51] INFO:     127.0.0.1:44432 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:52 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.12, #running-req: 7, #queue-req: 33, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:52] INFO:     127.0.0.1:44438 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:52] INFO:     127.0.0.1:44440 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:52] INFO:     127.0.0.1:44446 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:52] INFO:     127.0.0.1:44448 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:52] INFO:     127.0.0.1:44454 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:52 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.13, #running-req: 8, #queue-req: 37, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:52] INFO:     127.0.0.1:44458 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:52] INFO:     127.0.0.1:44468 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:52] INFO:     127.0.0.1:44478 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:10:53 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.14, #running-req: 9, #queue-req: 39, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:53 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.15, #running-req: 10, #queue-req: 39, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:54 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.17, #running-req: 10, #queue-req: 38, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:54 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.18, #running-req: 11, #queue-req: 37, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:54 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.19, #running-req: 12, #queue-req: 36, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:55 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.20, #running-req: 13, #queue-req: 35, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:55 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.22, #running-req: 14, #queue-req: 35, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:56 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.23, #running-req: 14, #queue-req: 34, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:56 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.24, #running-req: 15, #queue-req: 33, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:57 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.25, #running-req: 16, #queue-req: 32, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:57 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.27, #running-req: 17, #queue-req: 31, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:58 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.28, #running-req: 18, #queue-req: 30, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:58 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.29, #running-req: 19, #queue-req: 30, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:59 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.30, #running-req: 19, #queue-req: 29, 
[36m(task, pid=6695)[0m [2025-04-22 06:10:59 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.32, #running-req: 20, #queue-req: 28, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:00 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.33, #running-req: 21, #queue-req: 27, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:00 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.34, #running-req: 22, #queue-req: 26, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:01 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.35, #running-req: 23, #queue-req: 26, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:01 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.37, #running-req: 23, #queue-req: 25, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:02 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.38, #running-req: 24, #queue-req: 24, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:02 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.39, #running-req: 25, #queue-req: 23, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:03 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.40, #running-req: 26, #queue-req: 22, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:03 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.42, #running-req: 27, #queue-req: 21, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:04 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.43, #running-req: 28, #queue-req: 21, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:04 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.44, #running-req: 28, #queue-req: 20, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:05 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.45, #running-req: 29, #queue-req: 19, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:05 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.47, #running-req: 30, #queue-req: 18, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:06 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.48, #running-req: 31, #queue-req: 17, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:06 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.49, #running-req: 32, #queue-req: 17, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:07 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.50, #running-req: 32, #queue-req: 16, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:07 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.52, #running-req: 33, #queue-req: 15, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:07 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.53, #running-req: 34, #queue-req: 14, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:08 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.54, #running-req: 35, #queue-req: 13, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:08 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.55, #running-req: 36, #queue-req: 12, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:09 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.57, #running-req: 37, #queue-req: 12, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:09 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.58, #running-req: 37, #queue-req: 11, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:10 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.59, #running-req: 38, #queue-req: 10, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:10 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.60, #running-req: 39, #queue-req: 9, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:11 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.62, #running-req: 40, #queue-req: 8, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:11 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.63, #running-req: 41, #queue-req: 8, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:12 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.64, #running-req: 41, #queue-req: 7, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:12 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.65, #running-req: 42, #queue-req: 6, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:13 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.67, #running-req: 43, #queue-req: 5, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:13 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.68, #running-req: 44, #queue-req: 4, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:14 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.69, #running-req: 45, #queue-req: 3, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:14 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.70, #running-req: 46, #queue-req: 3, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:15 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.72, #running-req: 46, #queue-req: 2, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:15 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.73, #running-req: 47, #queue-req: 1, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:16 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.74, #running-req: 48, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:16 TP0] Prefill batch. #new-seq: 1, #new-token: 6721, #cached-token: 0, token usage: 0.75, #running-req: 49, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:18 TP0] Decode batch. #running-req: 50, #token: 501000, token usage: 0.77, gen throughput (token/s): 29.22, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:19 TP0] Decode batch. #running-req: 50, #token: 503000, token usage: 0.77, gen throughput (token/s): 1132.98, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:21 TP0] Decode batch. #running-req: 50, #token: 505000, token usage: 0.77, gen throughput (token/s): 1118.61, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:23 TP0] Decode batch. #running-req: 50, #token: 507000, token usage: 0.77, gen throughput (token/s): 1116.30, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:25 TP0] Decode batch. #running-req: 50, #token: 509000, token usage: 0.78, gen throughput (token/s): 1112.21, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:27 TP0] Decode batch. #running-req: 50, #token: 511000, token usage: 0.78, gen throughput (token/s): 1102.63, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:28 TP0] Decode batch. #running-req: 50, #token: 513000, token usage: 0.78, gen throughput (token/s): 1101.29, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:30 TP0] Decode batch. #running-req: 50, #token: 515000, token usage: 0.79, gen throughput (token/s): 1093.53, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:32 TP0] Decode batch. #running-req: 50, #token: 517000, token usage: 0.79, gen throughput (token/s): 1093.45, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:34 TP0] Decode batch. #running-req: 50, #token: 519000, token usage: 0.79, gen throughput (token/s): 1092.41, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:36 TP0] Decode batch. #running-req: 50, #token: 521000, token usage: 0.80, gen throughput (token/s): 1089.61, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:38 TP0] Decode batch. #running-req: 50, #token: 523000, token usage: 0.80, gen throughput (token/s): 1092.12, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:39 TP0] Decode batch. #running-req: 50, #token: 525000, token usage: 0.80, gen throughput (token/s): 1093.51, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:39] INFO:     127.0.0.1:41904 - "GET /get_server_info HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:42] INFO:     127.0.0.1:41916 - "GET /v1/models HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:47] INFO:     127.0.0.1:36848 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:47 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:47 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:48 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.03, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:48 TP0] Prefill batch. #new-seq: 1, #new-token: 5425, #cached-token: 0, token usage: 0.04, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:50] INFO:     127.0.0.1:36850 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:50] INFO:     127.0.0.1:36866 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:50 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:50 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:50] INFO:     127.0.0.1:36872 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:51] INFO:     127.0.0.1:36884 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:51] INFO:     127.0.0.1:36894 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:51] INFO:     127.0.0.1:36904 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:51] INFO:     127.0.0.1:36910 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:51] INFO:     127.0.0.1:36922 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:51] INFO:     127.0.0.1:36930 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:51] INFO:     127.0.0.1:36940 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:51] INFO:     127.0.0.1:36942 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:51] INFO:     127.0.0.1:36950 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:51] INFO:     127.0.0.1:36966 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:51] INFO:     127.0.0.1:36980 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:51 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.03, #running-req: 0, #queue-req: 13, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:51] INFO:     127.0.0.1:36984 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:51] INFO:     127.0.0.1:36986 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:51] INFO:     127.0.0.1:36996 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:51] INFO:     127.0.0.1:37004 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:51] INFO:     127.0.0.1:37014 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:51] INFO:     127.0.0.1:37016 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:52] INFO:     127.0.0.1:37026 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:52] INFO:     127.0.0.1:37038 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:52 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.04, #running-req: 0, #queue-req: 19, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:52] INFO:     127.0.0.1:37040 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:52] INFO:     127.0.0.1:37054 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:52] INFO:     127.0.0.1:37064 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:52 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.05, #running-req: 1, #queue-req: 22, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:52] INFO:     127.0.0.1:37076 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:53] INFO:     127.0.0.1:37084 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:53] INFO:     127.0.0.1:37086 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:53 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.06, #running-req: 1, #queue-req: 25, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:53] INFO:     127.0.0.1:37100 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:53] INFO:     127.0.0.1:37116 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:53] INFO:     127.0.0.1:37118 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:53] INFO:     127.0.0.1:37134 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:53] INFO:     127.0.0.1:37148 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:53] INFO:     127.0.0.1:37156 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:53 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.08, #running-req: 1, #queue-req: 31, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:53] INFO:     127.0.0.1:37166 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:53] INFO:     127.0.0.1:37178 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:53] INFO:     127.0.0.1:37192 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:54] INFO:     127.0.0.1:37196 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:54 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.09, #running-req: 1, #queue-req: 35, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:54] INFO:     127.0.0.1:37204 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:54] INFO:     127.0.0.1:37220 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:54] INFO:     127.0.0.1:37228 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:54 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.10, #running-req: 2, #queue-req: 38, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:54] INFO:     127.0.0.1:37240 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:54] INFO:     127.0.0.1:58880 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:54] INFO:     127.0.0.1:58884 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:55] INFO:     127.0.0.1:58890 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:55] INFO:     127.0.0.1:58898 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:55 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.11, #running-req: 2, #queue-req: 42, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:55] INFO:     127.0.0.1:58906 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:55] INFO:     127.0.0.1:58918 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:55] INFO:     127.0.0.1:58924 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:55] INFO:     127.0.0.1:58934 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:11:55 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.13, #running-req: 2, #queue-req: 46, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:56 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.14, #running-req: 3, #queue-req: 46, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:56 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.15, #running-req: 3, #queue-req: 46, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:57 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.16, #running-req: 3, #queue-req: 46, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:57 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.18, #running-req: 3, #queue-req: 45, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:58 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.19, #running-req: 4, #queue-req: 45, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:58 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.20, #running-req: 4, #queue-req: 45, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:59 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.21, #running-req: 4, #queue-req: 45, 
[36m(task, pid=6695)[0m [2025-04-22 06:11:59 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.23, #running-req: 4, #queue-req: 44, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:00 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.24, #running-req: 5, #queue-req: 44, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:00 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.25, #running-req: 5, #queue-req: 44, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:01 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.26, #running-req: 5, #queue-req: 43, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:02 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.28, #running-req: 6, #queue-req: 43, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:02 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.29, #running-req: 6, #queue-req: 43, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:03 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.30, #running-req: 6, #queue-req: 43, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:03 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.31, #running-req: 6, #queue-req: 42, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:04 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.33, #running-req: 7, #queue-req: 42, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:04 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.34, #running-req: 7, #queue-req: 42, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:05 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.35, #running-req: 7, #queue-req: 42, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:05 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.36, #running-req: 7, #queue-req: 41, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:06 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.38, #running-req: 8, #queue-req: 41, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:06 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.39, #running-req: 8, #queue-req: 41, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:07 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.40, #running-req: 8, #queue-req: 40, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:07 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.41, #running-req: 9, #queue-req: 40, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:08 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.43, #running-req: 9, #queue-req: 40, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:08 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.44, #running-req: 9, #queue-req: 40, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:09 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.45, #running-req: 9, #queue-req: 39, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:09 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.46, #running-req: 10, #queue-req: 39, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:10 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.48, #running-req: 10, #queue-req: 39, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:10 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.49, #running-req: 10, #queue-req: 39, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:11 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.50, #running-req: 10, #queue-req: 38, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:11 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.51, #running-req: 11, #queue-req: 38, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:12 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.53, #running-req: 11, #queue-req: 38, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:13 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.54, #running-req: 11, #queue-req: 37, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:13 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.55, #running-req: 12, #queue-req: 37, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:14 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.56, #running-req: 12, #queue-req: 37, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:14 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.58, #running-req: 12, #queue-req: 37, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:15 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.59, #running-req: 12, #queue-req: 36, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:15 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.60, #running-req: 13, #queue-req: 36, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:16 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.61, #running-req: 13, #queue-req: 36, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:16 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.63, #running-req: 13, #queue-req: 36, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:17 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.64, #running-req: 13, #queue-req: 35, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:17 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.65, #running-req: 14, #queue-req: 35, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:18 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.66, #running-req: 14, #queue-req: 35, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:18 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.68, #running-req: 14, #queue-req: 34, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:19 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.69, #running-req: 15, #queue-req: 34, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:19 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.70, #running-req: 15, #queue-req: 34, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:20 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.71, #running-req: 15, #queue-req: 34, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:20 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.73, #running-req: 15, #queue-req: 33, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:21 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.74, #running-req: 16, #queue-req: 33, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:21 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.75, #running-req: 16, #queue-req: 33, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:22 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.76, #running-req: 16, #queue-req: 33, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:22 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.78, #running-req: 16, #queue-req: 32, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:23 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.79, #running-req: 17, #queue-req: 32, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:23 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.80, #running-req: 17, #queue-req: 32, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:24 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.81, #running-req: 17, #queue-req: 31, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:25 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.83, #running-req: 18, #queue-req: 31, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:25 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.84, #running-req: 18, #queue-req: 31, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:26 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.85, #running-req: 18, #queue-req: 31, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:26 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.86, #running-req: 18, #queue-req: 30, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:27 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.88, #running-req: 19, #queue-req: 30, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:27 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.89, #running-req: 19, #queue-req: 30, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:28 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.90, #running-req: 19, #queue-req: 30, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:28 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.91, #running-req: 19, #queue-req: 29, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:29 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.93, #running-req: 20, #queue-req: 29, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:29 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.94, #running-req: 20, #queue-req: 29, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:30 TP0] Prefill batch. #new-seq: 1, #new-token: 7429, #cached-token: 0, token usage: 0.95, #running-req: 20, #queue-req: 29, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:31 TP0] Decode batch. #running-req: 21, #token: 630168, token usage: 0.96, gen throughput (token/s): 5.02, #queue-req: 29, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:32 TP0] Decode batch. #running-req: 21, #token: 631008, token usage: 0.96, gen throughput (token/s): 577.49, #queue-req: 29, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:34 TP0] Decode batch. #running-req: 21, #token: 631848, token usage: 0.97, gen throughput (token/s): 571.71, #queue-req: 29, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:34 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 28, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:34 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.01, #running-req: 0, #queue-req: 28, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:35 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.03, #running-req: 0, #queue-req: 28, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:35 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.04, #running-req: 0, #queue-req: 27, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:36 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.05, #running-req: 1, #queue-req: 27, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:37 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.06, #running-req: 1, #queue-req: 27, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:37 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.08, #running-req: 1, #queue-req: 27, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:38 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.09, #running-req: 1, #queue-req: 26, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:38 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.10, #running-req: 2, #queue-req: 26, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:39 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.11, #running-req: 2, #queue-req: 26, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:39 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.13, #running-req: 2, #queue-req: 25, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:40 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.14, #running-req: 3, #queue-req: 25, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:40 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.15, #running-req: 3, #queue-req: 25, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:41 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.16, #running-req: 3, #queue-req: 25, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:41 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.18, #running-req: 3, #queue-req: 24, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:42 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.19, #running-req: 4, #queue-req: 24, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:42 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.20, #running-req: 4, #queue-req: 24, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:43 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.21, #running-req: 4, #queue-req: 24, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:43 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.23, #running-req: 4, #queue-req: 23, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:44 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.24, #running-req: 5, #queue-req: 23, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:44 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.25, #running-req: 5, #queue-req: 23, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:45 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.26, #running-req: 5, #queue-req: 22, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:45 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.28, #running-req: 6, #queue-req: 22, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:46 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.29, #running-req: 6, #queue-req: 22, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:46 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.30, #running-req: 6, #queue-req: 22, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:47 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.31, #running-req: 6, #queue-req: 21, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:47 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.33, #running-req: 7, #queue-req: 21, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:48 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.34, #running-req: 7, #queue-req: 21, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:49 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.35, #running-req: 7, #queue-req: 21, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:49 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.36, #running-req: 7, #queue-req: 20, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:50 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.38, #running-req: 8, #queue-req: 20, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:50 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.39, #running-req: 8, #queue-req: 20, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:51 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.40, #running-req: 8, #queue-req: 19, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:51 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.41, #running-req: 9, #queue-req: 19, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:52 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.43, #running-req: 9, #queue-req: 19, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:52 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.44, #running-req: 9, #queue-req: 19, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:53 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.45, #running-req: 9, #queue-req: 18, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:53 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.46, #running-req: 10, #queue-req: 18, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:54 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.48, #running-req: 10, #queue-req: 18, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:54 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.49, #running-req: 10, #queue-req: 18, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:55 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.50, #running-req: 10, #queue-req: 17, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:55 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.51, #running-req: 11, #queue-req: 17, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:56 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.53, #running-req: 11, #queue-req: 17, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:56 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.54, #running-req: 11, #queue-req: 16, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:57 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.55, #running-req: 12, #queue-req: 16, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:58 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.56, #running-req: 12, #queue-req: 16, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:58 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.58, #running-req: 12, #queue-req: 16, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:59 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.59, #running-req: 12, #queue-req: 15, 
[36m(task, pid=6695)[0m [2025-04-22 06:12:59 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.60, #running-req: 13, #queue-req: 15, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:00 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.61, #running-req: 13, #queue-req: 15, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:00 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.63, #running-req: 13, #queue-req: 15, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:01 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.64, #running-req: 13, #queue-req: 14, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:01 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.65, #running-req: 14, #queue-req: 14, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:02 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.66, #running-req: 14, #queue-req: 14, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:02 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.68, #running-req: 14, #queue-req: 13, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:03 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.69, #running-req: 15, #queue-req: 13, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:03 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.70, #running-req: 15, #queue-req: 13, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:04 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.71, #running-req: 15, #queue-req: 13, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:04 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.73, #running-req: 15, #queue-req: 12, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:05 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.74, #running-req: 16, #queue-req: 12, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:05 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.75, #running-req: 16, #queue-req: 12, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:06 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.76, #running-req: 16, #queue-req: 12, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:06 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.78, #running-req: 16, #queue-req: 11, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:07 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.79, #running-req: 17, #queue-req: 11, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:07 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.80, #running-req: 17, #queue-req: 11, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:08 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.81, #running-req: 17, #queue-req: 10, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:09 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.83, #running-req: 18, #queue-req: 10, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:09 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.84, #running-req: 18, #queue-req: 10, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:10 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.85, #running-req: 18, #queue-req: 10, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:10 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.86, #running-req: 18, #queue-req: 9, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:11 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.88, #running-req: 19, #queue-req: 9, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:11 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.89, #running-req: 19, #queue-req: 9, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:12 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.90, #running-req: 19, #queue-req: 9, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:12 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.91, #running-req: 19, #queue-req: 8, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:13 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.93, #running-req: 20, #queue-req: 8, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:13 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.94, #running-req: 20, #queue-req: 8, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:14 TP0] Prefill batch. #new-seq: 1, #new-token: 7429, #cached-token: 0, token usage: 0.95, #running-req: 20, #queue-req: 8, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:15 TP0] Decode batch. #running-req: 21, #token: 630588, token usage: 0.96, gen throughput (token/s): 20.13, #queue-req: 8, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:17 TP0] Decode batch. #running-req: 21, #token: 631428, token usage: 0.97, gen throughput (token/s): 577.41, #queue-req: 8, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:18 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 7, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:18 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.01, #running-req: 0, #queue-req: 7, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:19 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.03, #running-req: 0, #queue-req: 7, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:19 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.04, #running-req: 0, #queue-req: 6, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:20 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.05, #running-req: 1, #queue-req: 6, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:20 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.06, #running-req: 1, #queue-req: 6, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:21 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.08, #running-req: 1, #queue-req: 6, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:22 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.09, #running-req: 1, #queue-req: 5, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:22 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.10, #running-req: 2, #queue-req: 5, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:23 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.11, #running-req: 2, #queue-req: 5, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:23 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.13, #running-req: 2, #queue-req: 4, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:24 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.14, #running-req: 3, #queue-req: 4, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:24 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.15, #running-req: 3, #queue-req: 4, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:25 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.16, #running-req: 3, #queue-req: 4, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:25 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.18, #running-req: 3, #queue-req: 3, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:26 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.19, #running-req: 4, #queue-req: 3, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:26 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.20, #running-req: 4, #queue-req: 3, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:27 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.21, #running-req: 4, #queue-req: 3, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:27 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.23, #running-req: 4, #queue-req: 2, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:28 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.24, #running-req: 5, #queue-req: 2, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:28 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.25, #running-req: 5, #queue-req: 2, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:29 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.26, #running-req: 5, #queue-req: 1, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:29 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.28, #running-req: 6, #queue-req: 1, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:30 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.29, #running-req: 6, #queue-req: 1, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:30 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.30, #running-req: 6, #queue-req: 1, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:31 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.31, #running-req: 6, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:31 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.33, #running-req: 7, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:32 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.34, #running-req: 7, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:33 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.35, #running-req: 7, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:33 TP0] Prefill batch. #new-seq: 1, #new-token: 2440, #cached-token: 0, token usage: 0.36, #running-req: 7, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:34 TP0] Decode batch. #running-req: 8, #token: 240064, token usage: 0.37, gen throughput (token/s): 45.63, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:35 TP0] Decode batch. #running-req: 8, #token: 240384, token usage: 0.37, gen throughput (token/s): 348.89, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:35 TP0] Decode batch. #running-req: 8, #token: 240704, token usage: 0.37, gen throughput (token/s): 347.42, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:36] INFO:     127.0.0.1:36326 - "GET /get_server_info HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:38] INFO:     127.0.0.1:36328 - "GET /v1/models HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:42] INFO:     127.0.0.1:36332 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:42 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:43 TP0] Decode batch. #running-req: 1, #token: 1028, token usage: 0.00, gen throughput (token/s): 19.04, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:44] INFO:     127.0.0.1:36340 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:44 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:44] INFO:     127.0.0.1:36354 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:44 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:44] INFO:     127.0.0.1:36368 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:44] INFO:     127.0.0.1:36384 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:44] INFO:     127.0.0.1:36392 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:44] INFO:     127.0.0.1:36408 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:44] INFO:     127.0.0.1:36412 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:44] INFO:     127.0.0.1:36426 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:44] INFO:     127.0.0.1:36438 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:44 TP0] Prefill batch. #new-seq: 6, #new-token: 6006, #cached-token: 0, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:44 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.01, #running-req: 8, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:44] INFO:     127.0.0.1:36448 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:44] INFO:     127.0.0.1:36456 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:44] INFO:     127.0.0.1:36458 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:44] INFO:     127.0.0.1:36474 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:44] INFO:     127.0.0.1:36484 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:44 TP0] Prefill batch. #new-seq: 5, #new-token: 5005, #cached-token: 0, token usage: 0.01, #running-req: 9, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:45] INFO:     127.0.0.1:40292 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:45] INFO:     127.0.0.1:40296 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:45 TP0] Prefill batch. #new-seq: 2, #new-token: 2002, #cached-token: 0, token usage: 0.02, #running-req: 14, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:45] INFO:     127.0.0.1:40304 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:45] INFO:     127.0.0.1:40318 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:45 TP0] Prefill batch. #new-seq: 2, #new-token: 2002, #cached-token: 0, token usage: 0.02, #running-req: 16, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:45] INFO:     127.0.0.1:40330 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:45] INFO:     127.0.0.1:40346 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:45] INFO:     127.0.0.1:40356 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:45 TP0] Prefill batch. #new-seq: 3, #new-token: 3003, #cached-token: 0, token usage: 0.03, #running-req: 18, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:45] INFO:     127.0.0.1:40372 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:45 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.03, #running-req: 21, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:45] INFO:     127.0.0.1:40378 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:45 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.03, #running-req: 22, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:45] INFO:     127.0.0.1:40386 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:46] INFO:     127.0.0.1:40392 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:46 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.04, #running-req: 23, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:46 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.04, #running-req: 24, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:46] INFO:     127.0.0.1:40398 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:46 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.04, #running-req: 25, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:46] INFO:     127.0.0.1:40404 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:46] INFO:     127.0.0.1:40412 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:46] INFO:     127.0.0.1:40418 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:46 TP0] Prefill batch. #new-seq: 3, #new-token: 3003, #cached-token: 0, token usage: 0.04, #running-req: 26, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:46] INFO:     127.0.0.1:40428 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:46 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.04, #running-req: 29, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:46] INFO:     127.0.0.1:40438 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:46] INFO:     127.0.0.1:40440 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:46] INFO:     127.0.0.1:40448 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:46 TP0] Prefill batch. #new-seq: 2, #new-token: 2002, #cached-token: 0, token usage: 0.05, #running-req: 30, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:46 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.05, #running-req: 32, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:47] INFO:     127.0.0.1:40464 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:47 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.05, #running-req: 33, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:47] INFO:     127.0.0.1:40470 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:47 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.05, #running-req: 34, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:47] INFO:     127.0.0.1:40474 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:47] INFO:     127.0.0.1:40482 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:47 TP0] Prefill batch. #new-seq: 2, #new-token: 2002, #cached-token: 0, token usage: 0.05, #running-req: 35, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:47] INFO:     127.0.0.1:40486 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:47 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.06, #running-req: 37, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:47] INFO:     127.0.0.1:40500 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:47] INFO:     127.0.0.1:40510 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:47 TP0] Prefill batch. #new-seq: 2, #new-token: 2002, #cached-token: 0, token usage: 0.06, #running-req: 38, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:47] INFO:     127.0.0.1:40526 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:47 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.06, #running-req: 40, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:48] INFO:     127.0.0.1:40530 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:48 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.06, #running-req: 41, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:48] INFO:     127.0.0.1:40538 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:48 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.07, #running-req: 42, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:48] INFO:     127.0.0.1:40544 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:48 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.07, #running-req: 43, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:48] INFO:     127.0.0.1:40560 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:48] INFO:     127.0.0.1:40562 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:48] INFO:     127.0.0.1:40578 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:48 TP0] Prefill batch. #new-seq: 3, #new-token: 3003, #cached-token: 0, token usage: 0.07, #running-req: 44, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:48] INFO:     127.0.0.1:40594 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:48 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 0, token usage: 0.07, #running-req: 47, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:48] INFO:     127.0.0.1:40606 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:48] INFO:     127.0.0.1:40612 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:13:49 TP0] Prefill batch. #new-seq: 2, #new-token: 2002, #cached-token: 0, token usage: 0.07, #running-req: 48, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:49 TP0] Decode batch. #running-req: 50, #token: 51286, token usage: 0.08, gen throughput (token/s): 191.98, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:50 TP0] Decode batch. #running-req: 50, #token: 53286, token usage: 0.08, gen throughput (token/s): 1489.10, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:52 TP0] Decode batch. #running-req: 50, #token: 55286, token usage: 0.08, gen throughput (token/s): 1480.45, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:53 TP0] Decode batch. #running-req: 50, #token: 57286, token usage: 0.09, gen throughput (token/s): 1466.77, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:54 TP0] Decode batch. #running-req: 50, #token: 59286, token usage: 0.09, gen throughput (token/s): 1456.75, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:56 TP0] Decode batch. #running-req: 50, #token: 61286, token usage: 0.09, gen throughput (token/s): 1444.83, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:57 TP0] Decode batch. #running-req: 50, #token: 63286, token usage: 0.10, gen throughput (token/s): 1442.87, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:13:58 TP0] Decode batch. #running-req: 50, #token: 65286, token usage: 0.10, gen throughput (token/s): 1437.09, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:00 TP0] Decode batch. #running-req: 50, #token: 67286, token usage: 0.10, gen throughput (token/s): 1428.26, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:01 TP0] Decode batch. #running-req: 50, #token: 69286, token usage: 0.11, gen throughput (token/s): 1431.73, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:03 TP0] Decode batch. #running-req: 50, #token: 71286, token usage: 0.11, gen throughput (token/s): 1426.61, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:04 TP0] Decode batch. #running-req: 50, #token: 73286, token usage: 0.11, gen throughput (token/s): 1427.97, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:05 TP0] Decode batch. #running-req: 50, #token: 75286, token usage: 0.12, gen throughput (token/s): 1429.15, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:07 TP0] Decode batch. #running-req: 50, #token: 77286, token usage: 0.12, gen throughput (token/s): 1424.15, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:08 TP0] Decode batch. #running-req: 50, #token: 79286, token usage: 0.12, gen throughput (token/s): 1426.36, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:10 TP0] Decode batch. #running-req: 50, #token: 81286, token usage: 0.12, gen throughput (token/s): 1424.23, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:11 TP0] Decode batch. #running-req: 50, #token: 83286, token usage: 0.13, gen throughput (token/s): 1417.29, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:13 TP0] Decode batch. #running-req: 50, #token: 85286, token usage: 0.13, gen throughput (token/s): 1415.89, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:14 TP0] Decode batch. #running-req: 50, #token: 87286, token usage: 0.13, gen throughput (token/s): 1423.72, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:15 TP0] Decode batch. #running-req: 50, #token: 89286, token usage: 0.14, gen throughput (token/s): 1417.68, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:17 TP0] Decode batch. #running-req: 50, #token: 91286, token usage: 0.14, gen throughput (token/s): 1412.27, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:18 TP0] Decode batch. #running-req: 50, #token: 93286, token usage: 0.14, gen throughput (token/s): 1413.09, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:20 TP0] Decode batch. #running-req: 50, #token: 95286, token usage: 0.15, gen throughput (token/s): 1407.71, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:21 TP0] Decode batch. #running-req: 50, #token: 97286, token usage: 0.15, gen throughput (token/s): 1411.25, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:22 TP0] Decode batch. #running-req: 50, #token: 99286, token usage: 0.15, gen throughput (token/s): 1411.33, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:24 TP0] Decode batch. #running-req: 50, #token: 101286, token usage: 0.15, gen throughput (token/s): 1412.61, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:25 TP0] Decode batch. #running-req: 50, #token: 103286, token usage: 0.16, gen throughput (token/s): 1407.72, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:27 TP0] Decode batch. #running-req: 50, #token: 105286, token usage: 0.16, gen throughput (token/s): 1415.70, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:28 TP0] Decode batch. #running-req: 50, #token: 107286, token usage: 0.16, gen throughput (token/s): 1413.16, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:30 TP0] Decode batch. #running-req: 50, #token: 109286, token usage: 0.17, gen throughput (token/s): 1411.13, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:31 TP0] Decode batch. #running-req: 50, #token: 111286, token usage: 0.17, gen throughput (token/s): 1408.13, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:32 TP0] Decode batch. #running-req: 50, #token: 113286, token usage: 0.17, gen throughput (token/s): 1404.38, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:34 TP0] Decode batch. #running-req: 50, #token: 115286, token usage: 0.18, gen throughput (token/s): 1409.48, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:35 TP0] Decode batch. #running-req: 50, #token: 117286, token usage: 0.18, gen throughput (token/s): 1412.49, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:37 TP0] Decode batch. #running-req: 50, #token: 119286, token usage: 0.18, gen throughput (token/s): 1400.40, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:38 TP0] Decode batch. #running-req: 50, #token: 121286, token usage: 0.19, gen throughput (token/s): 1398.24, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:39 TP0] Decode batch. #running-req: 50, #token: 123286, token usage: 0.19, gen throughput (token/s): 1406.86, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:41 TP0] Decode batch. #running-req: 50, #token: 125286, token usage: 0.19, gen throughput (token/s): 1402.94, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:42 TP0] Decode batch. #running-req: 50, #token: 127286, token usage: 0.19, gen throughput (token/s): 1413.24, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:44 TP0] Decode batch. #running-req: 50, #token: 129286, token usage: 0.20, gen throughput (token/s): 1407.44, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:45 TP0] Decode batch. #running-req: 50, #token: 131286, token usage: 0.20, gen throughput (token/s): 1400.69, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:47 TP0] Decode batch. #running-req: 50, #token: 133286, token usage: 0.20, gen throughput (token/s): 1402.72, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:48 TP0] Decode batch. #running-req: 50, #token: 135286, token usage: 0.21, gen throughput (token/s): 1403.88, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:49 TP0] Decode batch. #running-req: 50, #token: 137286, token usage: 0.21, gen throughput (token/s): 1398.76, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:51 TP0] Decode batch. #running-req: 50, #token: 139286, token usage: 0.21, gen throughput (token/s): 1399.37, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:52 TP0] Decode batch. #running-req: 50, #token: 141286, token usage: 0.22, gen throughput (token/s): 1393.60, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:54 TP0] Decode batch. #running-req: 50, #token: 143286, token usage: 0.22, gen throughput (token/s): 1387.65, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:55 TP0] Decode batch. #running-req: 50, #token: 145286, token usage: 0.22, gen throughput (token/s): 1390.85, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:57 TP0] Decode batch. #running-req: 50, #token: 147286, token usage: 0.23, gen throughput (token/s): 1388.96, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:58 TP0] Decode batch. #running-req: 50, #token: 149286, token usage: 0.23, gen throughput (token/s): 1391.23, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:14:59] INFO:     127.0.0.1:34502 - "GET /get_server_info HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:02] INFO:     127.0.0.1:34514 - "GET /v1/models HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:06] INFO:     127.0.0.1:50586 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:06 TP0] Prefill batch. #new-seq: 1, #new-token: 5001, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:06 TP0] Decode batch. #running-req: 1, #token: 5008, token usage: 0.01, gen throughput (token/s): 99.19, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:08] INFO:     127.0.0.1:50598 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:08 TP0] Prefill batch. #new-seq: 1, #new-token: 5001, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:08] INFO:     127.0.0.1:50610 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:08] INFO:     127.0.0.1:50616 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:08] INFO:     127.0.0.1:50632 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:08] INFO:     127.0.0.1:50634 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:08] INFO:     127.0.0.1:50642 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:08] INFO:     127.0.0.1:50652 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:08] INFO:     127.0.0.1:50660 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:08 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.01, #running-req: 1, #queue-req: 5, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:08] INFO:     127.0.0.1:50672 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:08] INFO:     127.0.0.1:50682 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:08] INFO:     127.0.0.1:50686 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:08] INFO:     127.0.0.1:50692 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:08 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.02, #running-req: 2, #queue-req: 7, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:08] INFO:     127.0.0.1:50708 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:08] INFO:     127.0.0.1:50724 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:09] INFO:     127.0.0.1:50740 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:09] INFO:     127.0.0.1:50756 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:09 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.03, #running-req: 4, #queue-req: 10, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:09] INFO:     127.0.0.1:50768 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:09] INFO:     127.0.0.1:50780 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:09] INFO:     127.0.0.1:50788 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:09] INFO:     127.0.0.1:50792 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:09] INFO:     127.0.0.1:50800 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:09] INFO:     127.0.0.1:50810 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:09 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.05, #running-req: 5, #queue-req: 14, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:09] INFO:     127.0.0.1:50818 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:09] INFO:     127.0.0.1:50826 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:10 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.06, #running-req: 7, #queue-req: 14, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:10] INFO:     127.0.0.1:50836 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:10] INFO:     127.0.0.1:50842 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:10 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.07, #running-req: 9, #queue-req: 15, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:10] INFO:     127.0.0.1:50854 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:10] INFO:     127.0.0.1:50860 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:10] INFO:     127.0.0.1:50874 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:10] INFO:     127.0.0.1:50888 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:10] INFO:     127.0.0.1:50896 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:10] INFO:     127.0.0.1:50906 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:10] INFO:     127.0.0.1:50914 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:10 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.08, #running-req: 10, #queue-req: 20, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:11] INFO:     127.0.0.1:50924 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:11] INFO:     127.0.0.1:50934 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:11] INFO:     127.0.0.1:50936 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:11 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.10, #running-req: 12, #queue-req: 20, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:11] INFO:     127.0.0.1:50946 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:11] INFO:     127.0.0.1:50962 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:11] INFO:     127.0.0.1:50964 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:11] INFO:     127.0.0.1:50968 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:11 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.11, #running-req: 14, #queue-req: 24, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:11] INFO:     127.0.0.1:50974 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:12 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.12, #running-req: 15, #queue-req: 23, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:12] INFO:     127.0.0.1:50980 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:12] INFO:     127.0.0.1:50994 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:12] INFO:     127.0.0.1:51002 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:12] INFO:     127.0.0.1:51012 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:12 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.13, #running-req: 17, #queue-req: 25, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:12] INFO:     127.0.0.1:51028 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:12] INFO:     127.0.0.1:51038 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:12] INFO:     127.0.0.1:51052 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:12] INFO:     127.0.0.1:51056 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:12] INFO:     127.0.0.1:51062 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:15:12 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.15, #running-req: 19, #queue-req: 29, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:13 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.16, #running-req: 20, #queue-req: 27, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:13 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.17, #running-req: 22, #queue-req: 26, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:14 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.18, #running-req: 23, #queue-req: 24, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:14 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.20, #running-req: 25, #queue-req: 22, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:15 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.21, #running-req: 27, #queue-req: 21, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:15 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.22, #running-req: 28, #queue-req: 19, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:15 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.23, #running-req: 30, #queue-req: 17, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:16 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.25, #running-req: 32, #queue-req: 16, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:16 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.26, #running-req: 33, #queue-req: 14, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:17 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.27, #running-req: 35, #queue-req: 12, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:17 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.28, #running-req: 37, #queue-req: 11, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:18 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.30, #running-req: 38, #queue-req: 9, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:18 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.31, #running-req: 40, #queue-req: 8, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:18 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.32, #running-req: 41, #queue-req: 6, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:19 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.33, #running-req: 43, #queue-req: 4, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:19 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.35, #running-req: 45, #queue-req: 3, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:20 TP0] Prefill batch. #new-seq: 3, #new-token: 8192, #cached-token: 0, token usage: 0.36, #running-req: 46, #queue-req: 1, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:20 TP0] Prefill batch. #new-seq: 2, #new-token: 7481, #cached-token: 0, token usage: 0.37, #running-req: 48, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:21 TP0] Decode batch. #running-req: 50, #token: 250751, token usage: 0.38, gen throughput (token/s): 45.59, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:23 TP0] Decode batch. #running-req: 50, #token: 252751, token usage: 0.39, gen throughput (token/s): 1316.20, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:24 TP0] Decode batch. #running-req: 50, #token: 254751, token usage: 0.39, gen throughput (token/s): 1294.60, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:26 TP0] Decode batch. #running-req: 50, #token: 256751, token usage: 0.39, gen throughput (token/s): 1284.92, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:27 TP0] Decode batch. #running-req: 50, #token: 258751, token usage: 0.40, gen throughput (token/s): 1278.28, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:29 TP0] Decode batch. #running-req: 50, #token: 260751, token usage: 0.40, gen throughput (token/s): 1274.49, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:31 TP0] Decode batch. #running-req: 50, #token: 262751, token usage: 0.40, gen throughput (token/s): 1272.42, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:32 TP0] Decode batch. #running-req: 50, #token: 264751, token usage: 0.40, gen throughput (token/s): 1267.44, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:34 TP0] Decode batch. #running-req: 50, #token: 266751, token usage: 0.41, gen throughput (token/s): 1261.70, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:35 TP0] Decode batch. #running-req: 50, #token: 268751, token usage: 0.41, gen throughput (token/s): 1263.18, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:37 TP0] Decode batch. #running-req: 50, #token: 270751, token usage: 0.41, gen throughput (token/s): 1263.93, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:38 TP0] Decode batch. #running-req: 50, #token: 272751, token usage: 0.42, gen throughput (token/s): 1258.96, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:40 TP0] Decode batch. #running-req: 50, #token: 274751, token usage: 0.42, gen throughput (token/s): 1254.11, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:42 TP0] Decode batch. #running-req: 50, #token: 276751, token usage: 0.42, gen throughput (token/s): 1255.75, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:43 TP0] Decode batch. #running-req: 50, #token: 278751, token usage: 0.43, gen throughput (token/s): 1255.92, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:45 TP0] Decode batch. #running-req: 50, #token: 280751, token usage: 0.43, gen throughput (token/s): 1258.00, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:46 TP0] Decode batch. #running-req: 50, #token: 282751, token usage: 0.43, gen throughput (token/s): 1255.95, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:48 TP0] Decode batch. #running-req: 50, #token: 284751, token usage: 0.44, gen throughput (token/s): 1251.93, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:50 TP0] Decode batch. #running-req: 50, #token: 286751, token usage: 0.44, gen throughput (token/s): 1254.73, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:51 TP0] Decode batch. #running-req: 50, #token: 288751, token usage: 0.44, gen throughput (token/s): 1252.11, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:53 TP0] Decode batch. #running-req: 50, #token: 290751, token usage: 0.44, gen throughput (token/s): 1250.50, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:54 TP0] Decode batch. #running-req: 50, #token: 292751, token usage: 0.45, gen throughput (token/s): 1249.81, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:56 TP0] Decode batch. #running-req: 50, #token: 294751, token usage: 0.45, gen throughput (token/s): 1251.74, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:58 TP0] Decode batch. #running-req: 50, #token: 296751, token usage: 0.45, gen throughput (token/s): 1248.32, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:15:59 TP0] Decode batch. #running-req: 50, #token: 298751, token usage: 0.46, gen throughput (token/s): 1252.30, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:00] INFO:     127.0.0.1:35924 - "GET /get_server_info HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:03] INFO:     127.0.0.1:35934 - "GET /v1/models HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:07] INFO:     127.0.0.1:57468 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:07 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:07 TP0] Prefill batch. #new-seq: 1, #new-token: 1809, #cached-token: 0, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:08 TP0] Decode batch. #running-req: 1, #token: 10015, token usage: 0.02, gen throughput (token/s): 154.79, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:09] INFO:     127.0.0.1:57470 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:09 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:09 TP0] Prefill batch. #new-seq: 1, #new-token: 1809, #cached-token: 0, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:09] INFO:     127.0.0.1:57484 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:09] INFO:     127.0.0.1:57490 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:09] INFO:     127.0.0.1:57502 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:09] INFO:     127.0.0.1:57518 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:09] INFO:     127.0.0.1:57524 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:10] INFO:     127.0.0.1:57526 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:10] INFO:     127.0.0.1:57542 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:10] INFO:     127.0.0.1:57554 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:10] INFO:     127.0.0.1:57560 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:10] INFO:     127.0.0.1:57576 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:10] INFO:     127.0.0.1:57586 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:10 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.02, #running-req: 1, #queue-req: 10, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:10] INFO:     127.0.0.1:57588 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:10] INFO:     127.0.0.1:57590 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:10] INFO:     127.0.0.1:57600 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:10] INFO:     127.0.0.1:57614 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:10 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.03, #running-req: 1, #queue-req: 13, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:10] INFO:     127.0.0.1:57624 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:10] INFO:     127.0.0.1:57638 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:10] INFO:     127.0.0.1:57646 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:10] INFO:     127.0.0.1:57652 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:10] INFO:     127.0.0.1:57654 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:11] INFO:     127.0.0.1:57666 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:11 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.04, #running-req: 2, #queue-req: 18, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:11] INFO:     127.0.0.1:57668 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:11] INFO:     127.0.0.1:57682 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:11 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.05, #running-req: 3, #queue-req: 19, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:11] INFO:     127.0.0.1:57684 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:11] INFO:     127.0.0.1:57686 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:12] INFO:     127.0.0.1:57702 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:12 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.07, #running-req: 4, #queue-req: 20, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:12] INFO:     127.0.0.1:57714 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:12] INFO:     127.0.0.1:57724 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:12] INFO:     127.0.0.1:57730 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:12] INFO:     127.0.0.1:57746 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:12] INFO:     127.0.0.1:57754 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:12] INFO:     127.0.0.1:57770 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:12 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.08, #running-req: 5, #queue-req: 27, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:12] INFO:     127.0.0.1:57774 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:12] INFO:     127.0.0.1:57790 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:12] INFO:     127.0.0.1:57792 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:12] INFO:     127.0.0.1:57806 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:12] INFO:     127.0.0.1:57812 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:13 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.09, #running-req: 5, #queue-req: 31, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:13] INFO:     127.0.0.1:57816 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:13] INFO:     127.0.0.1:57820 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:13] INFO:     127.0.0.1:57832 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:13 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.10, #running-req: 6, #queue-req: 33, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:13] INFO:     127.0.0.1:57840 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:13] INFO:     127.0.0.1:57850 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:13 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.12, #running-req: 7, #queue-req: 33, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:13] INFO:     127.0.0.1:57866 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:14] INFO:     127.0.0.1:57882 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:14] INFO:     127.0.0.1:57892 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:14] INFO:     127.0.0.1:57894 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:14] INFO:     127.0.0.1:57896 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:14 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.13, #running-req: 8, #queue-req: 37, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:14] INFO:     127.0.0.1:57908 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:14] INFO:     127.0.0.1:57914 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:16:14 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.14, #running-req: 9, #queue-req: 39, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:15 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.15, #running-req: 10, #queue-req: 39, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:15 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.17, #running-req: 10, #queue-req: 38, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:16 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.18, #running-req: 11, #queue-req: 37, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:16 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.19, #running-req: 12, #queue-req: 36, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:17 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.20, #running-req: 13, #queue-req: 35, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:17 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.22, #running-req: 14, #queue-req: 35, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:18 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.23, #running-req: 14, #queue-req: 34, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:18 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.24, #running-req: 15, #queue-req: 33, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:19 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.25, #running-req: 16, #queue-req: 32, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:19 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.27, #running-req: 17, #queue-req: 31, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:20 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.28, #running-req: 18, #queue-req: 30, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:20 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.29, #running-req: 19, #queue-req: 30, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:21 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.30, #running-req: 19, #queue-req: 29, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:21 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.32, #running-req: 20, #queue-req: 28, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:22 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.33, #running-req: 21, #queue-req: 27, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:22 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.34, #running-req: 22, #queue-req: 26, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:23 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.35, #running-req: 23, #queue-req: 26, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:23 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.37, #running-req: 23, #queue-req: 25, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:24 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.38, #running-req: 24, #queue-req: 24, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:24 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.39, #running-req: 25, #queue-req: 23, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:24 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.40, #running-req: 26, #queue-req: 22, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:25 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.42, #running-req: 27, #queue-req: 21, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:26 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.43, #running-req: 28, #queue-req: 21, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:26 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.44, #running-req: 28, #queue-req: 20, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:26 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.45, #running-req: 29, #queue-req: 19, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:27 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.47, #running-req: 30, #queue-req: 18, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:27 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.48, #running-req: 31, #queue-req: 17, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:28 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.49, #running-req: 32, #queue-req: 17, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:28 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.50, #running-req: 32, #queue-req: 16, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:29 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.52, #running-req: 33, #queue-req: 15, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:29 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.53, #running-req: 34, #queue-req: 14, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:30 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.54, #running-req: 35, #queue-req: 13, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:30 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.55, #running-req: 36, #queue-req: 12, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:31 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.57, #running-req: 37, #queue-req: 12, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:31 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.58, #running-req: 37, #queue-req: 11, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:32 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.59, #running-req: 38, #queue-req: 10, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:32 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.60, #running-req: 39, #queue-req: 9, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:33 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.62, #running-req: 40, #queue-req: 8, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:33 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.63, #running-req: 41, #queue-req: 8, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:34 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.64, #running-req: 41, #queue-req: 7, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:34 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.65, #running-req: 42, #queue-req: 6, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:35 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.67, #running-req: 43, #queue-req: 5, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:35 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.68, #running-req: 44, #queue-req: 4, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:36 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.69, #running-req: 45, #queue-req: 3, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:36 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.70, #running-req: 46, #queue-req: 3, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:37 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.72, #running-req: 46, #queue-req: 2, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:37 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.73, #running-req: 47, #queue-req: 1, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:38 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.74, #running-req: 48, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:38 TP0] Prefill batch. #new-seq: 1, #new-token: 6721, #cached-token: 0, token usage: 0.75, #running-req: 49, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:40 TP0] Decode batch. #running-req: 50, #token: 501150, token usage: 0.77, gen throughput (token/s): 33.77, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:41 TP0] Decode batch. #running-req: 50, #token: 503150, token usage: 0.77, gen throughput (token/s): 1129.80, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:43 TP0] Decode batch. #running-req: 50, #token: 505150, token usage: 0.77, gen throughput (token/s): 1117.52, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:45 TP0] Decode batch. #running-req: 50, #token: 507150, token usage: 0.78, gen throughput (token/s): 1115.36, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:47 TP0] Decode batch. #running-req: 50, #token: 509150, token usage: 0.78, gen throughput (token/s): 1110.15, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:49 TP0] Decode batch. #running-req: 50, #token: 511150, token usage: 0.78, gen throughput (token/s): 1100.56, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:50 TP0] Decode batch. #running-req: 50, #token: 513150, token usage: 0.78, gen throughput (token/s): 1099.95, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:52 TP0] Decode batch. #running-req: 50, #token: 515150, token usage: 0.79, gen throughput (token/s): 1092.11, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:54 TP0] Decode batch. #running-req: 50, #token: 517150, token usage: 0.79, gen throughput (token/s): 1092.33, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:56 TP0] Decode batch. #running-req: 50, #token: 519150, token usage: 0.79, gen throughput (token/s): 1090.73, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:16:58 TP0] Decode batch. #running-req: 50, #token: 521150, token usage: 0.80, gen throughput (token/s): 1089.56, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:00 TP0] Decode batch. #running-req: 50, #token: 523150, token usage: 0.80, gen throughput (token/s): 1090.90, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:01] INFO:     127.0.0.1:59764 - "GET /get_server_info HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:04] INFO:     127.0.0.1:59766 - "GET /v1/models HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:08] INFO:     127.0.0.1:43720 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:08 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:08 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:09 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.03, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:09 TP0] Prefill batch. #new-seq: 1, #new-token: 5425, #cached-token: 0, token usage: 0.04, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:10 TP0] Decode batch. #running-req: 1, #token: 30003, token usage: 0.05, gen throughput (token/s): 185.57, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:12] INFO:     127.0.0.1:43724 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:12] INFO:     127.0.0.1:43736 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:12 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:12 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:12] INFO:     127.0.0.1:43752 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:12] INFO:     127.0.0.1:43762 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:12] INFO:     127.0.0.1:43764 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:12] INFO:     127.0.0.1:43780 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:12] INFO:     127.0.0.1:43786 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:12] INFO:     127.0.0.1:43790 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:12] INFO:     127.0.0.1:43800 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:12] INFO:     127.0.0.1:43802 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:12] INFO:     127.0.0.1:43810 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:12] INFO:     127.0.0.1:43822 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:12] INFO:     127.0.0.1:43838 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:12] INFO:     127.0.0.1:43842 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:12 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.03, #running-req: 0, #queue-req: 13, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:12] INFO:     127.0.0.1:43852 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:12] INFO:     127.0.0.1:43858 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:13] INFO:     127.0.0.1:43868 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:13] INFO:     127.0.0.1:43876 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:13] INFO:     127.0.0.1:43886 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:13] INFO:     127.0.0.1:43888 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:13] INFO:     127.0.0.1:43894 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:13] INFO:     127.0.0.1:43910 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:13 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.04, #running-req: 0, #queue-req: 19, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:13] INFO:     127.0.0.1:43922 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:13] INFO:     127.0.0.1:43924 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:13] INFO:     127.0.0.1:43934 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:13 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.05, #running-req: 1, #queue-req: 22, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:14] INFO:     127.0.0.1:43946 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:14] INFO:     127.0.0.1:43960 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:14] INFO:     127.0.0.1:43970 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:14 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.06, #running-req: 1, #queue-req: 25, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:14] INFO:     127.0.0.1:43976 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:14] INFO:     127.0.0.1:43980 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:14] INFO:     127.0.0.1:43986 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:14] INFO:     127.0.0.1:43998 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:14] INFO:     127.0.0.1:44006 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:14] INFO:     127.0.0.1:51772 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:14 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.08, #running-req: 1, #queue-req: 31, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:15] INFO:     127.0.0.1:51782 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:15] INFO:     127.0.0.1:51798 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:15] INFO:     127.0.0.1:51802 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:15] INFO:     127.0.0.1:51804 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:15 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.09, #running-req: 1, #queue-req: 35, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:15] INFO:     127.0.0.1:51818 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:15] INFO:     127.0.0.1:51820 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:15] INFO:     127.0.0.1:51836 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:15 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.10, #running-req: 2, #queue-req: 38, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:16] INFO:     127.0.0.1:51838 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:16] INFO:     127.0.0.1:51850 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:16] INFO:     127.0.0.1:51864 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:16] INFO:     127.0.0.1:51876 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:16] INFO:     127.0.0.1:51890 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:16 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.11, #running-req: 2, #queue-req: 42, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:16] INFO:     127.0.0.1:51904 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:16] INFO:     127.0.0.1:51906 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:16] INFO:     127.0.0.1:51908 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:16] INFO:     127.0.0.1:51912 - "POST /v1/completions HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m [2025-04-22 06:17:17 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.13, #running-req: 2, #queue-req: 46, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:17 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.14, #running-req: 3, #queue-req: 46, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:18 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.15, #running-req: 3, #queue-req: 46, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:18 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.16, #running-req: 3, #queue-req: 46, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:19 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.18, #running-req: 3, #queue-req: 45, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:19 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.19, #running-req: 4, #queue-req: 45, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:20 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.20, #running-req: 4, #queue-req: 45, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:20 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.21, #running-req: 4, #queue-req: 45, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:21 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.23, #running-req: 4, #queue-req: 44, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:21 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.24, #running-req: 5, #queue-req: 44, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:22 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.25, #running-req: 5, #queue-req: 44, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:22 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.26, #running-req: 5, #queue-req: 43, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:23 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.28, #running-req: 6, #queue-req: 43, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:23 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.29, #running-req: 6, #queue-req: 43, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:24 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.30, #running-req: 6, #queue-req: 43, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:24 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.31, #running-req: 6, #queue-req: 42, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:25 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.33, #running-req: 7, #queue-req: 42, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:25 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.34, #running-req: 7, #queue-req: 42, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:26 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.35, #running-req: 7, #queue-req: 42, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:26 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.36, #running-req: 7, #queue-req: 41, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:27 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.38, #running-req: 8, #queue-req: 41, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:27 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.39, #running-req: 8, #queue-req: 41, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:28 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.40, #running-req: 8, #queue-req: 40, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:29 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.41, #running-req: 9, #queue-req: 40, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:29 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.43, #running-req: 9, #queue-req: 40, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:30 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.44, #running-req: 9, #queue-req: 40, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:30 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.45, #running-req: 9, #queue-req: 39, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:31 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.46, #running-req: 10, #queue-req: 39, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:31 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.48, #running-req: 10, #queue-req: 39, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:32 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.49, #running-req: 10, #queue-req: 39, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:32 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.50, #running-req: 10, #queue-req: 38, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:33 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.51, #running-req: 11, #queue-req: 38, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:33 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.53, #running-req: 11, #queue-req: 38, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:34 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.54, #running-req: 11, #queue-req: 37, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:34 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.55, #running-req: 12, #queue-req: 37, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:35 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.56, #running-req: 12, #queue-req: 37, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:35 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.58, #running-req: 12, #queue-req: 37, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:36 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.59, #running-req: 12, #queue-req: 36, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:36 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.60, #running-req: 13, #queue-req: 36, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:37 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.61, #running-req: 13, #queue-req: 36, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:37 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.63, #running-req: 13, #queue-req: 36, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:38 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.64, #running-req: 13, #queue-req: 35, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:38 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.65, #running-req: 14, #queue-req: 35, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:39 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.66, #running-req: 14, #queue-req: 35, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:40 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.68, #running-req: 14, #queue-req: 34, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:40 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.69, #running-req: 15, #queue-req: 34, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:41 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.70, #running-req: 15, #queue-req: 34, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:41 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.71, #running-req: 15, #queue-req: 34, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:42 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.73, #running-req: 15, #queue-req: 33, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:42 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.74, #running-req: 16, #queue-req: 33, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:43 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.75, #running-req: 16, #queue-req: 33, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:43 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.76, #running-req: 16, #queue-req: 33, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:44 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.78, #running-req: 16, #queue-req: 32, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:44 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.79, #running-req: 17, #queue-req: 32, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:45 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.80, #running-req: 17, #queue-req: 32, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:45 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.81, #running-req: 17, #queue-req: 31, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:46 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.83, #running-req: 18, #queue-req: 31, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:46 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.84, #running-req: 18, #queue-req: 31, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:47 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.85, #running-req: 18, #queue-req: 31, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:47 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.86, #running-req: 18, #queue-req: 30, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:48 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.88, #running-req: 19, #queue-req: 30, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:48 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.89, #running-req: 19, #queue-req: 30, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:49 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.90, #running-req: 19, #queue-req: 30, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:50 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.91, #running-req: 19, #queue-req: 29, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:50 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.93, #running-req: 20, #queue-req: 29, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:51 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.94, #running-req: 20, #queue-req: 29, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:51 TP0] Prefill batch. #new-seq: 1, #new-token: 7429, #cached-token: 0, token usage: 0.95, #running-req: 20, #queue-req: 29, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:52 TP0] Decode batch. #running-req: 21, #token: 630231, token usage: 0.96, gen throughput (token/s): 5.22, #queue-req: 29, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:54 TP0] Decode batch. #running-req: 21, #token: 631071, token usage: 0.96, gen throughput (token/s): 576.32, #queue-req: 29, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:55 TP0] Decode batch. #running-req: 21, #token: 631911, token usage: 0.97, gen throughput (token/s): 570.95, #queue-req: 29, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:56 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 28, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:56 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.01, #running-req: 0, #queue-req: 28, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:56 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.03, #running-req: 0, #queue-req: 28, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:57 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.04, #running-req: 0, #queue-req: 27, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:57 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.05, #running-req: 1, #queue-req: 27, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:58 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.06, #running-req: 1, #queue-req: 27, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:58 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.08, #running-req: 1, #queue-req: 27, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:59 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.09, #running-req: 1, #queue-req: 26, 
[36m(task, pid=6695)[0m [2025-04-22 06:17:59 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.10, #running-req: 2, #queue-req: 26, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:00 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.11, #running-req: 2, #queue-req: 26, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:01 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.13, #running-req: 2, #queue-req: 25, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:01 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.14, #running-req: 3, #queue-req: 25, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:02 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.15, #running-req: 3, #queue-req: 25, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:02 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.16, #running-req: 3, #queue-req: 25, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:03 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.18, #running-req: 3, #queue-req: 24, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:03 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.19, #running-req: 4, #queue-req: 24, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:04 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.20, #running-req: 4, #queue-req: 24, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:04 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.21, #running-req: 4, #queue-req: 24, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:05 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.23, #running-req: 4, #queue-req: 23, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:05 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.24, #running-req: 5, #queue-req: 23, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:06 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.25, #running-req: 5, #queue-req: 23, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:06 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.26, #running-req: 5, #queue-req: 22, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:07 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.28, #running-req: 6, #queue-req: 22, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:07 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.29, #running-req: 6, #queue-req: 22, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:08 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.30, #running-req: 6, #queue-req: 22, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:08 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.31, #running-req: 6, #queue-req: 21, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:09 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.33, #running-req: 7, #queue-req: 21, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:09 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.34, #running-req: 7, #queue-req: 21, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:10 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.35, #running-req: 7, #queue-req: 21, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:10 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.36, #running-req: 7, #queue-req: 20, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:11 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.38, #running-req: 8, #queue-req: 20, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:11 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.39, #running-req: 8, #queue-req: 20, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:12 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.40, #running-req: 8, #queue-req: 19, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:13 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.41, #running-req: 9, #queue-req: 19, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:13 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.43, #running-req: 9, #queue-req: 19, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:14 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.44, #running-req: 9, #queue-req: 19, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:14 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.45, #running-req: 9, #queue-req: 18, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:15 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.46, #running-req: 10, #queue-req: 18, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:15 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.48, #running-req: 10, #queue-req: 18, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:16 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.49, #running-req: 10, #queue-req: 18, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:16 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.50, #running-req: 10, #queue-req: 17, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:17 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.51, #running-req: 11, #queue-req: 17, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:17 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.53, #running-req: 11, #queue-req: 17, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:18 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.54, #running-req: 11, #queue-req: 16, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:18 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.55, #running-req: 12, #queue-req: 16, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:19 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.56, #running-req: 12, #queue-req: 16, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:19 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.58, #running-req: 12, #queue-req: 16, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:20 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.59, #running-req: 12, #queue-req: 15, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:20 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.60, #running-req: 13, #queue-req: 15, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:21 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.61, #running-req: 13, #queue-req: 15, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:21 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.63, #running-req: 13, #queue-req: 15, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:22 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.64, #running-req: 13, #queue-req: 14, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:22 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.65, #running-req: 14, #queue-req: 14, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:23 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.66, #running-req: 14, #queue-req: 14, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:24 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.68, #running-req: 14, #queue-req: 13, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:24 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.69, #running-req: 15, #queue-req: 13, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:25 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.70, #running-req: 15, #queue-req: 13, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:25 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.71, #running-req: 15, #queue-req: 13, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:26 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.73, #running-req: 15, #queue-req: 12, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:26 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.74, #running-req: 16, #queue-req: 12, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:27 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.75, #running-req: 16, #queue-req: 12, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:27 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.76, #running-req: 16, #queue-req: 12, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:28 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.78, #running-req: 16, #queue-req: 11, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:28 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.79, #running-req: 17, #queue-req: 11, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:29 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.80, #running-req: 17, #queue-req: 11, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:29 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.81, #running-req: 17, #queue-req: 10, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:30 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.83, #running-req: 18, #queue-req: 10, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:30 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.84, #running-req: 18, #queue-req: 10, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:31 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.85, #running-req: 18, #queue-req: 10, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:31 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.86, #running-req: 18, #queue-req: 9, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:32 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.88, #running-req: 19, #queue-req: 9, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:32 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.89, #running-req: 19, #queue-req: 9, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:33 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.90, #running-req: 19, #queue-req: 9, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:33 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.91, #running-req: 19, #queue-req: 8, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:34 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.93, #running-req: 20, #queue-req: 8, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:34 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.94, #running-req: 20, #queue-req: 8, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:35 TP0] Prefill batch. #new-seq: 1, #new-token: 7429, #cached-token: 0, token usage: 0.95, #running-req: 20, #queue-req: 8, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:37 TP0] Decode batch. #running-req: 21, #token: 630651, token usage: 0.96, gen throughput (token/s): 20.12, #queue-req: 8, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:38 TP0] Decode batch. #running-req: 21, #token: 631491, token usage: 0.97, gen throughput (token/s): 576.30, #queue-req: 8, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:39 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 7, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:39 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.01, #running-req: 0, #queue-req: 7, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:40 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.03, #running-req: 0, #queue-req: 7, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:41 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.04, #running-req: 0, #queue-req: 6, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:41 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.05, #running-req: 1, #queue-req: 6, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:42 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.06, #running-req: 1, #queue-req: 6, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:42 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.08, #running-req: 1, #queue-req: 6, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:43 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.09, #running-req: 1, #queue-req: 5, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:43 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.10, #running-req: 2, #queue-req: 5, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:44 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.11, #running-req: 2, #queue-req: 5, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:44 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.13, #running-req: 2, #queue-req: 4, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:45 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.14, #running-req: 3, #queue-req: 4, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:46 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.15, #running-req: 3, #queue-req: 4, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:46 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.16, #running-req: 3, #queue-req: 4, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:47 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.18, #running-req: 3, #queue-req: 3, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:47 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.19, #running-req: 4, #queue-req: 3, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:48 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.20, #running-req: 4, #queue-req: 3, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:48 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.21, #running-req: 4, #queue-req: 3, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:49 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.23, #running-req: 4, #queue-req: 2, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:49 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.24, #running-req: 5, #queue-req: 2, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:50 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.25, #running-req: 5, #queue-req: 2, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:50 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.26, #running-req: 5, #queue-req: 1, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:51 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.28, #running-req: 6, #queue-req: 1, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:51 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.29, #running-req: 6, #queue-req: 1, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:52 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.30, #running-req: 6, #queue-req: 1, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:52 TP0] Prefill batch. #new-seq: 2, #new-token: 8192, #cached-token: 0, token usage: 0.31, #running-req: 6, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:53 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.33, #running-req: 7, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:53 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.34, #running-req: 7, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:54 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.35, #running-req: 7, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:54 TP0] Prefill batch. #new-seq: 1, #new-token: 2440, #cached-token: 0, token usage: 0.36, #running-req: 7, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:55 TP0] Decode batch. #running-req: 8, #token: 240088, token usage: 0.37, gen throughput (token/s): 43.39, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:56 TP0] Decode batch. #running-req: 8, #token: 240408, token usage: 0.37, gen throughput (token/s): 348.43, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:57 TP0] Decode batch. #running-req: 8, #token: 240728, token usage: 0.37, gen throughput (token/s): 346.77, #queue-req: 0, 
[36m(task, pid=6695)[0m [2025-04-22 06:18:57] INFO:     127.0.0.1:56290 - "GET /get_server_info HTTP/1.1" 200 OK
[36m(task, pid=6695)[0m == End of server logs ==
[36m(task, pid=6695)[0m Benchmark results for sgl on deepseek-ai/DeepSeek-R1:
[36m(task, pid=6695)[0m | Input Tokens | Output Tokens | Output Token Throughput (tok/s) |
[36m(task, pid=6695)[0m |1000 | 2000 | 1329.14 |
[36m(task, pid=6695)[0m |5000 | 1000 | 951.64 |
[36m(task, pid=6695)[0m |10000 | 500 | 479.69 |
[36m(task, pid=6695)[0m |30000 | 100 | 47.38 |
[0m[32m✓ Job finished (status: SUCCEEDED).[0m
