name: vllm-benchmark

resources:
  accelerators: H200:8
  disk_size: 1000
  memory: 750+

file_mounts:
  /tmp/setup.sh: ./setup.sh

setup: |
  set -e
  # Run the common setup script
  bash -i /tmp/setup.sh

run: |
  # Log system information
  nvidia-smi

  cd ~/vLLM-Benchmark

  just serve ${ENGINE} ${MODEL} > ${ENGINE}_${MODEL}.log 2>&1 &

  if [ "${ENGINE}" == "vllm" ]; then
    until grep -q "Started server process" ${ENGINE}_${MODEL}.log; do
      sleep 5
      echo "Waiting for ${ENGINE} server to start..."
    done
    echo "vLLM server started"
  fi

  just run-sweeps ${ENGINE} ${MODEL}

  just show-results
  

envs:
  MODEL: "llama-8b"
  ENGINE: "vllm"
  HF_TOKEN:
