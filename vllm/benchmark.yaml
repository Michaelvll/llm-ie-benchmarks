name: vllm-benchmark

resources:
  accelerators: H200:8
  disk_size: 1000
  memory: 750+

file_mounts:
  /tmp/setup.sh: ./setup.sh

setup: |
  set -e
  # Run the common setup script
  bash -i /tmp/setup.sh

run: |
  # Log system information
  unset OMP_NUM_THREADS
  nvidia-smi
  
  lscpu

  cd ~/vLLM-Benchmark

  just serve ${ENGINE} ${MODEL} > ${ENGINE}_${MODEL}.log 2>&1 &

  until grep -q "Started server process" ${ENGINE}_${MODEL}.log; do
    sleep 5
    echo "Waiting for ${ENGINE} server to start..."
  done
  sleep 10
  echo "$ENGINE server started"

  just run-sweeps ${ENGINE} ${MODEL}

  if [ "$ENGINE" == "sgl" ]; then
    # SGLang has a JIT code generation overhead, so we discard the first run,
    # and run the sweeps again.
    just show-results ${MODEL}
    rm results/$MODEL/$ENGINE-*.json || true
    just run-sweeps ${ENGINE} ${MODEL}
  fi

  just show-results ${MODEL}
  

envs:
  MODEL: "llama-8b"
  ENGINE: "vllm"
  HF_TOKEN:


# experimental:
#   config_overrides:
#     nvidia_gpus:
#       disable_ecc: true
