name: vllm-benchmark

resources:
  accelerators: H200:8
  disk_size: 1000
  memory: 750+

file_mounts:
  /tmp/setup.sh: ./setup.sh

setup: |
  set -e
  # Run the common setup script
  bash -i /tmp/setup.sh

run: |
  # Log system information
  unset OMP_NUM_THREADS
  nvidia-smi
  
  lscpu

  cd ~/vLLM-Benchmark

  just serve ${ENGINE} ${MODEL} > ${ENGINE}_${MODEL}.log 2>&1 &

  if [ "${ENGINE}" == "vllm" || "${ENGINE}" == "sgl" ]; then
    until grep -q "Started server process" ${ENGINE}_${MODEL}.log; do
      sleep 5
      echo "Waiting for ${ENGINE} server to start..."
    done
    echo "$ENGINE server started"
  # elif [ "${ENGINE}" == "trt" ]; then
  #   until grep -q "Server started" ${ENGINE}_${MODEL}.log; do
  #     sleep 5
  #     echo "Waiting for ${ENGINE} server to start..."
  #   done
  #   echo "$ENGINE server started"
  fi

  just run-sweeps ${ENGINE} ${MODEL}

  just show-results ${MODEL}
  

envs:
  MODEL: "llama-8b"
  ENGINE: "vllm"
  HF_TOKEN:
